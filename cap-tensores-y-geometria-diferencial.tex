\chapter{Análisis tensorial y geometría diferencial}\label{cap:tensores}

\section{Variedades diferenciables}

\textbf{Definición:} Una variedad diferenciable $n$-dimensional $M$ es
un conjunto continuo de puntos que puede ser cubierto completamente por un
conjunto contable de vecindades abiertas $U_1, U_2,\dots$ sobre los cuales pueden ser definidos sistemas coordenados ($n$-dimensionales) continuo y diferenciables, tales que en las intersecciones de dichas vecindades, sus correspondientes sistemas están relacionados unos a otros por transformaciones de coordenadas diferenciables.

Una variedad puede ser concebida básicamente como un espacio de
dimensión $n$, análogo a una superficie $n$-dimensional. En general,
es posible que ella no pueda ser cubierta completamente por un único sistema coordenado.
Curvas y superficies en el espacio $n$-dimensional $R_n$ son ejemplos de variedades.
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=6cm]{fig/fig-coordenadas.pdf}}
\caption{Una variedad y un sistema coordenado.}
\label{1-1}
\end{figure}
\end{center}

Como vemos en la figura \ref{1-1}, sobre cada vecindad $U$ pueden definirse sistemas coordenados (SC's) que asignan unívocamente a cada punto $P\in U$
un conjunto ordenado de números $x^i=(x^1,\dots ,x^n)$,
$i,j,\dots =1,\dots ,n$, llamados \textit{coordenadas de} $P$. Se exige que este mapeo uno a
uno sea continuo, de modo que, cuando $P$ se mueve en $U$, la correspondiente $n$-upla
($x^1,\dots ,x^n$) se mueve continuamente en un dominio $D$ contenido en $R_n$. También se supone que la dimensión $n$ del mapeo de puntos de $M$ a $R_n$ es siempre la misma, en la vecindad de cada punto de la variedad.
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=6cm]{fig/fig-cambio-coordenadas.pdf}}
\caption{Una variedad y dos sistemas coordenados.}
\label{2-1}
\end{figure}
\end{center}

Consideremos la figura \ref{2-1}. El conjunto $M$ es una variedad diferenciable
si para cada punto $P$ en la
intersección de dos abiertos, $U_1$, $U_2$ $\subseteq M$, los
correspondientes sistemas de coordenadas están relacionados por transformaciones diferenciables e invertibles: 
\begin{equation}
\bar{x}^j  =\bar{x}^j (x^i ), \qquad x^i  =x^i (\bar{x}^j ),\label{v1}
\end{equation}
de modo que, en cada punto de $U_1\cap U_2$,
\begin{equation}
\frac{\partial x^i}{\partial\bar{x}^j}(P)\frac{\partial\bar{x}^j}{\partial
x^k}(P)=\delta_k^i, \qquad \frac{\partial\bar{x}^j}{\partial x^i}(P)\frac{\partial x^i}{\partial\bar{x}^l}(P)=\delta_l^j , \label{v2}
\end{equation}
y además
\begin{equation}
\frac{\partial(x^1,\dots ,x^n)}{\partial(\bar{x}^1
,\dots ,\bar{x}^n)}\cdot\frac{\partial(\bar{x}^1,\dots
,\bar{x}^n)}{\partial (x^1,\dots ,x^n)}=1. \label{v3}
\end{equation}

\section{Escalares, vectores y tensores}

En una variedad dada, es posible (y útil) definir cantidades que representen magnitudes de interés físico. Estas cantidades pueden en general poseer componentes cuyos valores dependen del sistema de coordenadas usado para describir una cierta región de la variedad. Por esto, es importante estudiar cómo cambian las distintas cantidades definidas sobre una variedad bajo TGC's, clasificándolas de acuerdo a la ley de transformación que satisfacen. A continuación estudiaremos la definición de vectores y tensores bajo TGC's. Note que los tensores no agotan todas las cantidades útiles de definir en una variedad\footnote{Por ejemplo, en algunas aplicaciones es útil considerar objetos llamados \textbf{densidades tensoriales}, cuya ley de transformación es distinta a los de los tensores. Además, más adelante definiremos objetos llamados \textbf{conexiones}, que tampoco son tensores.}

\subsection{Escalares}
El caso más simple de una magnitud definida sobre una variedad es aquel en que se asocia un valor (real) a cada punto $P\in M$, el cual, por definición, no cambia bajo una transformación de coordenadas (TC) arbitraria.

\begin{quotation}
\textbf{Definición:} Una cantidad real $\phi$, definido en un punto $P$ de la variedad $M$, es un \textbf{escalar} si bajo toda TGC se verifica que
\begin{equation}\marginnote{Escalar}
\boxed{\bar{\phi}(P)=\phi(P).} \label{t1}
\end{equation}
\end{quotation}

\subsection{Vectores contravariantes}

Consideremos dos puntos $P$ y $Q$, infinitesimalmente próximos, de la variedad
$M$. Las respectivas coordenadas de estos puntos en un SC $x$ son
\begin{equation}
x^i(P)=x^i, \qquad x^i(Q)=x^i+dx^i.
\end{equation}
En otro SC $\bar{x}$, relacionado con el original por medio de (\ref{v1}),
tendremos
\begin{equation}
\bar{x}^i(P)=\bar{x}^i, \qquad \bar{x}^i(Q)=\bar{x}^i+d\bar{x}^i. \label{xP}
\end{equation}

Encontremos ahora la relación entre las diferencias coordenadas $d\bar{x}^i$ y $dx^i$.
Usando (\ref{v1}) y (\ref{xP}b) podemos escribir
\begin{equation}
\bar{x}^i(Q)=\bar{x}^i(x(Q))=\bar{x}^i(x^j+dx^j)=\bar{x}^i(x^j)+\frac{
\partial\bar{x}^i}{\partial x^j}(x)dx^j,
\end{equation}
de modo que obtenemos
\begin{equation}\marginnote{Transf. de diferencias coordenadas}
\boxed{d\bar{x}^i_{P\rightarrow Q}=\frac{\partial\bar{x}^i}{\partial
x^j}(P)\,dx^j_{P\rightarrow Q}.} \label{tdx}
\end{equation}

Por tanto, bajo un cambio de SC, las diferencias de coordenadas entre puntos $P$ y
$Q$ infinitesimalmente próximos cambian su valor, pero están relacionados por
medio del Jacobiano de la TGC, \textit{evaluado en el punto $P$}. La ley de
transformación (\ref{tdx}) es el modelo para definir lo que llamaremos un
\textbf{vector contravariante}.

\begin{quotation}
\textbf{Definición:} Se dice que un conjunto de $n$ números
$A^i(P)=(A^1,\dots ,A^n)$, definidos en cada SC $x$, son las componentes  de un
\textbf{vector contravariante en un punto} $P$ (en el SC $x$), si bajo cada TGC (\ref{v1}), dichas componentes obedecen la siguiente ley de transformación:
\begin{equation}\marginnote{Vector contravariante}
\boxed{\bar{A}^i(P)=\frac{\partial\bar{x}^i}{\partial x^j}(P)A^j(P).} \label{t9}
\end{equation}
\end{quotation}

De esta forma, la diferencia de coordenadas entre dos puntos infinitesimalmente próximos $P$ y $Q$ define un vector contravariante infinitesimal en $P$. Equivalentemente, si $\cal C$ es una curva en $M$ (e.d. una subvariedad unidimensional de $M$), parametrizada (en una cierta vecindad y en algún SC definido en ella) por $x^i=x^i(\lambda)$ donde $\lambda$ es un parámetro real continuo, entonces los \textbf{vectores tangentes} $A^ i(P):=(dx^i/d\lambda)(\lambda_P)$ son componentes de un vector contravariante asociado al punto $P$ (con coordenadas $x^i(P)=x^i(\lambda_P)$).

Se debe enfatizar que la ley de transformación anterior define un vector
contravariante \textit{asociado a un punto dado} de la variedad. Esta identificación de un vector con un punto de la variedad es necesaria ya que el jacobiano  ${\partial\bar{x}^i}/{\partial x^j}$ \textit{no es constante en general}, es decir, su valor depende del punto donde es evaluado. En general, cada punto de la variedad puede tener asociado infinitos vectores contravariantes. Es
fácil comprobar que si $A^i(P)$ es un vector contravariante definido en un
punto $P$ dado, entonces $\alpha A^i(P)$ (donde $\alpha$ es un escalar) es un
nuevo vector contravariante en el punto $P$. Análogamente, dado dos vectores
contravariantes $A^i(P)$ y $B^i(P)$ definidos \textit{en el mismo punto}
entonces $\alpha A^i(P)+\beta B^i(P)$ es también un vector contravariante en
$P$ (para valores arbitrarios de los escalares $\alpha$ y $\beta$). En otras
palabras, el conjunto de vectores contravariantes definidos \textit{en un mismo punto} $P$ define un \textbf{espacio vectorial $n$-dimensional}, que llamaremos \textbf{espacio tangente en $P$}: $T_n(P)$. \marginnote{Espacio Tangente}

Es importante destacar que los vectores contravariantes definidos en $P$ son
elementos del espacio tangente de $P$, y \textit{no son elementos de} $M$ (los
elementos de $M$ son los \textit{puntos} de la variedad). Note además que las componentes $x^i$ de un punto $P$ de $M$ \textit{no son componentes de un vector} contravariante\marginnote{Coords. no son vectores bajo TGC's}, simplemente porque su ley de transformación bajo TGC's es dada por (\ref{v1}), que es diferente de (\ref{t9})\footnote{Ambas leyes de transformación, de las coordenadas y de vectores contravariantes, coinciden sólo si restringimos nuestra atención a transformaciones \textit{lineales} de coordenadas, de la forma $\bar{x}^i=L^i_{\ j}x^j$, donde $L^i_{\ j}$ son (en general 16 valores independientes) \textit{constantes}. Este hecho justifica porqué en mecánica newtoniana y en la Teoría Especial de la Relatividad sí es posible considerar las coordenadas como vectores, bajo rotaciones y transformaciones de Lorentz, respectivamente.}

\subsubsection{Representación pictórica de vectores contravariantes}
Es posible representar pictóricamente un vector contravariante $A^i(P)$ por
medio de una ``flechita'' (infinitesimal) a partir del punto $P$. Más detalladamente, dado un
vector contravariante $A^i(P)$ en $P$ es posible asociar consistentemente (esto quiere decir, independiente del SC usado) un punto $Q$ infinitesimalmente próximo a $P$. En un SC dado (pero arbitrario) con coordenadas $x^i$, definimos las componentes del nuevo punto $Q$ por
\begin{equation}
x^i(Q):=x^i(P)+\varepsilon A^i(P), \label{defQ}
\end{equation}
donde $\varepsilon$ es un \textit{parámetro escalar infinitesimal}.

La ley de transformación de $A^i(P)$, es decir, el hecho éste que sea un vector contravariante, asegura que la definición del punto $Q$ de la variedad por medio de (\ref{defQ}) sea independiente del SC usado. [En realidad, un vector contravariante define sólo la \textit{dirección} de la correspondiente flechita, la ubicación del punto $Q$ depende también del parámetro infinitesimal $\varepsilon$].

Equivalentemente, es posible definir la representación pictórica de vectores contravariantes asociándoles una curva $\cal C$ (al menos, en alguna vecindad que incluya al punto $P$) tal que su vector tangente en $P$ sea igual (o proporcional, con una constante de proporcionalidad positiva) al vector $A^i(P)$, es decir, tal que $(dx^i/d\lambda)(P)=A^i(P)$.

\subsection{Vectores covariantes}

Consideremos un campo escalar $\phi$ definido en una región de la variedad. En
un SC $x$ tendremos una dependencia explícita $\phi(x)$, y podemos calcular
las derivadas de $\phi$ respecto a las coordenadas $x^i$, es decir, el gradiente
del campo $\phi$, ${\partial \phi}/{\partial x^i}$, que define nuevos
campos. Análogamente, si usamos un nuevo SC $\bar{x}$, tendremos otras
funciones explícitas $\phi(\bar{x})$ y podemos calcular el correspondiente
gradiente ${\partial \phi}/{\partial \bar{x}^i}$. En un punto $P$ dado,
podemos relacionar estos gradientes usando la transformación coordenada
(\ref{v1}) y la regla de la cadena:
\begin{equation}
\boxed{\frac{\partial\phi}{\partial \bar{x}^i}(\bar{x}(P))=\frac{\partial x^j
}{\partial\bar{x}^i}(P)\frac{\partial\phi}{\partial x^j}(x(P)).} \label{tt2}
\end{equation}
Comparando (\ref{tt2}) con (\ref{t9}) vemos que las $n$ cantidades definidas por
\textit{el gradiente de un campo escalar} (calculado respecto a un SC) \textit{no forman
un vector contravariante}. En otras palabras, las derivadas de un campo escalar definen un nuevo tipo de objeto, que llamaremos \textbf{vector covariante}.

\begin{quotation}
\textbf{Definición:} Se dice que un conjunto de $n$ cantidades $A_i(P)=(A_1,\dots
,A_n)$ (definidas en cada SC) son las componentes (en el SC $x^i$) de un \textit{vector covariante en un punto} $P$, si bajo cada TGC (\ref{v1}) dichas componentes obedecen la siguiente ley de transformación:
\begin{equation}\marginnote{Vector Contravariante}
\boxed{\bar{A}_i(P)=\frac{\partial x^j}{\partial\bar{x}^i}(P)A_j(P).}
\label{t6}
\end{equation}
\end{quotation}

Análogamente al caso de vectores contravariantes, el conjunto de vectores
covariantes definidos en un punto dado $P$ de la variedad forma un espacio
vectorial $n$-dimensional: el \textbf{espacio cotangente en} $P$: $T_n^*(P)$. Es
importante notar que los espacios tangente y cotangente son espacios
vectoriales distintos definidos en cada punto de la variedad. Nuevamente, los
vectores covariantes son elementos del espacio cotangente de cada punto y no de la variedad $M$. Por otro lado, los vectores covariantes no pueden ser representados por ``flechitas'', ya que no están (directamente, al menos) relacionados con direcciones (desplazamientos) en la variedad.


\subsection{Tensores}
Consideremos el producto de las componentes $A^i (P)$ de un vector
contravariante y las componentes $B_j(P)$ de un vector covariante. Veamos
cómo transforma el siguiente producto bajo una TGC:
\begin{equation}
\bar{A}^i (P)\bar{B}_j (P)=\frac{\partial\bar{x}^i}{\partial x^l}(P)A^l (P)
\frac{\partial x^k}{\partial\bar{x}^j}(P)B_k(P)=\frac{\partial\bar{x}^i 
}{\partial x^l}(P)\frac{\partial x^k}{\partial\bar{x}^j}(P)A^l (P)B_k(P).
\label{t11}
\end{equation}

Las $n^2$ cantidades $(A^i B_j)(P)$ transforman bajo una TCG en forma lineal y homogénea según la relación (\ref{t11}). Esto motiva la siguiente definición:
\begin{quotation}
\textbf{Definición:} Un conjunto de $n^2$ cantidades $T_{\ j}^i (P)$ son las
componentes de un tensor de tipo $(^1_1)$ en un punto $P$, si bajo una TGC
dichas componentes transforman según la ley:
\begin{equation}
\boxed{\bar{T}_{\ j}^i (P)=\frac{\partial\bar{x}^i}{\partial
x^k}(P)\frac{\partial x^l}{\partial\bar{x}^j}(P)\,T_{\ l}^k(P).} \label{t12}
\end{equation}
\end{quotation}

Note que en el ejemplo anterior, el tensor $T^i_{\ j}:=A^i B_j$ fue construido como un producto de las componentes de dos vectores (uno covariante y otro contravariante, en este caso). Sin embargo, \textit{no todo tensor de tipo $(^1_1)$ puede ser ``factorizado"\, en un producto de dos vectores}.

Podemos generalizar la definición anterior a objetos con más índices:
\begin{quotation}
\textbf{Definición:} Un conjunto de $n^{r+s}$ cantidades $T_{\ \ \ \ \ \
j_1\dots j_{s}}^{i_1\dots i_{r}}(P)$ son las componentes de un tensor de
rango $r+s$ y tipo $(^r_s)$ en un punto $P$ si, bajo cada TGC, dichas componentes
transforman según la ley:
\begin{equation}\marginnote{Tensor general}
\boxed{\bar{T}_{\ \ \ \ \ \ \ j_1\cdots j_{s}}^{i_1\dots
i_{r}}(P)=\frac{\partial\bar{x}^{i_1}
}{\partial x^{k_1}}(P)\cdots \frac{\partial\bar{x}^{i_{r}}}{\partial
x^{k_{r}}}(P)\frac{\partial x^{l_1}}{\partial\bar{x}^{j_1}}(P)\cdots
\frac{\partial x^{l_{s}}}{\partial\bar{x}^{j_{s}}}(P)\,T_{\ \ \ \ \ \ \
l_1\cdots l_{s}}^{k_1\cdots k_{r}}(P).}
\label{t13}
\end{equation}
\end{quotation}
Note que $T_{\ \ \ \ \ \ \ j_1\dots j_s}^{i_1\dots i_r}(P)$ es un tensor de
carácter mixto, contravariante de rango $r$ y covariante de rango $s$, definido en
el punto $P$.

Algunas consecuencias directas de la definición de tensores bajo TGC's son:
\begin{itemize}
\item Los dos tipos de vectores definidos previamente son casos
especiales de tensores. Un vector contravariante es un tensor del tipo
$(^1_0)$ y un vector covariante es un tensor del tipo $(^0_1)$. Un
escalar puede ser considerado como un tensor de tipo $(^0_0)$.

\item La relación (\ref{v2}b) puede ser escrita en la forma:
\begin{equation}
\delta_l^j =\frac{\partial\bar{x}^j}{\partial x^i}(P)\frac{\partial
x^k}{\partial\bar{x}^l}(P)\,\delta_k^i , \label{t15}
\end{equation}
lo cual muestra que (los $n^2$ números definidos por) la delta de Kronecker
define un tensor del tipo $(^1_1)$. Además, la delta de Kronecker es una de las muy pocas entidades tensoriales \textit{numéricamente invariantes} (es decir, que asume los mismos valores en todo SC) que es posible definir en una variedad cualquiera.

\item Una de las características principales de los tensores es que si
todas las componentes de un tensor son nulas en un SC entonces ellas se anulan
\textit{en todo SC}. Esto se verifica directamente a partir de (\ref{t13}) y en
particular del hecho que la transformación es \textit{homogénea}\footnote{Los tensores no son los únicos objetos útiles que cumplen esta propiedad, las \textbf{densidades tensoriales} también lo hacen.}.
\end{itemize}

\section{Algebra tensorial sobre variedades diferenciables}

Dadas las componentes de uno o más tensores, es posible definir una infinidad de operaciones algebraicas usando sus componentes. Sin embargo, sólo algunas de estas operaciones definirán nuevos tensores. A continuación resumiremos las operaciones algebraicas básicas (más usadas) que mapean tensores en tensores (no necesariamente del mismo tipo).


\subsection{Multiplicación}

La multiplicación de todas las componentes de un tipo ($_{s_1}^{r_1}$) con las de otro de tipo ($_{s_2}^{r_2}$), definidos en un mismo punto $P$, conduce a un tensor de
tipo ($_{s_1+s_2}^{r_1+r_2}$) en $P$. Este proceso es llamado
\textbf{producto directo}.

Para ilustrar esto consideramos, por ejemplo, un tensor de tipo ($_1^2$) y
un tensor de tipo ($_2^0$), pues el razonamiento es general. Sus leyes de
transformación son, respectivamente\footnote{Desde ahora, para no recargar la notación, omitiremos el punto $P$ donde cada tensor es definido.}:
\begin{align}
\bar{T}_{\ \ m}^{jl} & =\frac{\partial\bar{x}^j}{\partial x^i}\frac
{\partial\bar{x}^l}{\partial x^k}\frac{\partial x^p}{\partial\bar{x}
^m}\,T_{\ \ p}^{ik},\label{mul1}\\
\bar{S}_{qr} & =\frac{\partial x^{t}}{\partial\bar{x}^q}\frac{\partial
x^{u}}{\partial\bar{x}^r}\,S_{tu}.\nonumber
\end{align}
El producto de las componentes de estos dos tensores se transforma de acuerdo a:
\begin{equation}
\bar{T}_{\ \ m}^{jl}\bar{S}_{qr}=\frac{\partial\bar{x}^j}{\partial x^i
}\frac{\partial\bar{x}^l}{\partial x^k}\frac{\partial
x^p}{\partial\bar{x}^m}
\frac{\partial x^t}{\partial\bar{x}^q}\frac{\partial
x^{u}}{\partial\bar{x}^r}\,T_{\ \ p}^{ik}S_{tu}, \label{2}
\end{equation}
que es la ley de transformación de las componentes de un
tensor de tipo $(^2_3)$, $V_{\ \ klm}^{ij}:=T_{\ \ k}^{ij}S_{lm}$. Note que esta conclusión es válida sólo si ambos tensores $T$ y $S$ están \textit{definidos en el mismo punto}.

Como caso particular vemos que la multiplicación de (todas) las componentes de
un tensor de tipo $(^r_s)$ por un escalar (es decir, un tensor de tipo $(^0_0)$ conduce a un tensor del mismo tipo.

\subsection{Adición}

Sea $S_{\ \ \ \ \ \ k_1\dots k_{s}}^{i_1\dots i_{r}}$ un tensor del
tipo $(^r_s)$ definido en el punto $P$. Su ley de transformación, de acuerdo
con
(\ref{t13}), es:
\begin{equation}
\bar{S}_{\ \ \ \ \ \ k_1\dots k_{s}}^{i_1\dots
i_{r}}=\frac{\partial\bar{x}^{i_1}
}{\partial x^{l_1}}\dots \frac{\partial\bar{x}^{i_{r}}}{\partial x^{l_{r}}
}\frac{\partial x^{m_1}}{\partial\bar{x}^{k_1}}\dots \frac{\partial x^{m_{s}
}}{\partial\bar{x}^{k_{s}}}\,S_{\ \ \ \ \ \ m_1\dots m_{s}}^{l_1\dots
l_{r}},
\label{ad1}
\end{equation}
y sea el tensor $T_{\ \ \ \ \ \ k_1\dots k_{s}}^{i_1\dots i_{r}}$ otro
tensor de tipo $(^r_s)$, entonces la suma de sus componentes $T_{\ \ \ \ \ \
k_1\dots k_{s}}^{i_1\dots i_{r}}+S_{\ \ \ \ \ \ k_1\dots
k_{s}}^{i_1\dots i_{r}}$ definen un nuevo tensor de tipo $(^r_s)$ ya que
\begin{equation}
\bar{T}_{\ \ \ \ \ \ k_1\dots k_{s}}^{i_1\dots i_{r}}+\bar{S}_{\ \ \ \ \ \
k_1\dots k_{s}}^{i_1\dots i_{r}}=\frac{\partial\bar{x}^{i_1}}{\partial
x^{l_1}}\dots \frac{\partial\bar{x}^{i_{r}}}{\partial x^{l_{r}}}\frac{\partial
x^{m_1}}{\partial\bar{x}^{k_1}}\dots
\frac{\partial x^{m_{s}}}{\partial\bar{x}^{k_{s}}}\left( T_{\ \ \ \ \ \
m_1\dots m_{s}}^{l_1\dots l_{r}}+S_{\ \ \ \ \ \ m_1\dots
m_{s}}^{l_1\dots l_{r}}\right) .\label{ad2}
\end{equation}

La operación de multiplicación puede ser combinada con la de adición de tensores, 
siempre que sus respectivos tipos sean apropiados. Estas operaciones satisfacen las leyes conmutativa, asociativa y distributiva.

Combinando la adición con la multiplicación por escalar vemos que el conjunto de todos los tensores de un tipo $(^r_s)$ dado, en el punto $P$ de $M$, constituye un espacio vectorial de dimensión $n^{r+s}$.

Es importante notar que una combinación lineal de tensores de \textit{distinto
tipo} en un mismo punto, o de tensores del mismo tipo, pero \textit{en distintos puntos},
 \textit{no suministra un nuevo tensor}.


\subsection{Contracción}

Dado un tensor de tipo $(^r_s)$ es posible construir un tensor de tipo
($_{s-1}^{r-1}$) seleccionando un superíndice y un subíndice y
sumando sobre componentes iguales, de acuerdo con la convención de suma de
Einstein. En efecto, de (\ref{t13}) obtenemos que si sumamos sobre el $p$-ésimo indice contravariante y el $q$-ésimo covariante, tendremos que
\begin{eqnarray}
\bar{T}_{\ \ \ \ \ \ \ \ \ k_1\cdots i\cdots k_{s}}^{i_1\cdots i\cdots
i_{r}}
&=&\frac{\partial\bar{x}^{i_1}}{\partial x^{l_1}}\cdots
\frac{\partial\bar{x}^{i_{p-1}}}{\partial x^{l_{p-1}}}\,\frac{\partial\bar{x}^i}{\partial x^l}\,\frac{\partial\bar{x}^{i_{p+1}}}{\partial x^{l_{p+1}}}\cdots \frac{\partial\bar{x}^{i_{r}}}{\partial x^{l_{r}}}
 \nonumber \\
&& \times\ \frac{\partial x^{m_1}}{\partial\bar{x}^{k_1}}\frac{\partial x^{m_{q-1}}}{\partial\bar{x}^{k_{q-1}}}\frac{\partial x^{m}
}{\partial\bar{x}^i}\frac{\partial x^{m_{q+1}}}{\partial\bar{x}^{k_{q+1}}}\cdots
\frac{\partial x^{m_{s}}}{\partial\bar{x}^{k_{s}}}\,T_{\ \ \ \ \ \ \ \ \
m_1\cdots m\cdots m_{s}}^{l_1\cdots l\cdots l_{r}} \label{con1}\\
&=&\left(\frac{\partial\bar{x}^i}{\partial x^l}\frac{\partial x^m
}{\partial\bar{x}^i}\right)\frac{\partial\bar{x}^{i_1}}{\partial x^{l_1}}\cdots
\frac{\partial\bar{x}^{i_{p-1}}}{\partial
x^{l_{p-1}}}\frac{\partial\bar{x}^{i_{p+1}}}{\partial
x^{l_{p+1}}}\cdots\frac{\partial\bar{x}^{i_{r}}}{\partial x^{l_{r}}}\nonumber \\
&& \times\ \frac{\partial x^{m_1}}{\partial\bar{x}^{k_1}}\frac{\partial x^{m_{q-1}}}{\partial\bar{x}^{k_{q-1}}} \frac{\partial
x^{m_{q+1}}}{\partial\bar{x}^{k_{q+1}}}\cdots \frac{\partial x^{m_{s}}}
{\partial\bar{x}^{k_{s}}}\,T_{\ \ \ \ \ \ \ \ \ m_1\cdots m\cdots
m_{s}}^{l_1\cdots l\cdots l_{r}}\\
&=&\delta_l^m\, \frac{\partial\bar{x}^{i_1}}{\partial x^{l_1}}\cdots
\frac{\partial\bar{x}^{i_{p-1}}}{\partial
x^{l_{p-1}}}\frac{\partial\bar{x}^{i_{p+1}}}{\partial
x^{l_{p+1}}}\cdots\frac{\partial\bar{x}^{i_{r}}}{\partial x^{l_{r}}}\nonumber \\
&& \times\ \frac{\partial x^{m_1}}{\partial\bar{x}^{k_1}}\frac{\partial x^{m_{q-1}}}{\partial\bar{x}^{k_{q-1}}} \frac{\partial
x^{m_{q+1}}}{\partial\bar{x}^{k_{q+1}}}\cdots \frac{\partial x^{m_{s}}}
{\partial\bar{x}^{k_{s}}}\,T_{\ \ \ \ \ \ \ \ \ m_1\cdots m\cdots
m_{s}}^{l_1\cdots l\cdots l_{r}}\\
&=&\underbrace{\frac{\partial\bar{x}^{i_1}}{\partial x^{l_1}}\cdots
\frac{\partial\bar{x}^{i_{p-1}}}{\partial x^{l_{p-1}}} \frac{\partial\bar{x}^{i_{p+1}}}{\partial x^{l_{p+1}}} \cdots\frac{\partial\bar{x}^{i_{r}}}{\partial x^{l_{r}}}}_{(r-1) \text{
términos}}\nonumber \\
&& \times \underbrace{\frac{\partial
x^{m_1}}{\partial\bar{x}^{k_1}}\frac{\partial
x^{m_{q-1}}}{\partial\bar{x}^{k_{q-1}}} \frac{\partial
x^{m_{q+1}}}{\partial\bar{x}^{k_{q+1}}}\cdots \frac{\partial x^{m_{s}}}
{\partial\bar{x}^{k_{s}}}}_{(s-1) \text{ términos}}\,T_{\ \ \ \ \ \ \ \ \
m_1\cdots l\cdots m_{s}}^{l_1\cdots l\cdots l_{r}}.
\end{eqnarray}
Por tanto, $T_{\ \ \ \ \ \ \ \ \ k_1\dots i\dots k_{s}}^{i_1\dots i\dots
i_{r}} $ satisface ley de transformación de un tensor de tipo ($_{s-1}^{r-1}$).
Este proceso es conocido como \textbf{contracción}.

Claramente, el proceso de contracción de un tensor de tipo ($_1^1$) da
origen a un escalar. En particular, para el caso de la delta de Kronecker
tenemos:
\begin{equation}
\delta_j^j =\delta_1^1+\dots \delta_n^n=n. \label{con2}
\end{equation}


\subsection{Permutación y (Anti-)Simetrización}

Es directo verificar que la \textit{permutación de dos índices del mismo tipo (covariante o contravariante)} de un tensor define un nuevo tensor. Por ejemplo, si $A_{ij}$ son las componentes de un tensor de tipo $(^0_2)$ entonces las nuevas componentes $B_{ij}$ determinadas por la permutación de las componentes de $A_{ij}$, es decir, $B_{ij}:=A_{ji}$ también son las componentes de un tensor del mismo tipo. Lo mismo ocurre si se permutan dos índices contravariantes. Sin embargo, \textit{esto no ocurre si se permutan índices de distinto tipo}\footnote{Por ejemplo, si a partir de $A^i_{\ j}$ se define $B^i_{\ j}:=A^j_{\ i}$ o, más explícitamente, $B^1_{\ 2}:=A^2_{\ 1}$, $B^2_{\ 3}:=A^3_{\ 2}$, etc.}.

 Un tensor es llamado \textit{simétrico} con respecto a un par de índices (del mismo tipo, ya sean covariantes o contravariantes) si una permutación de ellos no afecta el valor de las componentes de dicho tensor. Si, por otro lado, este proceso afecta a cada
componente multiplicándola por $-1$, entonces el tensor es llamado
\textit{antisimétrico} en dichos índices. Finalmente, se dice que un tensor es
\textit{totalmente (anti-)simétrico} si lo es con todo a cualquier de índices. En general, cuando hablemos de tensores (anti-)simétricos nos referimos a tensores totalmente (anti-)simétricos, salvo que se indique lo contrario.

Por ejemplo, si $A_{ij}$ y $B_{ij}$ son las componentes de tensores de tipo
($_2^0$), entonces $A_{ij}$ es un tensor simétrico si:
\begin{equation}
A_{ij}=A_{ji}, \label{sim1}
\end{equation}
y $B_{ij}$ será un tensor antisimétrico si
\begin{equation}
B_{ij}=-B_{ji}. \label{sim2}
\end{equation}

Como consecuencia inmediata de la forma de la ley de transformación de
tensores, estas ecuaciones son válidas en cualquier sistema coordenado.

\begin{quotation}
\textbf{Teorema:} \textit{Todas las propiedades de simetría y de
antisimetría de los tensores (respecto a índices del mismo tipo) son independientes de la elección del sistema coordenado.}
\end{quotation}

Dado un tensor de tipo $(^r_s)$ con $r>1$ ó $s>1$, podemos construir
a partir de dicho tensor, un tensor simétrico y un tensor
antisimétrico respecto a cualquier par de índices del mismo tipo. Por ejemplo, a partir del tensor $A_{ij}$ podemos definir los nuevos tensores
\begin{align}
A_{(ij)} & :=\frac{1}2\left( A_{ij}+A_{ji}\right) ,\label{sim3}\\
A_{[ij]} & :=\frac{1}2\left( A_{ij}-A_{ji}\right) ,\nonumber
\end{align}
que son simétricos y antisimétricos, respectivamente. El proceso
correspondiente a definir $A_{(ij)}$ a partir de $A_{ij}$ es llamado \textbf{simetrización}. Análogamente $A_{[ij]}$ resulta del proceso de \textbf{antisimetrización}.

Vemos también que todo tensor de tipo $(^r_s)$ con $r>1$ ó $s>1$, se
puede expresar como suma de un tensor simétrico y uno antisimétrico respecto a
cualquier par de índices del mismo tipo. En el caso anterior
tenemos:
\begin{equation}
A_{ij}=A_{(ij)}+A_{[ij]}.\label{sim4}
\end{equation}

Es posible extender el proceso de (anti-)simetrización a más de dos índices, por
medio de
\begin{eqnarray}
T_{[ij]}&:=&\frac{1}2(T_{ij}-T_{ji}), \\
T_{[ijk]}&:=&\frac{1}{3}(T_{i[jk]}+T_{j[ki]}+T_{k[ij]}), \label{as3}\\
T_{[ijkl]}&:=&\frac{1}{4}(T_{i[jkl]}-T_{j[kli]}+T_{k[lij]}-T_{l[ijk]}),
\end{eqnarray}
etcétera, y
\begin{eqnarray}
T_{(ij)}&:=&\frac{1}2(T_{ij}+T_{ji}), \\
T_{(ijk)}&:=&\frac{1}{3}(T_{i(jk)}+T_{j(ki)}+T_{k(ij)}), \\
T_{(ijkl)}&:=&\frac{1}{4}(T_{i(jkl)}+T_{j(kli)}+T_{k(lij)}+T_{l(ijk)}),
\end{eqnarray}
etcétera.

\subsection{Invariancia de las ecuaciones tensoriales}

Hemos visto que los tensores pueden ser sumados, restados o, más
generalmente, linealmente combinados con coeficientes escalares. Podemos
formar productos entre tensores y/o contraerlos, definiendo nuevos tensores, siempre
que  ellos estén definidos en el mismo punto de $M$. Todas estas operaciones suministran nuevos tensores.

Un importante tensor de cada tipo es el \textbf{tensor
nulo} de ese tipo. Dicho tensor es numéricamente invariante debido a que la ley de transformación de tensores es \textit{homogénea}. Esto tiene
como consecuencia que una ecuación tensorial como
$S_{\ \ \ \ kl\cdots}^{ij\cdots}=T_{\ \ \ \ kl\cdots}^{ij\cdots}$ sea
válida \textit{con la misma forma} en todo sistema coordenado, ya que es equivalente a afirmar que $S_{\
\ \ \ kl\cdots}^{ij\cdots}-T_{\ \ \ \ kl\cdots}^{ij\cdots}$ es el tensor
nulo.
Este hecho garantiza además el teorema de la invariancia de las propiedades de
simetría y antisimetría de los tensores. Si $S$ y $T$ son de distinto tipo, o
bien, si están definidos en puntos distintos, la ecuación carece de sentido
invariante (independiente de las coordenadas usadas).

% Consideremos ahora un tensor de tipo $(_{s}^r)$, $r$ vectores covariantes,
% $s$ vectores contravariantes, y el siguiente producto contraído:
% \begin{equation}
% S_{\ \ \ \ pq\dots}^{kl\dots}A_k B_l \cdots F^pG^q\cdots .
%\label{inv1}%
% \end{equation}
%
%
% Entonces, de acuerdo con las reglas de producto externo e interno, esta
% multiplicación es un escalar. La proposición inversa es también verdadera:
%Supongamos que no sabemos si un arreglo de números $S_{\ \ \ \ \dots
%}^{\dots}$ tiene caracter tensorial, pero
% sabemos que (\ref{inv1}) es un escalar para cualquier conjunto arbitrario de
% vectores $A\cdots G\cdots $. Entonces $S_{\ \ \dots}^{\dots}$ son las
%componentes de un tensor
% del tipo definido por sus índices.
%
% En efecto, para probar esto consideremos una transformación particular y
% llamemos $\bar{S}_{\ \ \dots}^{\dots}$ a las componentes de $S$
%transformadas como si
% fuera un tensor. Llamemos $\tilde{S}_{\ \ \dots}^{\dots}$ a cualquier
%conjunto de
% números que comparte con $\bar{S}_{\ \ \dots}^{\dots}$ la propiedad de
%hacer
% (\ref{inv1}) un escalar. Entonces tenemos que
% \begin{align}
% \bar{S}_{\ \ \ \ pq\dots}^{kl\dots}\bar{A}_k \bar{B}_l \cdots
%\bar{F}^p\bar{G}^q\cdots  &=S_{\ \ \ \ pq\dots}^{kl\dots}A_k B_l \cdots
%F^pG^q\cdots , \label{inv2}\\
% \tilde{S}_{\ \ \ \ pq\dots}^{kl\dots}\bar{A}_k \bar{B}_l \cdots
%\bar{F}^p\bar{G}^q\cdots
% & =S_{\ \ \ \ pq\dots}^{kl\dots}A_k B_l \cdots F^pG^q\cdots .
% \end{align}
% Estas ecuaciones expresan que $\bar{S}_{\ \ \dots}^{\dots}$ y $\tilde{S}_{\
%\ \dots}^{\dots}$
% dejan (\ref{inv1}) invariante. Restándolas obtenemos:
% \begin{equation}
% \left( \bar{S}_{\ \ \ \ pq\dots}^{kl\dots}-\tilde{S}_{\ \ \ \ pq\dots
%}^{kl\dots}\right) \bar{A}%
% _k \bar{B}_l \dots \bar{F}^p\bar{G}^q\dots  =0 .\label{inv3}
% \end{equation}
% Usando el supuesto que los vectores $A\cdots G\dots $ son arbitrarios,
%encontramos que
% \begin{equation}
% \bar{S}_{\ \ \ \ pq\dots}^{kl\dots}-\tilde{S}_{\ \ \ \ pq\dots}^{kl\dots}
%=0,
% \end{equation}
% es decir, que los tensores $\bar{S}$ y $\tilde{S}$ son iguales:
% \begin{equation}
% \bar{S}_{\ \ \ \ pq\dots}^{kl\dots}  =\tilde{S}_{\ \ \ \ pq\dots}^{kl\dots
%}.
% \end{equation}
% Por lo tanto, bajo las hipótesis mencionadas $S$ debe ser un tensor.

 \section{Densidades tensoriales*}

 Consideremos un campo vectorial contravariante $A^i $ y la integral:
 \begin{equation}
 \int_\Omega A^i d^nx, \label{den1}%
 \end{equation}
 donde $d^nx:=dx^1dx^2\cdots dx^n$ y la integral se toma sobre una
 región $\Omega$ dada de $M$. Observamos que el resultado de esta integral
 \textit{no constituye un vector bajo TGC's} (ya que básicamente se están
 sumando vectores definimos en distintos puntos de la variedad, que bajo una
 TGC transforman de forma distinta en cada punto). Del mismo modo,
 consideremos la integral de un campo escalar $A$:
 \begin{equation}
 I=\int_\Omega A\,d^nx. \label{den2}%
 \end{equation}
 Veamos cómo transforma bajo $I$ bajo una TGC. El elemento de volumen
 transforma de acuerdo al teorema de cambio de variables para integrales de
 Riemann múltiples:
 \begin{equation}
 d^nx=\left| \frac{\partial x^k}{\partial\bar{x}^j}\right| d^n
 \bar{x} ,
 \end{equation}
 donde $\left| \frac{\partial x^k}{\partial\bar{x}^j}\right|$ es el módulo
 del jacobiano de la transformación. Luego:
 \begin{equation}
 I=\int_\Omega A\,d^nx=\int_\Omega\bar{A}\left| \frac{\partial x^k
}{\partial\bar{x}^j %
}\right| d^n\bar{x}\neq\int_\Omega\bar{A}\,d^n\bar{x}=\bar{I}.
 \label{den3}%
 \end{equation}
 De este modo $I$ \textit{no es invariante bajo una TGC} (de hecho, $I$ y
 $\bar{I}$ ni siquiera son proporcionales, en general). Pero nos interesa poder
 definir integrales de la forma (\ref{den2}) cuyo resultado sí sea un
 escalar, es decir, independiente del SC en el que se calcule. Esto implica
 entonces que, para que $I$ sea un escalar, $A$ no puede ser un campo escalar.
 Por otra parte,
 sabemos que si sumamos sólo cantidades escalares el resultado es un escalar.
 En nuestro caso tenemos una integral en vez de una suma. Luego, si
 $A\,d^nx$ es
 un escalar entonces $I$ será un escalar. Para esto la ley de transformación
 de $A$ no debe ser $A=\bar{A}$, sino\footnote{Desde ahora, denotaremos a las
 densidades (tensoriales) usando letras cursivas.}
 \begin{equation}
 {\cal A}=\left| \frac{\partial\bar{x}^i}{\partial x^k}\right| \bar{{\cal
 A}},
 \label{den4}%
 \end{equation}
 de modo que
 \begin{equation}
 I=\int_\Omega {\cal A}\,d^nx=\int_\Omega\bar{{\cal A}}\left|
 \frac{\partial\bar{x}^i}{\partial x^k %
}\right| \left| \frac{\partial x^k}{\partial\bar{x}^j}\right|
 d^n\bar{x}
 =\int_\Omega\bar{{\cal A}}\,d^n\bar{x}=\bar{I}. \label{den5}%
 \end{equation}
 Ahora $I$ es un escalar, pero ${\cal A}$ no lo es. Esto motiva la siguiente
 definición:

 \textbf{Definición:} Una cantidad $\rho$ es llamada \textit{densidad escalar
 de peso} $p$ si bajo una TGC ésta obdece la ley:
 \begin{equation}
 \bar{\rho}=\left| \frac{\partial x^i}{\partial\bar{x}^j}\right|^p%
 \rho. \label{den6}%
 \end{equation}
 De lo anterior se concluye que $d^nx$ es una densidad escalar de peso $-1$,
 y que la integral de una densidad escalar de peso $+1$ es un escalar.

 \textbf{Definición:} Un conjunto de $n^{r+s}$ cantidades ${\cal A}_{\ \ \ \ \
 k_1\cdots k_{s}}^{i_1\cdots i_{r}}(P)$ son las componentes de una densidad
 tensorial de peso $p$ y tipo $(^r_s)$ en un punto $P$, si bajo una TGC dichas
 componentes transforman según la ley:
 \begin{equation}
 \boxed{\bar{{\cal A}}_{\ \ \ \ \ k_1\cdots k_{s}}^{i_1\cdots
 i_{r}}(P)=\left| \frac{\partial
 x^i}{\partial\bar{x}^k}\right|^p(P) \frac{\partial\bar{x}^{i_1}%
}{\partial x^{l_1}}(P)\cdots \frac{\partial\bar{x}^{i_{r}}}{\partial
 x^{l_{r}}}(P)\frac{\partial x^{m_1}}{\partial\bar{x}^{k_1}}(P)\cdots
 \frac{\partial x^{m_{s}}}{\partial\bar{x}^{k_{s}}}(P){\cal A}_{\ \ \ \ \
 m_1\cdots m_{s}}^{l_1\cdots l_{r}}(P).}
 \label{den7}%
 \end{equation}
 No se debe inferir que la integral de las componentes de una densidad
 tensorial es un tensor, pues no lo es (salvo el caso de una densidad escalar).

 Consecuencias inmediatas de la definición de densidades tensoriales son:
 \begin{itemize}
 \item Si todas las componentes de una densidad son nulas en un SC, entonces
 son nulas en cualquier SC. Como consecuencia, las ecuaciones entre densidades
 son independientes del SC.
 \item La suma o diferencia de densidades del mismo tipo y referidas al mismo
 punto es otra densidad del mismo tipo.
 \item El producto de una densidad tensorial de tipo $(^r_s)$ y peso $p$ por
 una densidad tensorial de tipo $(^t_u)$ y peso $q$ es una densidad tensorial
 de tipo $(^{r+t}_{s+u})$ y peso $p+q$.
 \item La contracción de índices puede realizarse para densidades
 tensoriales de tipo $(^r_s)$ con $r,s\geq1$, del mismo modo que para tensores,
 resultando una densidad de tipo ($_{s-1}^{r-1}$) y del mismo peso.
 \item Note que si ${\cal J}^i$ son componentes de una densidad vectorial contravariante de peso $+1$ entonces $\partial_i {\cal J}^i$ es una densidad escalar de peso $+1$. Contraste eso con el conocido hecho que $\partial_iA^i$ \textit{no es un escalar} si $A^i$ son componentes de un vector contravariante.
 \end{itemize}

 \subsection{Densidades tensoriales de Levi-Civita*}
 Considere ahora un tensor $T_{i_1\cdots i_n}$ de tipo ($_n^0$)
 \textit{totalmente
 antisimétrico}. Si denotamos el valor numérico de $T_{12\dots n}$ por
 ${\cal T}$, entonces cualquier otra componente $T_{i_1\cdots i_n}$ tendrá
 valor
 $\pm{\cal T}$ de acuerdo a si la permutación $i_1\cdots i_n$ es par o
 impar, o
 bien será nula si se repite un índice. Escribamos la ley de transformación
 para la componente $T_{12\cdots n}$:
 \begin{equation}
 \bar{T}_{12\cdots n}=\frac{\partial
 x^{k_1}}{\partial\bar{x}^1}\frac{\partial
 x^{k_2}}{\partial\bar{x}^2}\cdots \frac{\partial x^{k_n}}{\partial\bar
 {x}^n}\,T_{k_1\cdots k_n}. \label{den8}%
 \end{equation}
 Escribiendo explícitamente la suma y usando las propiedades
 antisimétricas de $T$ se obtiene:
 \begin{equation}
 \bar{T}_{12\cdots n}=\left| \frac{\partial x^k}{\partial\bar{x}^i}\right|
 T_{12\cdots n}, \label{den9}%
 \end{equation}
 es decir,
 \begin{equation}
 \bar{{\cal T}}=\left| \frac{\partial x^k}{\partial\bar{x}^i}\right| {\cal
 T}.
 \label{den10}%
 \end{equation}
 Esto significa que el valor de una componente (no nula) de un tensor
 antisimétrico de tipo ($_n^0$) es una densidad escalar de tipo $+1$. Por
 otro lado, ya que todos los valores de $T_{i_1\cdots i_n}$ son
 proporcionales a $T_{12\cdots n}={\cal T}$, podemos escribir que
 \begin{equation}
 T_{i_1\cdots i_n}={\cal T}\hat{\epsilon}_{i_1\cdots i_n},
 \label{TcalT}
 \end{equation}
 donde $\hat{\epsilon}_{i_1\cdots i_n}$ denota un objeto que es definido
 tal que es totalmente antisimétrico y que \textit{en todo SC}
 $\hat{\epsilon}_{12\cdots n}:=1$. Ya que $T_{i_1\cdots i_n}$ es un tensor
 (es decir, una densidad tensorial de peso $0$) y ${\cal T}$ es una densidad
 tensorial de peso $+1$ concluimos de (\ref{TcalT}) que
 $\hat{\epsilon}_{i_1\cdots i_n}$ \textbf{es una densidad tensorial de
 peso} $-1$, llamada \textbf{densidad tensorial de Levi-Civita}.

 Análogamente, podemos descomponer cualquier tensor contravariante totalmente antisimétrico  $A^{i_1\cdots i_n}$ en
 \begin{equation}
 A^{i_1\cdots i_n}={\cal A}\,\epsilon^{i_1\cdots i_n}, \label{AcalA}
 \end{equation}
 donde ${\cal A}:=A^{1\cdots n}$ es una densidad escalar de peso $-1$ y
 $\epsilon^{i_1\cdots i_n}$ es la densidad de Levi-Civita de peso $+1$,
 definida como totalmente antisimétrica y tal que
 \textit{en todo SC} $\epsilon^{1\cdots n}:=1$.

 Las densidades tensoriales de Levi-Civita son cantidades numéricamente
 invariantes que pueden ser definidas en cualquier variedad.


 En el caso que $n=4$, ellas satisfacen las siguientes identidades:
 \begin{eqnarray}
 \epsilon^{ijkl}\, \hat{\epsilon}_{mnpq}&=&4!\, \delta^i_{[m}\delta^j_n
 \delta^k_p \delta^l_{q]}, \\
 \epsilon^{ijkl}\, \hat{\epsilon}_{mnpl}&=&3!\, \delta^i_{[m}\delta^j_n
 \delta^k_{p]} , \\
 \epsilon^{ijkl}\, \hat{\epsilon}_{mnkl}&=&2!\, \left( \delta^i_m\delta^j_n -
 \delta^j_m \delta^i_n\right), \label{idep2} \\
 \epsilon^{ijkl}\, \hat{\epsilon}_{mjkl}&=&3!\, \delta^i_m, \label{idep3}\\
 \epsilon^{ijkl}\, \hat{\epsilon}_{ijkl}&=&4! .
 \end{eqnarray}

 \section{Tensores antisimétricos y densidades tensoriales duales*}
 Consideremos una variedad de 4 dimensiones. Si disponemos de un tensor
 antisimétrico de tipo ($_2^0$), $A_{ij}$, que tendrá en general 6
 componentes linealmente independientes, podemos definir una densidad tensorial
 antisimétrica de peso $+1$, por
 \begin{equation}
 {\cal A}^{ij}:=\frac{1}2\,\epsilon^{ijkl}A_{kl} .\label{den13}
 \end{equation}
 Puede verificarse que ${\cal A}_{ij}$ es equivalente a $A_{ij}$ en el sentido
 que contiene la misma información. En efecto, usando la identidad
 (\ref{idep2}) podemos encontrar que
 \begin{equation}
 A_{ij} :=\frac{1}{2!}\,\epsilon_{ijkl}{\cal A}^{kl}.
 \end{equation}
 Por tanto, es posible considerar en forma alternativa, pero equivalentemente,
 al tensor antisimétrico $A_{ij}$ y a la densidad antisimétrica de peso
 $+1$ ${\cal A}^{ij}$, que es usualmente llamado \textit{dual de} $A_{ij}$

 Análogamente (para $n=4$) existe una relación 1 a 1 entre tensores
 totalmente antisimétricos de tipo $(^0_3)$ $A_{ijk}$ y densidades
 (``duales'') vectoriales
 contravariante de peso $+1$ ${\cal A}^i$ definidas por ${\cal
 A}^i:=\frac{1}{3!}\epsilon^{ijkl}A_{jkl}$, ya que
 $A_{jkl}:=\epsilon_{ijkl}{\cal A}^i$, como puede verificarse usando
 (\ref{idep3}).

 En general, puede establecerse el siguiente resultado:

 \begin{quotation}
 \textbf{Teorema:} A todo tensor antisimétrico $T_{i_1\dots i_{p}}
 $ de tipo $(^0_p)$ ($p\leq n$) se le puede hacer corresponder una densidad
 tensorial ${\cal T}^{j_1\dots j_{n-p}}$ de tipo $(_{\ \, 0}^{n-p})$ y peso
 $+1$, cuyas componentes contienen las componentes independientes del tensor.
 Las componentes de ${\cal T}^{j_1\dots j_{n-p}}$ están dadas por:
 \begin{equation}
 {\cal T}^{j_1\dots j_{n-p}}:=\frac{1}{p!}\epsilon^{j_1\dots
 j_{n-p}i_1\dots i_{p}}\,T_{i_1\dots i_{p}}, \label{den16}%
 \end{equation}
 y recibe el nombre de \textbf{densidad adjunta o dual} del tensor original.
 Además se satisface que
 \begin{equation}
 T_{i_1\dots i_{p}}=\frac{1}{(n-p)!}\epsilon_{j_1\dots j_{n-p}i_1\dots
 i_{p}}\,{\cal T}^{j_1\dots j_{n-p}}.
 \end{equation}
 Finalmente, una equivalencia similar es válida entre tensores antisimétricos
 de tipo $(^p_0)$ y densidades tensoriales antisimétricas de tipo $(^{\ \,
 0}_{n-p})$ y peso $-1$.
 \end{quotation}

\section{Métrica}

La métrica es un objeto geométrico que permite introducir el
concepto de \textbf{distancia} en una variedad.

\begin{quotation}
\textbf{Definición:} Se denomina \textbf{espacio métrico} al
espacio que cuenta con una ley para definir distancias.
\end{quotation}

Sean $x^i $ y $x^i +dx^i $ las coordenadas de dos puntos infinitesimalmente
cercanos $P$ y $Q$ en una variedad, respecto a un SC $x$. La expresión para la distancia $d\ell$ entre estos puntos puede (en principio) ser muy general, pero se exige que sea un escalar, es decir, que su valor sea independiente del sistema coordenado. El caso más estudiado es el que supone que la distancia entre dichos puntos
está dada por una expresión de la forma:
\begin{equation}\marginnote{métrica y elem. de línea}
\boxed{d\ell^2:= g_{ij}(x)dx^i dx^j ,} \label{g1}
\end{equation}
donde los coeficientes $g_{ij}$ son funciones de las coordenadas $x^i$.
Además, por definición, $g_{ij}$ son las componentes de un
tensor covariante de segundo orden, simétrico\footnote{... por lo que, en general, tiene $n(n+1)/2$ componentes linealmente independientes.} y recibe el nombre de
\textbf{tensor métrico}. Esto asegura que $d\ell$ sea efectivamente un
escalar y se denomina \textbf{elemento de línea}.

Consideraremos espacios métricos ``no degenerados'', en los cuales la distancia
elemental se define por una expresión de la forma (\ref{g1}) con $g:=\det(g_{ij})\neq 0$.

\subsection{Longitud, producto interior, ángulos}
Con el tensor métrico es posible definir la longitud de una curva, el módulo de un vector, el producto escalar entre vectores, y en ángulo entre vectores.
\begin{quotation}
\textbf{Definición:} Sea $\cal C$ una curva en la variedad, parametrizada por $x^i =x^i(\lambda)$ en un SC $x$, con $\lambda\in[\lambda_i,\lambda_f]$. Definimos la
\textbf{longitud de la curva} como el escalar
\begin{equation}\marginnote{Longitud de curva}
\boxed{L:=\int_{P_i}^{P_f}d\ell=\int_{P_i}^{P_f}\sqrt
{g_{ij}dx^i dx^j}=\int_{\lambda_i}^{\lambda_{f}}\sqrt{g_{ij}\frac{dx^i}{d\lambda}\frac{dx^j}{d\lambda}}\,d\lambda,}
\label{g2}
\end{equation}
donde $P_i$ y $P_f$ son los puntos de $M$ asociados a los extremos de la curva: $x^i(P_i)=x^i(\lambda_i)$, $x^i(P_f)=x^i(\lambda_f)$.
\end{quotation}

\begin{quotation}
\textbf{Definición:} Sean $A^i $ y $B^i $ dos vectores contravariantes
definidos en un mismo punto $P$ de la variedad. Se define el \textbf{producto escalar}
entre dichos vectores como:
\begin{equation}\marginnote{Prod. escalar entre vectores}
\boxed{A\cdot B:= g_{ij}A^i B^j .} \label{g4}
\end{equation}
\end{quotation}

Es importante recordar que en la ecuación (\ref{g3}) $g_{ij}$, $A^i $ y $B^j $
deben estar evaluados en el mismo punto $P$.

\begin{quotation}
\textbf{Definición:} Sea $A^i $ un vector contravariante definido en un
punto $P$ de la variedad. Llamamos \textbf{módulo (o norma) de} $A^i $ al
escalar $\left\vert A\right\vert$ definido por
\begin{equation}\marginnote{Módulo de vect. contrav.}
\boxed{\left\vert A\right\vert^2:= A\cdot A=g_{ij}A^i A^j .} \label{g3}
\end{equation}
\end{quotation}

\begin{quotation}
\textbf{Definición:} Se define el \textbf{ángulo $\sphericalangle(A,B)$ entre
los vectores} $A^i $ y $B^i $ (de norma no nula) mediante la relación:
\begin{equation}\marginnote{ángulo entre vectores}
\boxed{\cos\sphericalangle(A,B):=\frac{A\cdot B}{\left\vert A\right\vert
\left\vert
B\right\vert}.} \label{g5}
\end{equation}
\end{quotation}

% Por ejemplo, en el espacio euclidiano $n$-dimensional y en coordenadas
% cartesianas la métrica es $g_{ij}=diag(+1,\cdots,+1)$, de modo que:%
% \begin{equation}
% ds^2\overset{\ast}{=}\sum_{i=1}^n dx^i dx^i . \label{gej1}%
% \end{equation}
% La longitud de una curva $\cal C$ está dada por:%
% \begin{equation}
% L\overset{\ast}{=}\int_{\lambda i}^{\lambda_{f}}\sqrt{\sum_{i=1}^n \frac{dx^i
%}{d\lambda}\frac{dx^i}{d\lambda}}d\lambda . \label{gej2}%
% \end{equation}
% El módulo de un vector $A$, el producto escalar de dos vectores $A$ y $B$
% y el ángulo entre ellos están dados por:%
% \begin{align}
% \left\vert A\right\vert^2 & \overset{\ast}{=}\sum_{i=1}^n A^i A^i , %
% \label{gej3}\\
% A\cdot B & \overset{\ast}{=}\sum_{i=1}^n A^i B^i ,\\
% \cos\sphericalangle(A,B) & \overset{\ast}{=}\frac{\sum_{i=1}^n A^i B^i
%}{\sqrt{\sum_{j=1}^nA^j A^j}\sqrt{\sum_{k=1}^nB^{k}B^{k}}}.
% \end{align}

\subsection{Métrica Inversa}
En el caso de espacios métricos no degenerados, en los que $g:=\det(g_{ij})\neq0$ en cada punto, está asegurada la existencia de un tensor contravariante $g^{ij}(x)$ tal que:%
\begin{equation}\marginnote{Métrica inversa}
\boxed{g^{ij}(x)\,g_{jk}(x)=\delta_k^i .} \label{g6}%
\end{equation}
Este tensor recibe el nombre de \textbf{métrica inversa}\footnote{$g^{ij}=(1/(n-1)!g)\varepsilon^{ii_2i_3\cdots i_n}\varepsilon^{jj_2j_3\cdots j_n}g_{i_2j_2}g_{i_3j_3}\cdots g_{i_nj_n}$, $g:=(1/n!)\varepsilon^{i_1i_2\cdots i_n}\varepsilon^{j_1j_2\cdots j_n} g_{i_1j_1}g_{i_2j_2}\cdots g_{i_nj_n}$.} y es único, en cada SC, dadas las componentes de la métrica $g_{ij}(x)$.

Consideremos ahora un vector contravariante $A^i $. Es posible definir un
vector covariante asociado, $A_i'$, como:%
\begin{equation}\marginnote{``bajando'' índices}
A_i ':= g_{ij}A^j. \label{g7}
\end{equation}
Análogamente, con $g^{ij}$ podemos definir un vector contravariante asociado, $A^{\prime\prime i}$, como:
\begin{equation}\marginnote{``subiendo'' índices}
A^{\prime\prime i}:= g^{ij}A_j '. \label{g8}
\end{equation}
De este modo vemos que, usando la métrica y su inversa, podemos mapear vectores
contravariantes (pertenecientes al espacio tangente) en vectores covariantes (pertenecientes al espacio cotangente) y viceversa.

Además, si usamos (\ref{g7}) en (\ref{g8}) obtenemos:
\begin{equation}
A^{\prime\prime i}=g^{ij}A_j '=g^{ij}g_{ j k
}A^{k}=\delta_k^i A^{k}=A^i , \label{g9}
\end{equation}
que prueba que hay una relación uno a uno entre vectores del espacio
tangente y cotangente. 

En espacios métricos es posible (y se acostumbra) no distinguir mayormente (en cuanto a su interpretación) los vectores covariantes de sus ``duales'' contravariantes, y viceversa. Por el contrario, se acostumbra decir que los vectores son ``objetos geométricos'', cada uno de los cuales puede expresarse tanto por sus componentes covariantes como contravariantes. A esto se suma el hecho que el espacio euclideano $E_n$ es un espacio métrico en el cual $g_{ij}\overset{\ast}{=}\delta_{ij}=diag(+1,+1,\cdots)$ en coordenadas cartesianas, de modo que las componentes covariantes y contravariantes de cualquier vector coinciden numéricamente (pero sólo en coordenadas cartesianas!). De aquí que en tales espacios no sea necesaria la distinción entre estos dos tipos de vectores \textit{cuando se trabaja en coordenadas cartesianas}.
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=6cm]{fig/fig-espacios-tangente-y-cotangente.pdf}}
\caption{El tensor métrico define una relación uno a uno (isomorfismo) entre
los espacios tangente y cotangente en cada punto de la variedad.}
\label{5}
\end{figure}
\end{center}

El uso del tensor métrico para ``subir'' o ``bajar'' índices también es útil para tensores de mayor rango. Por ejemplo, a partir del tensor de tipo $(^2_2)$, $T^{kl}{}_{i j}$, podemos definir el tensor de tipo $(^3_1)$:
\begin{equation}
T^{kl i}{}_j:=g^{im}T^{kl}{}_{mj},
\label{g10}
\end{equation}
etc.

\subsection{Métrica inducida}
Considere una variedad $M$ de dimensión $n$ y una \textbf{subvariedad} $S$ de dimensión $d<n$. Entonces la métrica de $M$ \textit{induce la métrica sobre} $S$, ya que la distancia entre dos puntos arbitrarios de $S$ puede calcularse aplicando (o restringiendo) la distancia entre puntos infinitesimalmente próximos de $M$ al caso particular de dos puntos infinitesimalmente próximos de $S$. Si se usan coordenadas $x^i$ en (una vecindad de) $M$, y la subvariedad $S$ es definida por medio de una \textbf{parametrización} de la forma
\begin{equation}
x^i=x^i(y^a), \qquad a=1,\dots,d,
\end{equation}
donde $y^a$ son \textit{coordenadas sobre la subvariedad} $S$, entonces la diferencia de coordenadas de $M$ correspondiente a dos puntos infinitesimalmente próximos de $S$ puede expresarse como
\begin{equation}
\left.dx^i\right|_{S}=\frac{\partial x^i}{\partial y^a}(x(y))\,dy^a.
\end{equation}
Con esto, podemos expresar el elemento de línea sobre $S$ como
\begin{align}
\left.dl^2\right|_{S}=&\left.g_{ij}(x)\right|_{S}\left.dx^i\right|_{S}\left.dx^j\right|_{S} \\
=& g_{ij}(x(y))\left(\frac{\partial x^i}{\partial y^a}dy^a\right)\left(\frac{\partial x^j}{\partial y^b}dy^b\right) \\
=& g_{ab}(y)dy^ady^b.
\end{align}
De esta forma, las componentes del tensor métrico inducido sobre $S$, en coordenadas $y^a$ está dadas por
\begin{equation}\marginnote{métrica inducida}\label{metind2}
\boxed{g_{ab}(y)= g_{ij}(x(y))\frac{\partial x^i}{\partial y^a}\frac{\partial x^j}{\partial y^b}.}
\end{equation}

Note que esta relación \textit{no es} simplemente la usual ley de transformación de las componentes del tensor métrico bajo una TGC, ya que $g_{ij}$ y $g_{ab}$ \textit{tienen dimensiones distintas} ($n$ y $d<n$ respectivamente) y son componentes de representan las \textit{métricas de dos variedades distintas} (pero relacionadas).

\subsection{Curvas geodésicas}

Consideremos dos puntos $P$ y $Q$ en una variedad provista de métrica.
Se define la \textbf{geodésica} como la curva \textit{de longitud mínima  (extrema) entre} $P$ y $Q$.

Si $\cal C$ es una curva representada paramétricamente por
$x^i =x^i (\lambda)$, entonces $\cal C$ es una geodésica entre $P$ y $Q$ si y
sólo si\footnote{Suponemos, por simplicidad, pero sin perder generalidad, que el parámetro $\lambda$ \textit{crece} desde $P$ a $Q$, de modo que en la integral $d\lambda>0$.}
\begin{equation}
L=\int_P^Qd\ell= \int_P^Q\sqrt{g_{ij}(x(\lambda))dx^idx^j}=\int_{\lambda i}^{\lambda_{f}}\sqrt{g_{ij}(x(\lambda))\frac{dx^i}{d\lambda}\frac{dx^j}{d\lambda}}\, d\lambda \label{gm1}
\end{equation}
es mínima. Esto implica que su variación respecto a pequeñas desviaciones de la trayectoria geodésica es nula:
\begin{equation}
\delta L=0. \label{gm2}
\end{equation}
La condición (\ref{gm2}) implica que las geodésicas satisfacen las ecuaciones de Euler-Lagrange,
\begin{equation}
\frac{\delta L}{\delta x^i}=\frac{\partial \tilde{L}}{\partial x^i}-\frac
{d}{d\lambda}\left( \frac{\partial
\tilde{L}}{\partial\left(\frac{dx^i}{d\lambda}\right)}\right) =0,
\label{gm3}
\end{equation}
donde
\begin{equation}
\tilde{L}(x^i,\frac{dx^i}{d\lambda})=\sqrt{g_{ij}(x(\lambda))\frac{dx^i
}{d\lambda}\frac{dx^j}{d\lambda}}.
\label{gm4}
\end{equation}
De aquí, tenemos que
\begin{equation}
\frac{\partial\tilde{L}}{\partial x^k}=\frac{1}{2}(g_{lm}\dot{x}^l\dot{x}^m)^{-1/2}(\partial_k g_{ij})\dot{x}^i\dot{x}^j=\frac{1}{2\tilde{L}}(\partial_k g_{ij})\dot{x}^i\dot{x}^j,
\end{equation}
\begin{equation}
\frac{\partial\tilde{L}}{\partial \dot{x}^k}=\frac{1}{2}(g_{lm}\dot{x}^l\dot{x}^m)^{-1/2}(2g_{kj}\dot{x}^j)=\frac{1}{\tilde{L}}g_{kj}\dot{x}^j,
\end{equation}
\begin{equation}
\frac{d}{d\lambda}\left(\frac{\partial\tilde{L}}{\partial \dot{x}^k}\right)= -\frac{\dot{\tilde{L}}}{\tilde{L}^2}g_{kj}\dot{x}^j+\frac{1}{\tilde{L}}(\partial_l g_{kj})\dot{x}^j\dot{x}^l+\frac{1}{\tilde{L}}g_{kj}\ddot{x}^j.
\end{equation}
De esta forma, encontramos que
\begin{equation}
\frac{\delta L}{\delta x^k}=\frac{1}{2\tilde{L}}(\partial_k g_{ij})\dot{x}^i\dot{x}^j+\frac{\dot{\tilde{L}}}{\tilde{L}^2}g_{kj}\dot{x}^j-\frac{1}{\tilde{L}}(\partial_l g_{kj})\dot{x}^j\dot{x}^l-\frac{1}{\tilde{L}}g_{kj}\ddot{x}^j.
\end{equation}
Multiplicando por $\tilde{L}g^{ki}$ y reordenando términos obtenemos
\begin{align}
\frac{\dot{\tilde{L}}}{\tilde{L}}\dot{x}^i &= \ddot{x}^i+g^{ik}(\partial_l g_{kj})\dot{x}^j\dot{x}^l-\frac{1}{2}(\partial_k g_{jl})\dot{x}^j\dot{x}^l \\
&= \ddot{x}^i+\frac{1}{2}g^{ik}\left(\partial_l g_{kj}+\partial_j g_{kl}-\partial_k g_{jl}\right)\dot{x}^j\dot{x}^l \\
&= \ddot{x}^i+\left\{ _{jl}^{\,i}\right\}\dot{x}^j\dot{x}^l.
\end{align}
Por lo tanto \eqref{gm3} es equivalente a 
\begin{equation}\marginnote{Ecuación Geodésicas}
\boxed{\frac{d^2x^i}{d\lambda^2}+\left\{ _{ j l}^{\, i}\right\}
\frac{dx^j}{d\lambda}\frac{dx^{ l}}{d\lambda}=f(\lambda)\frac{dx^{ i
}}{d\lambda},} \label{gm5}
\end{equation}
donde
\begin{equation}\marginnote{Símbolos de Christoffel}
\boxed{\left\{ _{ jk}^{\,i}\right\}:=\frac{1}2g^{il}\left[
\partial_kg_{jl}+\partial_j g_{lk}-\partial_l g_{kj}\right] ,} \label{gm6}
\end{equation}
es llamado \textbf{símbolo de Christoffel}\footnote{En honor a Elwin Bruno Christoffel: 1829-1900, físico y matemático alemán. Ver \url{http://es.wikipedia.org/wiki/Elwin_Bruno_Christoffel}.} (de segunda especie), y
\begin{equation}
f(\lambda):=\frac{\dot{\tilde{L}}}{\tilde{L}}=\frac{d\ }{d\lambda}\left(\ln\tilde{L}\right)=\frac{d\ }{d\lambda}\left(\ln\frac{d\ell}{d\lambda}\right) =\frac{\frac{d^2\ell}{d\lambda^2}}{\frac{d\ell}{d\lambda}}\label{efe}
\end{equation}
 es una función que depende de la elección del parámetro $\lambda$ usado para parametrizar la curva. Note que aquí hemos implícitamente introducido una función $\ell(\lambda)$, el largo de la curva desde el punto inicial $P_i$, dada por $\ell(\lambda):=\int_{\lambda_i}^\lambda \tilde{L}\,d\lambda$, de modo que $\tilde{L}=d\ell/ d\lambda$.

La relación (\ref{efe}) determina la función $f$ a partir de la relación
entre la longitud propia de la curva y el parámetro $\lambda$. De aquí es
directo verificar que un cambio de parámetro $\lambda\to\lambda'$ implicará
un cambio en la función $f$, de modo que
\begin{equation}\label{fpf}
f'=\left(\frac{d\lambda'}{d\lambda}\right)^{-1}\left[f-\frac{d\ }{d\lambda}\left(\ln\frac{d\lambda'}{d\lambda}\right)\right] .
\end{equation}
En efecto, como
\begin{equation}
\frac{d\ell}{d\lambda'}=\frac{d\ell}{d\lambda}\frac{d\lambda}{d\lambda'},
\end{equation}
\begin{equation}
\frac{d^2\ell}{d\lambda'^2}=\frac{d^2\ell}{d\lambda^2}\left(\frac{d\lambda}{d\lambda'}\right)^2+\frac{d\ell}{d\lambda}\frac{d^2\lambda}{d\lambda'^2},
\end{equation}
entonces
\begin{align}
f'&=\frac{\frac{d^2\ell}{d\lambda'^2}}{\frac{d\ell}{d\lambda'}} = \frac{\frac{d^2\ell}{d\lambda^2}\left(\frac{d\lambda}{d\lambda'}\right)^2+\frac{d\ell}{d\lambda}\frac{d^2\lambda}{d\lambda'^2}}{\frac{d\ell}{d\lambda}\frac{d\lambda}{d\lambda'}} =f\frac{d\lambda}{d\lambda'}+\frac{\frac{d^2\lambda}{d\lambda'^2}}{\frac{d\lambda}{d\lambda'}}
=f\frac{d\lambda}{d\lambda'}+\frac{d}{d\lambda'}\ln\left(\frac{d\lambda}{d\lambda'}\right),
\end{align}
que es equivalente a \eqref{fpf}.

Como consecuencia de la arbitrariedad de $\lambda$ es posible \textit{elegir parámetros especiales que simplifiquen los cálculos}. Por ejemplo, \textit{si $d\ell\neq 0$ sobre toda la geodésica}, entonces podemos usar un \textbf{parámetro afín}, de la forma $\lambda_{\rm afin}:=\alpha\cdot \ell+\beta$, donde $\ell$ es la distancia \textit{sobre la curva} desde un punto de referencia hasta un punto arbitrario de ella, y además $\alpha$ y $\beta$ son \textit{constantes} (escalares). En este caso (\ref{efe}) implica que $f=0$. Por simplicidad, usualmente se elige en estos casos la longitud (propia) de la curva ($\alpha=1$), de modo que la ecuación de la geodésica adopta la forma ``canónica":
\begin{equation}\marginnote{Ec. Geodésica c/param. afín}
\boxed{\frac{d^2x^i}{d\ell^2}+\left\{ _{ j l}^{\, i}\right\}
\frac{dx^j}{d\ell}\frac{dx^{ l}}{d\ell}=0.} \label{gm7}%
\end{equation}

Por ejemplo, en el espacio euclidiano y en coordenadas cartesianas
$\left\{_{ j l}^{\, i}\right\}\overset{\ast}{=}0$, de modo que, de acuerdo con
(\ref{gm7}), las geodésicas de este espacio son rectas con ecuaciones
$x^i (\ell)\overset{\ast}{=}x_0^i +\ell v_0^i $, con $x_0^i $ y $v_0^i$ constantes.

A partir de la ley de transformación del tensor métrico puede verificarse fácilmente que \textit{los símbolos de Christoffel no son componentes de un tensor bajo TGC's}, ya que
\begin{equation}
\overline{\left\{_{j k}^{\,\, i}\right\}}=\frac{\partial\bar{x}^i}{\partial x^l}\frac{\partial
x^p}{\partial\bar{x}^j}\frac{\partial x^q}{\partial\bar{x}^k}\,
\left\{_{p q}^{\,\, l}\right\} +\frac{\partial\bar{x}^i}{\partial x^l}\frac{\partial
^2x^l}{\partial\bar{x}^j \partial\bar{x}^k}. \label{dinv7}
\end{equation}

\subsection{Isometrías (simetrías de la métrica)}
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=5cm]{fig/fig-Killing.pdf}}
\caption{Trasladando puntos en la dirección $\xi^i(x)$.}
\label{fig:Killing}
\end{figure}
\end{center}
Las \textbf{isometrías} de una métrica (en caso de que existan) describen \textbf{direcciones de simetría} de la métrica en (una región de) la variedad, de una forma covariante, es decir, independiente del sistema de coordenadas usado. 

Sean $P$ y $Q$ dos puntos infinitesimalmente cercanos, con coordenadas $x^i$ y $x^i+dx^i$ respectivamente y $\xi^i=\xi^i(x)$ un campo vectorial (contravariante) definido en una vecindad que incluye a $P$ y $Q$. Como se muestra en la figura, podemos localizar dos nuevos puntos $P'$ y $Q'$ cuyas coordenadas están dadas por:%
\begin{align}
x^i(P')  & = x^i(P)+\varepsilon\,\xi^i(P)\label{is1},\\
x^i(Q')  & = x^i(Q)+\varepsilon\,\xi^i(Q), \label{is1b}
\end{align}
donde $\varepsilon$ es un parámetro escalar infinitesimal. Los puntos $P'$ y $Q'$ son entonces los resultantes de ``trasladar'' los puntos $P$ y $Q$ en la dirección definida por el vector $\xi$. Se dice entonces que $\xi^i(x)$ describe una isometría de la variedad, si para puntos arbitrarios $P$ y $Q$ se cumple que la distancia entre ellos es la misma que entre los puntos ``trasladados'' $P'$ y $Q'$, es decir, si
\begin{equation}
dl_{PQ}\stackrel{!}{=}dl_{P'Q'}  \label{is2}%
\end{equation}
o, más explícitamente,
\begin{align}
dl_{PQ}^2  &  =g_{ij}(P)(dx^i_{PQ})(dx^j_{PQ}) , \label{is3}\\
dl_{P'Q'}^2  &  =g_{ij}(P')(dx^i_{P'Q'})(dx^j_{P'Q'}) . \label{is4}
\end{align}
Usando (\ref{is1}), (\ref{is1b}) podemos escribir
\begin{align}
dx^i_{P'Q'} &= dx^i_{PQ}+\varepsilon\left[\xi^i(Q)-\xi^i(P)\right] \\
&= dx^i+\varepsilon\left[\xi^i(x+dx)-\xi^i(x)\right] \\
&= dx^i+\varepsilon \left[\partial_j\xi^i(x)\right]dx^j.
\end{align}
Con esta expresión (\ref{is4}) se reduce, a primer orden en $\varepsilon$, a
\begin{equation}
dl_{P'Q'}^2=g_{ij}dx^idx^j+\varepsilon\left[g_{ij}dx^idx^k\partial_k\xi^j
+g_{ij}dx^kdx^j\partial_k\xi^i+\xi^kdx^idx^j\partial_kg_{ij}\right],  \label{is5}
\end{equation}
donde ahora todos los campos están evaluados en el punto $P$, de coordenadas $x^i$.
Cambiando adecuadamente los índices de suma, la expresión (\ref{is5}) toma la forma
\begin{equation}
dl_{P'Q'}^2=g_{ij}dx^idx^j+\varepsilon\left[g_{ik}\partial_j\xi^k
+g_{kj}\partial_i\xi^k+\xi^k\partial_kg_{ij}\right] dx^idx^j,
\label{is6}%
\end{equation}
de modo que la condición (\ref{is2}) es equivalente a
\begin{equation}\marginnote{Ecuación de Killing}
\boxed{g_{ik}\partial_j\xi^k+g_{kj}\partial_i\xi^k+\xi^k\partial_kg_{ij}=0.} \label{is8}
\end{equation}
ya que $\varepsilon$, $dx^i$ son infinitesimales, pero arbitrarios.

La ecuación\footnote{Más exactamente, las $n(n+1)/2$ ecuaciones.} diferencial parcial lineal (\ref{is8}) es la condición que debe satisfacer el campo $\xi^i$ para que describa una isometría. Las soluciones (no nulas) son llamadas \textbf{vectores de Killing}\footnote{Wilhelm Karl Joseph Killing (1847-1923): matemático alemán. Ver \url{http://en.wikipedia.org/wiki/Wilhelm_Killing}  \textbf{$\leftarrow$ se necesita traducción al español de esto!}.}.

\subsubsection{Plano euclideano}

En el plano euclidiano y en coordenadas cartesianas, $x^i=(x,y)$, la
métrica es $g_{ij}\overset{\ast}{=}\delta_{ij}$.
%Luego, de la ecuación (\ref{gm6}) tenemos que los símbolos de Christoffel son
%nulos. y por lo tanto la curvatura también es nula.
%
%La ecuación de las geodésicas (\ref{gm7}) es%
%\begin{equation}
%\frac{d^2x^i}{d\ell^2}=0. %
%\end{equation}
%Por lo tanto, las geodésicas en el plano son rectas.
En este caso la ecuación (\ref{is8}) se reduce a:
\begin{align}
\delta_{ik}\partial_j\xi^k+\delta_{kj}\partial_i\xi^k  &  =0,\label{isej1}\\
\partial_j\xi_i+\partial_i\xi_j  &  =0. \nonumber
\end{align}

Escribimos explícitamente estas ecuaciones:%
\begin{align}
\partial_1\xi^1+ & \partial_1\xi^1= 0, \label{isej2}\\
\partial_1\xi^2+ & \partial_2\xi^1= 0, \label{isej3} \\
\partial_2\xi^2+ & \partial_2\xi^2= 0.  \label{isej4}
\end{align}
De (\ref{isej2}) y de (\ref{isej4}) se obtiene respectivamente que $\xi
^1=\xi^1(y)$ y $\xi^2=\xi^2(x)$. De este modo la ecuación (\ref{isej3}) toma la forma:
\begin{equation}
\frac{d\xi^2}{dx}+\frac{d\xi^1}{dy}=0.  \label{isej5}%
\end{equation}
Usando el hecho de que $x$ e $y$ son coordenadas independientes, vemos que
esta ecuación se satisface si y sólo si:
\begin{align}
\frac{d\xi^1}{dy}  &  =\alpha=\text{cte.},\label{isej6}\\
\frac{d\xi^2}{dx}  &  =-\alpha, \nonumber
\end{align}
de modo que:
\begin{equation}
\xi^1 =\alpha y+\beta, \qquad
\xi^2 =-\alpha x+\gamma. \label{isej7}
\end{equation}
Esta solución se puede expresar como combinación lineal de tres
soluciones linealmente independientes:
\begin{align}
\xi^i  &  =\left(  \xi^1,\xi^2\right)  \\
& =\left(  \alpha y+\beta,-\alpha x+\gamma\right) \\
  &  =\alpha\left(  y,-x\right)  +\beta\left(  1,0\right)
+\gamma\left(  0,1\right)  \\
 &  =\alpha\,\xi_{(1)}^i+\beta\,\xi_{(2)}^i+\gamma\,\xi_{(3)}^i, \nonumber
\end{align}
donde
\begin{align}
\xi_{(1)}^i  &  :=\left(  y,-x\right)  ,\\
\xi_{(2)}^i  &  :=\left(  1,0\right)  ,\\
\xi_{(3)}^i  &  :=\left(  0,1\right)  .
\end{align}
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=4cm]{fig/fig-Killing-E2.pdf}}
\caption{Vectores de Killing en el plano euclideano bidimensional.}
\label{KE2}
\end{figure}
\end{center}
Los tres campos independientes $\xi^i_{(a)}$, $a=1,2,3$ son representados en la figura \ref{KE2}. Los vectores $\xi^i_{(1)}$ y $\xi^i_{(2)}$ corresponden a \textit{traslaciones} a lo largo de los ejes $x$ e $y$ respectivamente, mientras que $\xi^i_{(1)}$ a \textit{rotaciones} respecto al origen. Cualquier combinación lineal (a coeficientes constantes) de estos tres vectores básicos define una dirección de simetría del espacio Euclideano bidimensional.
\subsubsection{Esfera unitaria}

Consideremos como segundo ejemplo la métrica que el espacio euclidiano
tridimensional $E_{3}$ induce sobre la esfera unitaria. Sabemos que en
coordenadas cartesianas la métrica de $E_{3}$ es $g_{ij}=\delta_{ij}$ y la restricción a los puntos de la esfera es:
\begin{align}
x^1  &  =\sin\theta\cos\varphi,\\
x^2  &  =\sin\theta\sin\varphi,\\
x^{3}  &  =\cos\theta,
\end{align}
de modo que las coordenadas sobre la esfera son $\theta$ y $\varphi$. Usando
(\ref{metind2}) para calcular la métrica inducida, obtenemos
\begin{equation}
g_{ab}=\left(
\begin{array}[c]{cc}
1 & 0\\
0 & \sin^2\theta
\end{array}\right) ,  \label{ind7}
\end{equation}
donde $a,b=\theta,\varphi$. Usando (\ref{gm6}) obtenemos que las componentes no
nulas del símbolo de Christoffel son:
\begin{align}
\left\{  _{\varphi\varphi}^{\ \theta}\right\}  &  =-\sin\theta\cos\theta,\label{ind8}\\
\left\{  _{\theta\varphi}^{\, \varphi}\right\}   &  =\left\{  _{\varphi\theta}^{\,\varphi}\right\}  =\cot\theta.
\end{align}
%De este modo, se obtiene directamente que las ecuaciones para las
%geodésicas sobre la esfera son:%
%\begin{align}
%\frac{d^2\theta}{d\ell^2}-\sin\theta\cos\theta\left(\frac{d\varphi}{d\ell}\right)^2  &  =0,\label{ind9}\\
%\frac{d^2\varphi}{d\ell^2}+2\cot\theta\frac{d\theta}{d\ell}\frac{d\varphi}{d\ell}  &  =0.
%\end{align}
%También, mediante un cálculo directo, encontramos que las componentes
%no nulas de la curvatura de la esfera son:%
%\begin{align}
%R_{\phi\theta\phi}^{\theta}  &  =-R_{\phi\phi\theta}^{\theta}=\sin^2%
%\theta,\label{ind10}\\
%R_{\theta\theta\phi}^{\phi}  &  =-R_{\theta\phi\theta}^{\phi}=-1. %
%\nonumber
%\end{align}

El sistema de ecuaciones para los vectores de Killing sobre la esfera unitaria,
seg\'{u}n (\ref{is8}) es:
\begin{align}
g_{\theta\theta}\partial_\theta\xi^\theta+g_{\theta\theta}\partial_\theta\xi^\theta +\xi^a\partial_ag_{\theta\theta}  &  =0,\label{isej12}\\
g_{\theta\theta}\partial_\varphi\xi^\theta+g_{\varphi\varphi}\partial_\theta\xi^\varphi +\xi^a\partial_ag_{\theta\varphi}  &  =0, \\
g_{\varphi\varphi}\partial_\varphi\xi^\varphi+g_{\varphi\varphi}\partial_\varphi\xi^\varphi +\xi^a\partial_ag_{\varphi\varphi}  &  =0.
\end{align}
Luego:
\begin{align}
\partial_\theta\xi^\theta  &  =0,\label{isej13}\\
\partial_\varphi\xi^\theta+\sin^2\theta\,\partial_\theta\xi^\varphi  &  =0, \\
2\sin^2\theta\,\partial_\varphi\xi^\varphi+\xi^\theta\partial_\theta(\sin^2\theta)  & =0.
\end{align}
La primera ecuación nos dice que $\xi^\theta=\xi^\theta(\varphi)$, de modo que la
segunda y tercera ecuación nos queda:
\begin{align}
\partial_\varphi\xi^\theta+\sin^2\theta\,\partial_\theta\xi^\varphi  &
=0,\label{isej14}\\
\sin\theta\,\partial_\varphi\xi^\varphi+\xi^\theta\cos\theta  &  =0.
\end{align}
Se puede verificar, reemplazando directamente en estas ecuaciones, que el
sistema tiene por solución:
\begin{equation}
\xi^a=\alpha\,\xi^a_{(1)} +\beta\,\xi^a_{(2)} +\gamma\,\xi^a_{(3)}
,\label{isej15}
\end{equation}
con
\begin{eqnarray}
\xi^a_{(1)} &=&(\sin\varphi, \cot\theta\cos\varphi ), \\
\xi^a_{(2)} &=&(\cos\varphi,-\cot\theta\sin\varphi) , \\
\xi^a_{(3)} &=&(0,1).
\end{eqnarray}

\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=4cm]{fig/fig-KS2x.pdf}\hfill\includegraphics[height=4cm]{fig/fig-KS2y.pdf}\hfill\includegraphics[height=4cm]{fig/fig-KS2z.pdf}}
\caption{Vectores de Killing sobre la esfera: 
$\xi^a_{(1)}$, $\xi^a_{(2)}$ y $\xi^a_{(3)}$ respectivamente.}
\label{KS2}
\end{figure}
\end{center}

\paragraph{Teorema:} El número máximo de vectores de Killing independientes que una variedad de dimensión $n$ puede admitir es $n(n+1)/2$ *** Agregar ref. ***

***
\subsection{Coodenadas adaptadas a una isometría}

Definimos una coordenada, por ejemplo, $x^1$, a lo largo de las líneas integrales del campo $\xi$, es decir, tal que los vectores tangentes a las líneas coordenadas de $x^1$ son paralelos a $\xi$. Esto implica que en el SC adaptado 
\begin{equation}
\xi^i\stackrel{*}{=}(\xi^1,0,\cdots,0).
\end{equation}
Siempre es posible definir la coordenada $x^1$ (en particular, qué tan rápido varía ésta sobre las curvas integrales de $\xi$) de modo que
\begin{equation}\label{xi100}
\xi^i\stackrel{*}{=}(1,0,\cdots,0).
\end{equation}
Entonces, si $\xi$ es un vector de Killing, la ecuación \eqref{is8} implica que
\begin{equation}\label{p1g0}
\partial_1g_{ij}\stackrel{*}{=}0,
\end{equation}
es decir, que en sistema de coordenadas adaptado al vector de Killing $\xi$ la métrica no depende de la coordenada adaptada $x^1$.

Note que si bien es posible definir coordenadas adaptadas a cada vector de Killing independiente que existe en una variedad, no necesariamente existe un SC adaptado simultáneamente a todos los vectores de Killing existentes.

En forma recíproca, si en un SC particular (todas) las componentes del tensor métrico son independientes de una de las coordenadas (por ejemplo, $x^1$, de modo que se satisface \eqref{p1g0}), entonces campo vectorial tangente a las líneas coordenadas correspondientes a $x^1$ define un vector de Killing, con componentes de la forma \eqref{xi100} en ese SC.

\subsection{Vectores de Killing y operadores diferenciales}
\begin{equation}
\hat{\xi}:=\xi^i\partial_i.
\end{equation}

Estos operadores pueden interpretarse como los generadores de traslaciones en la dirección $\xi$, ya que, al actuar sobre un campo escalar $\phi$ satisfacen
\begin{equation}
\phi(x+\varepsilon\xi) = \phi(x) + \varepsilon(\hat{\xi}\phi)(x) + \mathcal{O}(\varepsilon^2)
\end{equation}

Teorema: el conjunto de todos operadores asociados a los vectores de Killing l.i. forman un álgebra de Lie.

\section{Conexión, derivadas covariantes y transporte paralelo}

\subsection{Derivada parcial de campos tensoriales}

Salvo en el caso de un escalar, \textit{la derivada de las componentes de un tensor no define un nuevo tensor bajo TGC's}. Esto puede entenderse como una consecuencia del hecho que la definición de la derivada parcial de un tensor involucra (el límite) de la sustracción de tensores \textit{en puntos distintos}.

En efecto, veamos cómo transforman las derivadas parciales $\partial_i
A_j:={\partial A_j}/{\partial x^i}$ de un vector covariante $A_i$:
\begin{equation}
\bar{\partial}_i\bar{A}_j=\frac{\partial \bar{A}_j}{\partial
\bar{x}^i}=\frac{\partial}{\partial \bar{x}^i}\left(\frac{\partial x^k
}{\partial\bar{x}^j}A_k \right)
=\frac{\partial x^k}{\partial\bar{x}^j}\frac{\partial x^l}{\partial\bar{x}^i}
\partial_l A_k +\frac{\partial^2 x^k}{\partial\bar
{x}^i \partial\bar{x}^j}A_k . \label{ord2}
\end{equation}
Vemos que la derivada parcial de un vector covariante se comporta como un tensor tipo $(_2^0)$ \textit{excepto por el segundo término}. La ley de
transformación es lineal, pero inhomogénea, lo que implica que si las
derivadas de un vector se anulan en un SC entonces éstas no se anulan
necesariamente en otro.

Una consecuencia importante de este hecho es que, \textit{sin introducir elementos adicionales definidos sobre la variedad}, no existe un criterio \textit{independiente de coordenadas} de cuándo un tensor es constante sobre (una región de) la variedad. Esto se debe a que, como se deduce de la expresión \eqref{ord2}, aunque las componentes de un tensor sean iguales en dos puntos (o en una región) de la variedad en algún SC ($x$, de modo que $\partial_iA_j=0$) en otros SC's ($\bar{x}$) éste no será el caso: $\bar{\partial}_i\bar{A}_j\neq 0$.
 
Lo mismo ocurre para tensores de distinto tipo. Por ejemplo, la ley de transformación de un tensor tipo ($_2^0$) es:
\begin{equation}
\bar{A}_{jk}=\frac{\partial x^l}{\partial\bar{x}^j} \frac{\partial x^m}{\partial\bar{x}^k}A_{lm}. \label{ord3}
\end{equation}
Luego, sus derivadas parciales transforman bajo una TGC de la forma siguiente:
\begin{equation}
\bar{\partial}_i\bar{A}_{jk}=\frac{\partial x^l}{\partial\bar{x}^j} 
\frac{\partial x^m}{\partial\bar{x}^k} \frac{\partial x^n}{\partial \bar{x}^i}\,\partial_nA_{lm}+\left(
\frac{\partial^2 x^l}{\partial\bar{x}^i\partial\bar{x}^j} \frac{\partial x^m}{\partial\bar{x}^k} 
+\frac{\partial x^l}{\partial\bar{x}^j}\frac{\partial^2 x^m}{\partial\bar{x}^i\partial\bar{x}^k}\right) A_{lm}. \label{ord4}
\end{equation}

Vemos que los dos últimos términos en (\ref{ord4}) son lineales en las
componentes (no derivadas) del tensor original y son también lineales en las segundas derivadas del cambio de coordenadas. Estos términos hacen que la ley de transformación de las derivadas parciales difiera de la ley correspondiente a un tensor.

\subsubsection{Excepciones*}
Existen, sin embargo, ciertas combinaciones particulares de derivadas parciales que sí son tensores. Por ejemplo, el ``rotor'' de un campo vectorial covariante $A_k $, $\partial_k A_i-\partial_iA_k $, es un tensor tipo ($_2^0$), ya que
\begin{align}
\bar{A}_k  & =\frac{\partial x^l}{\partial\bar{x}^k} A_l ,\label{ord6}\\
\bar{\partial}_i\bar{A}_k  & =\frac{\partial x^l}{\partial\bar{x}^k} 
\frac{\partial x^m}{\partial\bar{x}^i}\, \partial_mA_l +
\frac{\partial^2 x^l}{\partial\bar{x}^i\partial\bar{x}^k}\, A_l ,\\
\bar{\partial}_k \bar{A}_i & =\frac{\partial x^l}{\partial\bar{x}^i}
\frac{\partial x^m}{\partial\bar{x}^k}\, \partial_mA_l +
\frac{\partial^2 x^l}{\partial\bar{x}^k\partial\bar{x}^i}\, A_l ,\\
\end{align}
y por lo tanto
\begin{equation}
 \bar{\partial}_i\bar{A}_k -\bar{\partial}_k \bar{A}_i =\frac{\partial x^l}{\partial\bar{x}^k} \frac{\partial x^m}{\partial\bar{x}^i} \left( \partial_m A_l -\partial_l A_m\right) .
\end{equation}
De este modo, $\left(\partial_iA_j -\partial_j A_i\right)$ define un tensor
antisimétrico de tipo ($_2^0$).

% \item La divergencia cíclica de cualquier tensor antisimétrico tipo
% ($_2^0$) es un tensor antisimétrico tipo ($_{3}^0$). En efecto, sea
% $T_{ik}$ un tensor antisimétrico. Entonces:
% \begin{equation}
% \bar{T}_{ik}=\frac{\partial x^l}{\partial\bar{x}^i} \frac{\partial x^m}{\partial\bar{x}^k} %
% T_{lm}. \label{ord8}%
% \end{equation}
% Queremos mostrar que \ $\partial_jT_{ik}+\partial_iT_{kj}+\partial
% _k T_{ji}$ es un tensor. Para esto calculemos dichos sumandos a partir de
% (\ref{ord8}):
% \begin{equation}
% \bar{\partial}_j\bar{T}_{ik}=\frac{\partial x^l}{\partial\bar{x}^i} \bar{\partial
%}_k x^m \bar{\partial}_jx^n\partial_nT_{lm}+\left(
% \bar{\partial}_j\frac{\partial x^l}{\partial\bar{x}^i} \,\frac{\partial x^m}{\partial\bar{x}^k} %
% +\frac{\partial x^l}{\partial\bar{x}^i} \,\bar{\partial}_j\frac{\partial x^m}{\partial\bar{x}^k} \right)
% T_{lm}, \label{ord9}%
% \end{equation}%
% \begin{equation}
% \bar{\partial}_i\bar{T}_{kj}=\frac{\partial x^l}{\partial\bar{x}^k} \bar{\partial
%}_jx^m \frac{\partial x^n}{\partial\bar{x}^i}\partial_nT_{lm}+\left(
% \bar{\partial}_i\frac{\partial x^l}{\partial\bar{x}^k} \,\bar{\partial}_jx^m %
% +\frac{\partial x^l}{\partial\bar{x}^k} \,\bar{\partial}_i\bar{\partial}_jx^m \right)
% T_{lm}, \label{ord10}%
% \end{equation}%
% \begin{equation}
% \bar{\partial}_k \bar{T}_{ji}=\frac{\partial x^l}{\partial\bar{x}^j} \bar{\partial
%}_ix^m \bar{\partial}_k x^n\partial_nT_{lm}+\left(
% \bar{\partial}_k \frac{\partial x^l}{\partial\bar{x}^j} \,\frac{\partial x^m}{\partial\bar{x}^i} %
% +\frac{\partial x^l}{\partial\bar{x}^j} \,\bar{\partial}_k \frac{\partial x^m}{\partial\bar{x}^i} \right)
% T_{lm}. \label{ord11}%
% \end{equation}
% Si se suman los tres términos, se combinan adecuadamente los índices
% mudos $l$, $m$ y $n$, y se usan las propiedades de antisimetría de $T$,
% se obtiene:
% \begin{equation}
% \bar{\partial}_j\bar{T}_{ik}+\bar{\partial}_i\bar{T}_{kj}+\bar{\partial
%}_k \bar{T}_{ji}=\frac{\partial x^l}{\partial\bar{x}^i} \frac{\partial x^m}{\partial\bar{x}^k} \text{
%}\bar{\partial}_jx^n\left( \partial_nT_{lm}+\partial_l T_{mn}%
% +\partial_mT_{nl}\right), \label{ord12}%
% \end{equation}
% lo cual prueba que la divergencia cíclica de un tensor antisimétrico
% tipo ($_2^0$) es un tensor antisimétrico tipo ($_{3}^0$).


Todo esto no es suficiente para establecer un análisis tensorial
exhaustivo sobre una variedad. Una (aparentemente simple) pregunta aún no tiene respuesta: \textquestiondown Qué condición caracteriza un campo vectorial
\textit{constante}? Claramente la respuesta \underline{no es}
$\partial_iA_j=0$, pues esta condición no es independiente del SC usado. De hecho, sin estructuras adicionales definidas sobre la variedad, el concepto de ``campo vectorial constante"\, simplemente no está definido. La estructura adicional que permite formular éste y otros conceptos, por ejemplo el de derivada covariante, es llamada \textbf{conexión}.

\subsection{Conexión y derivada covariante de tensores}

\textbf{Definición:} Llamamos \textbf{conexión}\footnote{También llamada \textbf{conexión afín} o \textbf{afinidad}.}, $\Gamma$, \textit{a un arreglo de} $n^3$ \textit{cantidades definidas en cada punto de la variedad para las cuales:}
\begin{itemize}
\item Suponemos un conjunto de valores dados en cada SC particular y,
\item Cambian sus valores bajo una TGC de acuerdo a la siguiente ley de transformación:
\begin{equation}\marginnote{Conexión}
\bar{\Gamma}_{\ jk}^i=\frac{\partial\bar{x}^i}{\partial x^l}\frac{\partial
x^p}{\partial\bar{x}^j}\frac{\partial x^q}{\partial\bar{x}^k}\,
\Gamma_{\ pq}^l +\frac{\partial\bar{x}^i}{\partial x^l}\frac{\partial
^2x^l}{\partial\bar{x}^j \partial\bar{x}^k}. \label{dinv72}
\end{equation}
\end{itemize}
A partir de una conexión y de un campo vectorial (covariante) es posible definir la siguiente combinación, que es un tensor bajo TGC's:
\begin{equation}
\boxed{\nabla_iA_j:=\partial_iA_j-\Gamma_{\ ij}^kA_k.}
\label{dinv6}
\end{equation}
El tensor $\nabla_iA_j$ es llamado la \textbf{derivada covariante} de $A_j$ \textit{con respecto a la conexión} $\Gamma$. Se debe notar que $\Gamma$ está definida \textit{sobre} nuestra variedad y puede ser concebida,\textit{ en general}, como un objeto geométrico \textit{no necesariamente relacionado con una  métrica}.


Es simple verificar a partir de \eqref{ord2} y \eqref{dinv6} que $\nabla_iA_j$ es efectivamente covariante (es decir, un tensor), si suponemos que las componentes de $\Gamma$ transforman de acuerdo a \eqref{dinv72}.

Note que el segundo término del lado derecho de \eqref{dinv72} es independiente de $\Gamma$ y depende entonces sólo de la transformación de coordenadas. Esta es la propiedad implica que una conexión $\Gamma$ no sea nula en todo SC, aún cuando puede serlo en algunos. Otra consecuencia de la ley de transformación \eqref{dinv72} es que si \textit{dos conexiones}, $\Gamma$ y $\hat{\Gamma}$, definidas en la misma variedad, entonces \textit{su diferencia es un tensor}. En efecto, bajo una TGC se tiene que
\begin{align}
\bar\Gamma_{\ ik}^n-\hat{\overline{\Gamma}}{}^n_{\ ik} & =\frac{\partial
\bar{x}^n}{\partial x^l}\frac{\partial x^r}{\partial\bar{x}^i}
\frac{\partial x^s}{\partial\bar{x}^k}\,\Gamma_{\ rs}^l +\frac{\partial
\bar{x}^n}{\partial x^l}\frac{\partial^2 x^l}{\partial\bar{x}^i \partial\bar{x}^k}-\frac{\partial\bar{x}^n}{\partial x^l}
\frac{\partial x^r}{\partial\bar{x}^i}\frac{\partial x^s}{\partial
\bar{x}^k}\,\hat{\Gamma}_{\ rs}^l -\frac{\partial\bar{x}^n}{\partial x^l}\frac{\partial^2 x^l}{\partial\bar{x}^i \partial\bar{x}^k}\label{dinv8}\\
& =\frac{\partial
\bar{x}^n}{\partial x^l}\frac{\partial x^r}{\partial\bar{x}^i}
\frac{\partial x^s}{\partial\bar{x}^k}\left( \Gamma_{\ rs}^l
-\hat{\Gamma}_{\ rs}^l \right) .\nonumber
\end{align}
Luego, la diferencia $\Gamma_{jk}^i -\hat{\Gamma}_{jk}^i $ es un tensor. Equivalentemente, la suma de una conexión y un tensor de tipo $(_2^1)$ es una nueva conexión.

% Veamos qué sucede si sumamos dos afinidades, $\Gamma_{lm}^k $ y
% $\hat{\Gamma}_{lm}^k $:
% \begin{align}
% \bar\Gamma_{\ ik}^n+\overline{\hat{\Gamma}}_{ik}^n & =\frac{\partial
% \bar{x}^n}{\partial x^l}\frac{\partial x^r}{\partial\bar{x}^i}%
% \frac{\partial x^s}{\partial\bar{x}^k}\Gamma_{\ rs}^l +\frac{\partial
% \bar{x}^n}{\partial x^l}\frac{\partial^2 x^l}{\partial\bar{x}%
%^i \partial\bar{x}^k}+\frac{\partial\bar{x}^n}{\partial x^l}%
% \frac{\partial x^r}{\partial\bar{x}^i}\frac{\partial x^s}{\partial
% \bar{x}^k}\hat{\Gamma}_{rs}^l +\frac{\partial\bar{x}^n}{\partial x^l %
%}\frac{\partial^2 x^l}{\partial\bar{x}^i \partial\bar{x}^k %
%},\label{dinv9}\\
% \bar\Gamma_{\ ik}^n+\overline{\hat{\Gamma}}_{ik}^n & =\frac{\partial
% \bar{x}^n}{\partial x^l}\frac{\partial x^r}{\partial\bar{x}^i}%
% \frac{\partial x^s}{\partial\bar{x}^k}\left( \Gamma_{\ rs}^l +\hat{\Gamma
%}_{rs}^l \right) +2\frac{\partial\bar{x}^n}{\partial x^l}\frac
% {\partial^2 x^l}{\partial\bar{x}^i \partial\bar{x}^k}.\nonumber
% \end{align}
%
%
% Luego la suma de dos afinidades no es una afinidad. Pero si $\lambda$ y $\mu$
% son números tales que $\lambda+\mu=1$ entonces tenemos:
% \begin{align}
% \lambda\bar\Gamma_{\ ik}^n+\mu\overline{\hat{\Gamma}}_{ik}^n &
% =\lambda\frac{\partial\bar{x}^n}{\partial x^l}\frac{\partial x^r%
%}{\partial\bar{x}^i}\frac{\partial x^s}{\partial\bar{x}^k}\Gamma
% _{rs}^l +\lambda\frac{\partial\bar{x}^n}{\partial x^l}\frac{\partial
%^2x^l}{\partial\bar{x}^i \partial\bar{x}^k}+\mu\frac{\partial\bar
% {x}^n}{\partial x^l}\frac{\partial x^r}{\partial\bar{x}^i}%
% \frac{\partial x^s}{\partial\bar{x}^k}\hat{\Gamma}_{rs}^l +\mu
% \frac{\partial\bar{x}^n}{\partial x^l}\frac{\partial^2 x^l}%
% {\partial\bar{x}^i \partial\bar{x}^k}\label{dinv10}\\
% \lambda\bar\Gamma_{\ ik}^n+\mu\overline{\hat{\Gamma}}_{ik}^n &
% =\frac{\partial\bar{x}^n}{\partial x^l}\frac{\partial x^r}{\partial
% \bar{x}^i}\frac{\partial x^s}{\partial\bar{x}^k}\left( \lambda
% \Gamma_{\ rs}^l +\mu\hat{\Gamma}_{rs}^l \right) +\lambda\frac{\partial\bar
% {x}^n}{\partial x^l}\frac{\partial^2 x^l}{\partial\bar{x}^i %
% \partial\bar{x}^k}+\mu\frac{\partial\bar{x}^n}{\partial x^l}%
% \frac{\partial^2 x^l}{\partial\bar{x}^i \partial\bar{x}^k}\nonumber\\
% \lambda\bar\Gamma_{\ ik}^n+\mu\overline{\hat{\Gamma}}_{ik}^n &
% =\frac{\partial\bar{x}^n}{\partial x^l}\frac{\partial x^r}{\partial
% \bar{x}^i}\frac{\partial x^s}{\partial\bar{x}^k}\left( \lambda
% \Gamma_{\ rs}^l +\mu\hat{\Gamma}_{rs}^l \right) +\frac{\partial\bar{x}^n%
%}{\partial x^l}\frac{\partial^2 x^l}{\partial\bar{x}^i \partial\bar
% {x}^k}.\nonumber
% \end{align}
%
%
% Por lo tanto, $\lambda\Gamma_{\ rs}^l +\mu\hat{\Gamma}_{rs}^l $ es una
% afinidad si y sólo si $\lambda+\mu=1$.

 La noción de derivada covariante introducida en (\ref{dinv6}) no es un concepto intrínseco, ``natural'' de una variedad, sino que está definida a partir de una conexión, la cual debe ser indicada. Además, es posible introducir más que una conexión sobre la misma variedad, por lo que es en general necesario distinguir entre las derivadas definidas con respecto a distintas conexiones.

Se extenderá ahora la noción de derivada covariante a tensores de distinto tipo. Existen distintas formas de motivar la definición de derivadas covariantes de tensores de tipo arbitrario. Una manera simple es  \textit{asumiendo} que la derivación covariante satisface:
\begin{itemize}
\item[i)] la usual regla de la derivación de un producto (regla de
Leibniz\footnote{En honor de Gottfried Leibniz: 1646-1716, filósofo, matemático, jurista, bibliotecario y político alemán. Ver \url{http://es.wikipedia.org/wiki/Gottfried_Leibniz}.}) cuando se aplica al producto de tensores y,
\item[ii)] que la derivada covariante de un escalar coincide con la usual derivación parcial.
\end{itemize}

A partir de estas propiedades es posible deducir la forma de la derivada covariante de cualquier cantidad tensorial. Por ejemplo, la derivada $\nabla_iB^j$ de un vector contravariante $B^j$ puede encontrarse aplicando las propiedades arriba descritas al escalar $A_k B^k $, donde $A_k$ son las componentes de un vector covariante (auxiliar). En efecto, la propiedad ii) implica que
\begin{equation}
\nabla_i\left( A_k B^k\right)=\partial_i\left( A_k B^k \right).\label{dinv12}
\end{equation}
Usando ahora la propiedad i), es decir, la regla de Leibniz podemos escribir
\begin{equation}
A_k (\nabla_iB^k)+(\nabla_i A_k)B^k = A_k (\partial_iB^k) +(\partial_iA_k)B^k .
\end{equation}
Finalmente, usando \eqref{dinv6} encontramos que
\begin{equation}
A_k (\nabla_iB^k) +\left(\partial_iA_k-\Gamma_{\ ik}^jA_j\right) B^k=A_k (\partial_iB^k) +(\partial_iA_k)B^k 
\end{equation}
Cancelando términos y renombrando índices apropiadamente, podemos
escribir:
\begin{equation}
A_k \left( \nabla_iB^k -\partial_iB^k -\Gamma_{\ ij}^k B^j\right) =0,
\end{equation}
pero $A_k $ es arbitrario, de modo que el término entre paréntesis debe anularse para cada valor del índice $k$. De aquí podemos despejar una expresión explícita para la derivada covariante de un vector contravariante:
\begin{equation}
\boxed{\nabla_iB^k =\partial_iB^k +\Gamma_{\ ij}^kB^j .} \label{dinv15}
\end{equation}

Para un tensor general $T^{kl\cdots}{}_{pq\cdots}$ aplicamos un método similar. Consideramos la derivada covariante del escalar
\begin{equation}
T^{kl\cdots}{}_{pq\cdots}A_k B_l \cdots F^pG^q, \label{dinv16}
\end{equation}
donde $A_k $, $B_l $, $\dots $, $F^p$ y $G^q$ vectores arbitrarios. Se obtiene
así que la derivada covariante del tensor $T^{kl\cdots}{}_{pq\cdots}$ consta de su derivada parcial y de términos adicionales, uno por
cada índice de $T^{kl\cdots}{}_{pq\cdots}$, cada uno de los cuales consta de
una contración entre las componentes de $T^{kl\cdots}{}_{pq\cdots}$ y
$\Gamma$:
\begin{align}\marginnote{Derivada Covariante de un Tensor}
\nabla_iT^{kl\cdots}{}_{pq\cdots} =&\ \partial_iT^{kl\cdots
}{}_{pq\cdots}+\Gamma_{\ in}^kT^{nl\cdots}{}_{pq\cdots} +\Gamma_{\
in}^lT^{kn\cdots}{}_{pq\cdots} +\cdots \nonumber \\
&  -\Gamma_{ip}^nT^{kl\cdots}{}_{nq\cdots
}-\Gamma_{iq}^nT^{kl\cdots}{}_{pn\cdots}-\cdots . \label{dinv17}
\end{align}

Note que, \textit{de acuerdo a nuestras convenciones}, el índice de diferenciación es siempre el segundo índice covariante de $\Gamma$ y los restantes lugares son llenados de modo de asignar el que falta de $T^{kl\cdots}{}_{pq\cdots}$.

Con el concepto de derivada covariante es posible definir el concepto de un
campo tensorial \textbf{covariantemente constante}: un campo tensorial es
(covariantemente) constante si y sólo si su derivada covariante es nula.


\subsection{Transporte paralelo*}

Una forma interesante de entender los conceptos de derivada covariante y de
conexión es introduciendo la noción de \textbf{transporte paralelo}. Hemos visto que la derivada parcial de un campo vectorial covariante no define un tensor, ya que, por definición ésta resta vectores definidos en distintos puntos (aún cuando sean infitesimalmente próximos):
\begin{equation}
\partial_jA_i(x):=\lim_{\Delta x^j \rightarrow0}\frac{A_i(x +\Delta x)-A_i(x)}{\Delta x^j}. \label{tp1}
\end{equation}
Análogamente, podemos escribir:
\begin{eqnarray}
\nabla_jA_i&=&\partial_jA_i-\Gamma_{\ ji}^kA_k \\
&=& \lim_{\Delta x^j\rightarrow0}\frac{A_i(x+\Delta x)-A_i(x)}{\Delta x^j}-\Gamma_{\ ji}^kA_k \\
&=& \lim_{\Delta x^j\rightarrow0}\frac{A_i(x+\Delta x)-A_i(x)-\Gamma_{\
li}^k(x)A_k(x)\Delta x^l}{\Delta x^j}\\
&=& \lim_{\Delta x^j\rightarrow0}\frac{A_i(x+\Delta x)-\left[A_i(x)+\Gamma_{\
li}^k(x)A_k(x)\Delta x^l\right]}{\Delta x^j}\\
&=& \lim_{\Delta x^j\rightarrow0}\frac{A_i(x+\Delta x)-A^T_i(x+\Delta x)}{\Delta x^j}.
\end{eqnarray}
Basados en esto, definimos (en el punto con coordenadas $x+dx$) el vector $A^T_i(x+dx)$ por:
\begin{equation}\marginnote{transporte vector covariante}
 \boxed{A^T_i(x+dx):=A_i(x)+\Gamma_{\ li}^k(x)dx^lA_k(x)=A_i(x)+\delta A_i(x),} \label{tranAcov}
\end{equation}
que interpretaremos geométricamente como el vector resultante de \textit{transportar}
(``\textit{paralelamente}'') el vector $A_i(x)$ desde el punto con coordenadas $x$ hasta el punto con coordenadas $x+dx$. De esta forma, la derivada covariante de $A_i$ puede
ser interpretada como el límite de la diferencia de dos vectores definidos en el \textit{mismo punto}: el vector $A_i(x+dx)$ y el vector $A^T_i(x+dx)$ resultante de transportar $A_i(x)$ desde $x$ hasta $x+dx$.

Análogamente, la derivada covariante de un vector contravariante $A^i$ puede
ser interpretada como el límite de la diferencia del vector $A^i(x+dx)$ y el
vector $A_T^i(x+dx)$ resultante de transportar $A^i(x)$ desde $x$ hasta $x+dx$,
con
\begin{equation}\marginnote{transporte vector contravariante}
 \boxed{A_T^i(x+dx):=A^i(x)-\Gamma_{\ kj}^i(x)dx^kA^j(x)=A^i(x)+\delta
A^i(x).}\label{tranAcontr}
\end{equation}

El proceso anterior puede repetirse de forma análoga para definir
el transporte paralelo de un tensor de rango arbitrario. Para esto, es útil notar que 
\eqref{tranAcov} y \eqref{tranAcontr} pueden ser escritas como
\begin{equation}
A^T_i(x+dx):=A_i(x)+dx^j\left[\partial_jA_i-\nabla_jA_i\right],
\end{equation}
\begin{equation}
A_T^i(x+dx):=A^i(x)+dx^j\left[\partial_jA^i-\nabla_jA^i\right].
\end{equation}
Similarmente, para un tensor $A^{i_1\cdots i_r}_{\ \ \ \ \ \ j_1\cdots j_s}$ de tipo $(^r_s)$ tendremos que
\begin{equation}
(A_T)^{i_1\cdots i_r}_{\ \ \ \ \ \ j_1\cdots j_s}(x+dx):=A^{i_1\cdots i_r}_{\ \ \ \ \ \ j_1\cdots j_s}(x)+dx^j\left[\partial_j-\nabla_j\right]A^{i_1\cdots i_r}_{\ \ \ \ \ \ j_1\cdots j_s}.
\end{equation}


\subsection{Integrabilidad*}\label{sec:integ}
Habiendo definido el transporte paralelo de un vector (o, en
general, de un tensor) desde un punto $P$
de coordenadas $x^i $ a un punto $Q$ de coordenadas $x^i +dx^i$, podemos
ahora realizar una \textbf{sucesión de transportes} para definir el transporte de un vector \textit{a lo largo de una curva dada}. La pregunta natural que surge es si el transporte de un vector sobre una curva depende de la trayectoria usada o, por el contrario depende sólo de los puntos iniciales y finales. Equivalentemente, podemos preguntarnos si al transportar un vector por una curva \textit{cerrada}, volviendo así al punto $P$ de partida, se obtiene o no el vector original.
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=5cm]{fig/fig-transporte-curva-cerrada.pdf}}
\caption{Transporte paralelo de un vector sobre una curva cerrada.}
\label{dibujo}
\end{figure}
\end{center}
\begin{quotation}
\textbf{Definición:} Decimos que una conexión $\Gamma$ es \textit{integrable} si y sólo si el transporte asociado a ella es independiente de la trayectoria.
\end{quotation}
Encontremos las condiciones que debe satisfacer una conexión para que sea
integrable.
Para ello, consideremos un vector contravariante $A^i(P)$ asociado a un punto
$P$ de coordenadas $x^i$. Si el transporte es independiente de la trayectoria
usada, entonces puede definirse en forma única un campo vectorial $A^i(x)$ (en todo punto de la variedad) transportando el vector $A^i(P)$ desde $P$ hasta cada punto de la variedad. En este caso podemos expresar el vector en un punto $Q$ infinitesimalmente
próximo a $P$ como
\begin{equation}
A^i(x+dx)=A^i_T(x+dx) . \label{condint}
\end{equation}
Usando $A^i(x+dx)=A^i(x)+(\partial_j A^i)dx^j $ y (\ref{tranAcontr}) obtenemos que
la condición (\ref{condint}) es equivalente a
\begin{equation}
\nabla_i A^k=\partial_i A^k+\Gamma_{\ ij}^k A^j =0.
\label{ecdeintegr}%
\end{equation}
\begin{quotation}
\textbf{Resumiendo: si una conexión es integrable, existen vectores (no nulos) covariantemente constantes.}
\end{quotation}
Por otro lado, puede considerarse a (\ref{ecdeintegr}) como una ecuación
diferencial que determina el campo (covariantemente constante) $A^i(x)$ dada la conexión, suponiendo que ésta es integrable. Las \textbf{condiciones de
integrabilidad} de esta ecuación son por lo tanto las condiciones para que la
conexión sea integrable.


\subsection{No-conmutatividad de las derivadas covariantes, curvatura y torsión}\label{sec:RT}
Las segundas derivadas covariantes de un tensor \textit{no conmutan}.
Por ejemplo, usando (\ref{dinv15}) y (\ref{dinv17}) podemos calcular la segunda derivada covariante de un vector arbitrario $A^i$:
\begin{equation}
\nabla_i \nabla_jA^k =\partial_i \partial_jA^k +\partial_i A^l\Gamma_{\ jl}^k +A^l\partial_i \Gamma_{\ jl}^k +\partial_jA^l\Gamma_{\ il}^k
+A^l\Gamma_{\ jl}^m\Gamma_{\ im}^k -\partial_{l}A^k \Gamma_{\ ij}^l-A^l\Gamma_{\ ml}^k \Gamma_{\ ij}^m. \label{segder1}
\end{equation}
A partir de aquí, encontramos la siguiente identidad\footnote{Existen identidades similares para cada tipo de tensor. Por ejemplo: $\left[\nabla_i \nabla_j -\nabla_j\nabla_i\right]\phi\equiv -T_{\ ij}^{k} \nabla_k\phi$,  $\left[\nabla_i \nabla_j -\nabla_j\nabla_i\right]A_k\equiv -R_{\ kij}^lA_l - T_{\ ij}^{l} \nabla_lA_k$. Es un buen ejercicio demostrar estas identidades ...}
\begin{equation}
\boxed{\nabla_i \nabla_jA^k -\nabla_j\nabla_i A^{k}\equiv R_{\ lij}^k
A^l - T_{\ \ ij}^l \nabla_l A^k ,} \label{cdc}
\end{equation}
donde hemos definido el \textbf{tensor de curvatura} como:
\begin{equation}\marginnote{Curvatura}
\boxed{R_{\ lij}^k :=\partial_i\Gamma_{\ jl}^k -\partial_j\Gamma_{\ il}^k 
+\Gamma_{\ im}^k \Gamma_{\ jl}^m -\Gamma_{\ jm}^k \Gamma_{\ il}^m .}
\label{curvatura}
\end{equation}
Además, definimos el \textbf{tensor de torsión} como:
\begin{equation}\marginnote{Torsión}
\boxed{T_{\ ij}^k :=\Gamma_{\ ij}^k -\Gamma_{\ ji}^k .}
\end{equation}

Aunque la curvatura y la torsión se definen en términos de la conexión,
que no es un tensor, $R_{\ lij}^k$ y $T_{\ ij}^k$ sí son tensores, como
puede verificarse considerando la consistencia de \eqref{cdc}, o directamente
a partir de su definición y la ley de transformación \eqref{dinv72} de la conexión.

Note que, como consecuencia directa de su definición, estos tensores poseen las siguientes propiedades de antisimetría:
\begin{equation}
R_{\ lij}^k\equiv - R_{\ lji}^k, \qquad T_{\ ij}^k\equiv -T_{\ ji}^k. \label{asRT}
\end{equation}
Estas simetrías, reducen el número de componentes linealmente independientes de la curvatura y la torsión de \textit{una conexión arbitraria} a $n^3(n-1)/2$ y $n^2(n-1)/2$, respectivamente.

Retornando a la identidad \eqref{cdc}, vemos que ella implica que la derivada covariante es independiente del orden de derivación (cuando se aplica a un vector arbitrario) si y sólo si $R_{\ lji}^k=0$ y $T_{\ ij}^{k}=0$.

Por otro lado, si existe un sistema coordenado donde $\Gamma_{\ ij}^k
\overset{\ast}{=}0$ \textit{en todo punto de una región dada}, entonces la curvatura y la torsión serán idénticamente nulas, $R_{\ ijk}^l =0$ y $T_{\ ij}^{k}=0$, en esa región. Además, como la curvatura y la torsión son tensores, éstas se anularán en todo sistema coordenado.
El contrarecíproco también es válido. Si existe un sistema
coordenado en el cual la curvatura o la torsión sean distintas de cero, $R_{\ ijk}^l \neq 0$ ó $T_{\ ij}^{k}\neq 0$, entonces no existirá ningún sistema coordenado en el cual la conexión se anule en la región dada.

También es posible probar (aunque es bastante más laborioso, y omitiremos la prueba aquí. Ver, por ejemplo, la sección 6.7 de \cite{Dinverno}) que si la curvatura y la torsión de anulan en una región de la variedad entonces es posible encontrar un SC tal que la conexión se anule en ese sistema.

En resumen:
\begin{quotation}
\textbf{La condición necesaria y suficiente para que exista un sistema coordenado donde todas las componentes de la conexión se anulen en todo punto de una región dada, $\Gamma_{\ ij}^k \overset{\ast}{=}0$, es que $R_{\ ijk}^l=0$ y $T_{\ ij}^k =0$ en esa región.}
\end{quotation}

\subsection{Interpretación geométrica de la curvatura*}
\begin{center}
\begin{figure}[h!]
\centerline{\includegraphics[height=6cm]{fig/fig-transporte-y-curvatura-01.pdf}}
\caption{Transporte de un vector desde $P$ a $R$ por dos trayectorias (infinitesimales) distintas.}
\label{intgeomcurv}
\end{figure}
\end{center}

Consideremos cuatro puntos $P,Q,R$ y $S$ como en la figura (\ref{intgeomcurv}), con coordenadas $x^i $, $x^i+dx_1^i$, $x^i+dx_1^i+dx_2^i$ y $x^i+dx_2^i$ respectivamente.
Sean $A^i (P)$ las componentes de un vector contravariante definido en el punto $P$. Traslademos este vector a través de la trayectoria $PQR$, obteniendo $\bar{A}^i_{PQR}(R)$, y comparémoslo con el vector resultante del transporte por la trayectoria $PSR$, $\bar{A}^i_{PSR}(R)$.

Transportamos primero $A^i$ de $P$ a $Q$, obteniendo el vector $A_{T}^i(Q)$:
\begin{equation}
A_{T}^i(Q)=A^i(P) -\Gamma_{\ kj}^i(P)A^j(P)\, dx_1^k .
\label{AT1}%
\end{equation}

Transportamos ahora $A_{T}^i $ desde $Q$ hasta $R$, obteniendo así el vector denotado por $A_{PQR}^i (R)$:
\begin{equation}
A_{PQR}^i (R) = A_{T}^i (Q) -\Gamma_{\ kj}^i(Q)A_{T}^j (Q)\,dx_2^k. \label{AT2}%
\end{equation}
 La conexión en el lado derecho de (\ref{AT2}) está evaluada en el punto $Q$, de coordenadas $x^i +dx_1^i$, y por lo tanto podemos escribir
\begin{equation}
\Gamma_{\ kj}^i(Q)=\Gamma_{\ kj}^i (x+dx_1)=\Gamma_{\ kj}^i(x)+dx_1^l(\partial_l\Gamma_{\ kj}^i) (x) . \label{expGam}
\end{equation}
Reemplazando \eqref{expGam} en \eqref{AT2} y utilizando (\ref{AT1}) obtenemos:%
\begin{align}
A_{PQR}^i (R) &= A^i -\Gamma_{\ kj}^i A^j\,dx_1^k-\Gamma_{\ kj}^i A^j\,dx_2^k \nonumber\\
& \quad  +\Gamma_{\ kj}^i \Gamma_{\ ml}^jA^l\, dx_1^m\,dx_2^k -(\partial_l\Gamma_{\ kj}^i) A^j dx_1^l dx_2^k. \label{AT22}
\end{align}
El cálculo del vector $A_{\rm PSR}^i(R)$ es análogo, con la única diferencia que primero se realiza el desplazamiento coordenado $dx_2^i$ hasta el punto $S$ y luego el desplazamiento en $dx_1^i$ hasta el punto $R$. Por lo tanto, el vector buscado tiene la misma forma que \eqref{AT22}, pero intercambiando $dx_1^i$ con $dx_2^i$, es decir,
\begin{align}
A_{PSR}^i (R) &= A^i -\Gamma_{\ kj}^i A^j\,dx_2^k-\Gamma_{\ kj}^i A^j\,dx_1^k \nonumber\\
& \quad  +\Gamma_{\ kj}^i \Gamma_{\ ml}^jA^l\, dx_2^m\,dx_1^k -(\partial_l\Gamma_{\ kj}^i) A^j dx_2^l dx_1^k. \label{APSR}
\end{align}

Comparamos ambos vectores transportado calculando la diferencia de sus componentes. Restando \eqref{AT22} y \eqref{APSR} obtenemos (luego de renombrar algunos índices) que el resultado es proporcional al tensor de curvatura \eqref{curvatura},
\begin{equation}
\boxed{(A_{PSR}^i-A^i_{PQR})(R) =R_{\ jkl}^i(x)\, A^j(x)\,dx_1^k\,dx_2^l.} \label{19}%
\end{equation}
%Por lo tanto, la diferencia entre el vector transportado y el original es
%proporcional a la curvatura.
Esto demuestra que el tensor de curvatura de una conexión mide localmente (en una región infinitesimal) la magnitud de la dependencia del transporte (inducido por la conexión, de vectores y tensores) con la trayectoria.

La relación \eqref{19} que implica que, si la curvatura es nula, el transporte de cualquier vector es independiente de la trayectoria en una región infinitesimal. Este resultado puede extenderse a trayectorias finitas que unan extremos comunes considerando una sucesión de ``deformaciones infinitesimales'' de una curva en otra, tal como lo ilustra la figura. Para esto, suponemos que la variedad ``no tiene agujeros'', es decir, que es \textbf{simplemente conexa}. Por lo tanto, si el tensor de curvatura es nulo y la variedad es simplemente conexa entonces el transporte es independiente de la trayectoria, es decir, la conexión es integrable.

\subsection{Interpretación geométrica de la torsión*}
Consideremos tres puntos $P$, $Q$ y $S$ de la variedad, infinitesimalmente
cercanos, tal que las coordenadas de $P$ son $x^i $, $dx_1^i $ es la
diferencia de las coordenadas entre los puntos $P$ y $Q$, y $dx_2^k $ es la
diferencia de las coordenadas entre los puntos $P$ y $S$ (ver figura \ref{fig:torsion}a). 
Ahora transportamos los vectores contravariantes $dx_1^k $ y $dx_2^k $, ambos definidos en $P$, hasta los puntos $S$ y $Q$, para obtenener así $\overline{dx}^i_1$ y $\overline{dx}^i_2$, respectivamente (ver figura \ref{fig:torsion}b). Las componentes de estos nuevos vectores (infinitesimales) están dadas entonces por
\begin{center}
\begin{figure}[H]
\centerline{\includegraphics[height=4cm]{fig/fig-torsion-01.pdf}
\hspace{1cm}\includegraphics[height=4cm]{fig/fig-torsion-02.pdf}}
\caption{}
\label{fig:torsion}
\end{figure}
\end{center}
\begin{equation}
\overline{dx}_2^i (Q)=dx_2^i -\Gamma_{\ kj}^i(x)\, dx_2^j dx_1^k ,
\end{equation}
\begin{equation}
\overline{dx}_1^i (S)=dx_2^i -\Gamma_{\ kj}^i(x)\, dx_1^j dx_2^k .
\end{equation}

Los nuevos vectores $\overline{dx}_2^i (Q)$ y $\overline{dx}_1^i (S)$ permiten definir nuevos puntos $R_1$ y $R_2$, cuyas coordenadas son
\begin{equation}
x^i(R_1):=x^i(Q)+\overline{dx}_2^i (Q)=x^i+dx_1^i+dx_2^i -\Gamma_{\ kj}^i(x)\, dx_2^j dx_1^k, 
\end{equation}
\begin{equation}
x^i(R_2):=x^i(S)+\overline{dx}_1^i (S)=x^i +dx_2^i +dx_1^i
 -\Gamma_{\ kj}^i(x)\, dx_1^j dx_2^k .
\end{equation}
En general, los puntos $R_1$ y $R_2$ \textit{no coinciden}: la diferencia entre sus coordenadas está dada por
\begin{align}
x^i(R_2)-x^i(R_1) &= -\Gamma_{\ kj}^i(x)\, dx_1^j dx_2^k+ \Gamma_{\ kj}^i(x)\, dx_2^j dx_1^k \\
&=  \left[-\Gamma_{\ kj}^i(x)+ \Gamma_{\ jk}^i(x)\right] dx_1^j dx_2^k \\
&=  T_{\ jk}^i(x)\, dx_1^j dx_2^k .
\end{align}
Vemos entonces que en la construcción geométrica discutida de un ``paralelógramo infinitesiamal'' los puntos finales no coinciden (el ``paralelógramo no se cierra'') si el tensor de torsión en no nulo. Debido a esto a menudo se dice que la torsión describe (posibles) ``fallas en el cierre local de paralelógramos infinitesimales''.



%\subsection{Integrabilidad y curvatura*}
%Vimos en la sección \ref{sec:integ} que si una conexión es integrable entonces
%existen campos vectoriales no nulos $A^i(x)$ tal que $\nabla_iA^j=0$. Usando esto y la identidad (\ref{cdc}) encontramos el siguiente resultado:
%\begin{quotation}
%\textbf{Teorema:} Si una conexión es integrable entonces su tensor de curvatura es nulo.
%\end{quotation}
%Equivalentemente, si el tensor de curvatura de una conexión es no nulo, ésta no es integrable.
%
%Puede también ser probado que el recíproco del teorema anterior es támbién válido. Para esto, usamos \eqref{19} que implica que, si la curvatura es nula, el transporte de cualquier vector es independiente de la trayectoria en una región infinitesimal. Este resultado puede extenderse a trayectorias finitas generales considerando una sucesión de ``deformaciones infinitesimales'' de una curva en otra, tal como lo ilustra la figura. Para esto, asumimos que la variedad ``no tiene agujeros'', es decir, que es \textbf{simplemente conexa}.
%
%En resumen:
%\begin{quotation}
%\textbf{Teorema:} Una conexión es integrable si y sólo si su curvatura es
%igual a cero.
%\end{quotation}
% Estas condiciones de integrabilidad pueden obtenerse de la forma usual: las
% segundas derivadas parciales $A^i $ deben conmutar, es decir,
% \begin{equation}
% \frac{\square A^k}{\partial x^m \partial x^j}=\frac{\square %
% A^k}{\partial x^j \partial x^m}. \label{segder}%
% \end{equation}
% De las ecuaciones (\ref{ecdeintegr}) y (\ref{segder}) se obtiene la
% condición:
% \begin{equation}
% \frac{\partial}{\partial x^m}\left( \Gamma_{\ ij}^k A^i \right)
% -\frac{\partial}{\partial x^j}\left( \Gamma_{im}^k A^i \right) =0.
% \label{exac}%
% \end{equation}
% Desarrollando obtenemos que%
% \begin{equation}
% A^i \partial_m\Gamma_{\ ij}^k +\Gamma_{\ ij}^k \partial_mA^i %
% -A^i \partial_j\Gamma_{im}^k -\Gamma_{im}^k \partial_jA^i =0.
% \label{10}%
% \end{equation}
% Reemplazando $\partial_jA^k $ en (\ref{10}) tenemos:%
% \begin{align}
% A^i \partial_m\Gamma_{\ ij}^k -A^i \partial_j\Gamma_{im}^k +\Gamma
% _{lm}^k \Gamma_{\ ij}^l A^i -\Gamma_{lj}^k \Gamma_{im}^l A^i  & =0,\\
% A^i \left( \partial_m\Gamma_{\ ij}^k -\partial_j\Gamma_{im}^k %
% +\Gamma_{lm}^k \Gamma_{\ ij}^l -\Gamma_{lj}^k \Gamma_{im}^l \right)  & =0.
% \label{12}%
% \end{align}
% Como $A^i $ es un vector arbitrario, la condición de integrabilidad es:%
% \begin{equation}
% \partial_m\Gamma_{\ ij}^k -\partial_j\Gamma_{im}^k +\Gamma_{lm}^k %
% \Gamma_{\ ij}^l -\Gamma_{lj}^k \Gamma_{im}^l =0.
% \end{equation}


Por ejemplo, en el caso de un plano ($E_2$) sabemos que el transporte es independiente de la trayectoria, y por lo tanto la conexión es integrable. Además, en coordenadas cartesianas\footnote{Utilizamos el símbolo $\overset{\ast}{=}$ para denotar que esta igualdad es válida sólo en un sistema coordenado particular.} $\Gamma_{\ ij}^k \overset{\ast}{=}0$. En este caso,
según (\ref{curvatura}), tenemos que la curvatura se anula, $R_{\ ijk}^l =0$.

%\subsection{Condiciones de Integrabilidad}\label{sec-ci}
%Puesto que el análisis realizado es sólo fruto de reescribir la
%ecuación del movimiento de una part{\'\i}cula libre en un SR
%no-inercial, debe entonces ser posible, dados $\bar{\Gamma}$ y/o
%$\bar{g}$ en algún SR, retornar a un SRI con sus coordenas inerciales asociadas. En
%este caso (compare (\ref{tsri}) y (\ref{tsrni})) la conexión $\Gamma$ se anula
%idénticamente y la métrica adopta su forma minkowskiana. En otras palabras, un SRI queda determinado por los requerimientos que $g_{\mu\nu}=\eta _{\mu\nu}$ y $%
%\Gamma _{\ \mu\nu}^\lambda =0$ \emph{globalmente}, es decir, en todo punto del
%espaciotiempo.
%
%Estudiaremos ahora las condiciones que las componentes de una conexión
%$\bar{\Gamma}$ en un sistema coordenado $\bar x$ deben satisfacer de modo que
%exista la transformación $\bar{x}\to x$ de vuelta a un SRI.
%
%De la ley de transformación (\ref{tigam}) vemos que la condición para que en
%el nuevo sistema de coordenadas $\Gamma=0$ es
%\begin{equation}
%\frac{\partial x^\mu}{\partial \bar{x}^\sigma}\frac{\partial \bar{x}^\rho
%}{\partial x^{\nu}}\frac{\partial\bar{x}^{\eta}}{\partial
%x^\lambda}\bar{\Gamma}_{\ \rho \eta}^\sigma (\bar{x})+\frac{\partial x^{\mu
%}}{\partial \bar{x}^\rho}\frac{\partial^2\bar{x}^\rho}{\partial
%x^{\nu}\partial x^\lambda}=0.
%\end{equation}
%De aquí, obtenemos
%\begin{equation}
%\frac{\partial^2\bar{x}^\alpha}{\partial x^{\nu}\partial
%x^\lambda}=-\frac{\partial \bar{x}^\rho}{\partial x^{\nu}}%
%\frac{\partial \bar{x}^{\eta}}{\partial x^\lambda}\bar{%
%\Gamma}_{\ \rho \eta}^\alpha (\bar{x}) .\label{2der}
%\end{equation}
%
%Ya que, asumiendo que la transformación $\bar{x}\to x$ existe, el lado
%izquierdo de (\ref{2der}) es simétrico bajo intercambio de los índices $\nu$
%y $\lambda$, debe cumplirse que el lado derecho también lo sea. Esta
%condición es equivalente a
%\begin{equation}
%\boxed{\bar{\Gamma}_{\ \mu\nu}^\lambda
%=\bar{\Gamma}_{\ \nu\mu}^\lambda.}
%\end{equation}
%
%Otra condición puede ser encontrada usando el hecho que el lado derecho de
%(\ref{2der}) debe efectivamente ser igual a las segundas derivadas de las
%funciones que definen el cambio de coordenadas. Para esto, derivamos
%(\ref{2der}) nuevamente y obtenemos:
%\begin{eqnarray}
%\frac{\partial^3\bar{x}^\alpha}{\partial x^\rho \partial
%x^{\nu}\partial x^\lambda} &=&-\frac{\partial}{\partial x^\rho}\left(
%\frac{\partial \bar{x}^\beta}{\partial x^{\nu}}\frac{\partial
%\bar{x}^{\gamma}}{\partial x^\lambda}\bar{\Gamma}_{\ \beta
%\gamma}^\alpha \right) \\
%\frac{\partial^3\bar{x}^\alpha}{\partial x^\rho \partial
%x^{\nu}\partial x^\lambda} &=&-\left[ \frac{\partial^2\bar{x}%
%^\beta}{\partial x^\rho \partial x^{\nu}}\frac{\partial \bar{x}%
%^{\gamma}}{\partial x^\lambda}\bar{\Gamma}_{\ \beta \gamma
%}^\alpha +\frac{\partial \bar{x}^\beta}{\partial x^{\nu}}%
%\frac{\partial^2\bar{x}^{\gamma}}{\partial x^\lambda \partial
%x^\rho}\bar{\Gamma}_{\ \beta \gamma}^\alpha +\frac{\partial
%\bar{x}^\beta}{\partial x^{\nu}}\frac{\partial \bar{x}%
%^{\gamma}}{\partial x^\lambda}\frac{\partial \bar{x}^{\delta}}{%
%\partial x^\rho}\frac{\partial \bar{\Gamma}_{\ \beta \gamma
%}^\alpha}{\partial \bar{x}^{\delta}}\right].
%\end{eqnarray}%
%Reemplazando, las segundas derivadas usando (\ref{2der}) nuevamente, llegamos
%a
%\begin{eqnarray}
%\frac{\partial^3\bar{x}^\alpha}{\partial x^\rho \partial
%x^{\nu}\partial x^\lambda}&=&\frac{\partial \bar{x}^\sigma}{%
%\partial x^\rho}\frac{\partial \bar{x}^{\eta}}{\partial x^{\nu}}%
%\bar{\Gamma}_{\ \sigma \eta}^\beta \frac{\partial \bar{x}%
%^{\gamma}}{\partial x^\lambda}\bar{\Gamma}_{\ \beta \gamma
%}^\alpha +\frac{\partial \bar{x}^\beta}{\partial x^{\nu}}%
%\frac{\partial \bar{x}^\sigma}{\partial x^\lambda}\frac{\partial
%\bar{x}^{\eta}}{\partial x^\rho}\bar{\Gamma}_{\ \sigma \eta
%}^{\gamma}\bar{\Gamma}_{\ \beta \gamma}^\alpha -\frac{\partial
%\bar{x}^\beta}{\partial x^{\nu}}\frac{\partial \bar{x}%
%^{\gamma}}{\partial x^\lambda}\frac{\partial \bar{x}^{\delta}}{%
%\partial x^\rho}\frac{\partial \bar{\Gamma}_{\ \beta \gamma
%}^\alpha}{\partial \bar{x}^{\delta}} \\
%&=&-\frac{\partial \bar{x}^\beta}{\partial x^{\nu}}\frac{%
%\partial \bar{x}^{\gamma}}{\partial x^\lambda}\frac{\partial
%\bar{x}^{\delta}}{\partial x^\rho}\left( \frac{\partial \bar{%
%\Gamma}_{\ \beta \gamma}^\alpha}{\partial \bar{x}^{\delta}}
%-\bar{\Gamma}_{\sigma\gamma}^\alpha \bar{\Gamma}_{\ \delta \beta
%}^\sigma -\bar{\Gamma}_{\ \beta \sigma}^\alpha \bar{\Gamma}_{\ \gamma
%\delta}^\sigma \right).
%\end{eqnarray}
%Nuevamente, ya que el lado izquierdo de esta expresión debe ser simétrico
%bajo intercambio de los índices $\lambda$ y $\rho$, es decir,
%\begin{equation}
%\frac{\partial^3\bar{x}^\alpha}{\partial x^\rho \partial
%x^{\nu}\partial x^\lambda}\equiv\frac{\partial^3\bar{x}^\alpha}{\partial
%x^\lambda \partial x^{\nu}\partial x^\rho},
%\end{equation}
%obtenemos la siguiente condición para las componentes de $\bar\Gamma$ y sus
%derivadas:
%\begin{equation}
%\frac{\partial \bar{\Gamma}_{\ \beta \gamma}^{\alpha
%}}{\partial \bar{x}^{\delta}}-\bar{\Gamma}_{\ \sigma \gamma
%}^\alpha \bar{\Gamma}_{\ \delta \beta}^\sigma -\bar{\Gamma}_{\ \beta
%\sigma}^\alpha \bar{\Gamma}_{\ \gamma \delta}^\sigma =\frac{\partial
%\bar{\Gamma}_{\ \beta \delta}^\alpha}{\partial \bar{x}^{\gamma
%}}-\bar{\Gamma}_{\ \sigma\delta}^\alpha\bar{\Gamma}_{\ \gamma\beta}^\sigma -\bar{\Gamma}_{\ \beta\sigma}^\alpha \bar{\Gamma}_{\ \delta
%\gamma}^\sigma .
%\end{equation}
%Definiendo (el tensor de curvatura, ver sección \ref{sec:intcurv})
%\begin{equation}
%\boxed{\bar{R}_{\ \mu\nu\lambda}^\rho :=\frac{\partial
%\bar{\Gamma}_{\ \mu\lambda}^\rho}{\partial \bar{x}^{\nu
%}}-\frac{\partial \bar{\Gamma}_{\ \mu\nu}^\rho}{\partial
%\bar{x}^{\lambda}}+\bar{\Gamma}_{\ \sigma \nu}^{\rho}\bar{\Gamma}_{\ \mu\lambda}^\sigma-\bar{\Gamma}_{\ \sigma \lambda}^{\rho}\bar{\Gamma}_{\ \mu\nu}^\sigma,}
%\end{equation}
%podemos escribir la segunda condición de integrabilidad simplemente como:
%\begin{equation}
%\boxed{\bar{R}_{\ \mu\nu\lambda}^\rho =0.}
%\end{equation}
%Note que el tensor de curvatura es antisimétrico en sus dos últimos índices:
%\begin{equation}
%\bar{R}_{\ \mu\nu\lambda}^\rho\equiv -\bar{R}_{\ \mu\lambda\nu}^\rho.
%\end{equation}
%
%Este tensor de curvatura puede ser calculado en cualquier SC, por ejemplo, en coordenadas $\tilde{x}$, tendremos
%\begin{equation}
%\tilde{R}_{\ \mu\nu\lambda}^\rho =\frac{\partial
%\tilde{\Gamma}_{\ \mu\lambda}^\rho}{\partial \tilde{x}^{\nu
%}}-\frac{\partial \tilde{\Gamma}_{\ \mu\nu}^\rho}{\partial
%\tilde{x}^{\lambda}}+\tilde{\Gamma}_{\ \sigma \nu}^{\rho}\tilde{\Gamma}_{\ \mu\lambda}^\sigma-\tilde{\Gamma}_{\ \sigma \lambda}^{\rho}\tilde{\Gamma}_{\ \mu\nu}^\sigma,
%\end{equation}
%que determinan entonces las condiciones de integrabilidad para la conexión $\tilde{\Gamma}$ en ese SC. Usando el hecho que lsa componentes $\tilde{\Gamma}$ y $\bar{\Gamma}$ están relacionadas por medio de (\ref{tigam}), es posible verificar que la relación entre las componentes de $\tilde{R}$ y $\bar{R}$ es
%\begin{equation}
% \tilde{R}_{\ \mu\nu\lambda}^\rho =\frac{\partial \tilde{x}^\rho}{\partial \bar{x}^{\alpha}}\frac{\partial\bar{x}^{\beta}}{\partial \tilde{x}^\mu}\frac{\partial\bar{x}^{\gamma}}{\partial \tilde{x}^\nu}\frac{\partial\bar{x}^{\delta}}{\partial\tilde{x}^\lambda}\,\bar{R}_{\ \beta\gamma\delta}^\alpha . \label{ltR}
%\end{equation}
%Esta ley de transformación justifica, como discutiremos en el próximo capítulo, considerar que $\tilde{R}_{\ \mu\nu\lambda}^\rho$ y $\bar{R}_{\ \beta\gamma\delta}^\alpha$ con componentes en distintos SC's de un \textit{tensor}, el tensor de curvatura de Riemann.
%
%En resumen, hemos encontrado que la condición de integrabilidad de una conexión, es decir, la condición necesaria para que sus componentes puedan ser anuladas al realizar una transformación de coordenadas, es que ésta sea simétrica en sus índices inferiores (condición de torsión nula) y que su tensor de curvatura asociado sea nulo. Además, la ley de transformación (\ref{ltR}) asegura que si la condición de integrabilidad es satisfecha por las componentes de la conexión en un SC, entonces será satisfecha \textit{en todo SC}. Note finalmente que en el caso que hemos analizado, que es simplemente la teoría de RE formulada en SC's asociados a SR no-inerciales, las condiciones de integrabilidad de la conexión son naturalmente satisfechas.
%
%Como discutiremos con mayor detalle y precisión en el capítulo \ref{capTEG}, la teoría de gravitación de Einstein, la teoría General de la Relatividad, se basa en la idea que los efectos de un campo gravitacional homogéneo son equivalentes a aquellos observados en sistemas de referencia acelerados, y por lo tanto pueden ser ``anulados'' en todo punto eligiendo convenientemente el SR, pero que campos no-homogéneos (campos gravitacionales ``no-triviales'', como el producido por la Tierra, por ejemplo) no pueden ser anulados globalmente. Esto se describe matemáticamente usando conexiones que \textit{no satisfacen la condición de integrabilidad}, es decir, que tienen un tensor de curvatura no nulo.

\subsection{Curvas autoparalelas*}
% \begin{center}
% \begin{figure}[H]
% \centerline{\psfig{file=4.ps,height=3cm,angle=0}}
% \caption{}
% \label{autoparalelas}
% \end{figure}
% \end{center}
Se dice que una curva es \textbf{autoparalela} si satisface que sus vectores tangentes son paralelos (respecto al transporte definido por una conexón $\Gamma$) a lo largo de la misma curva. En otras palabras, si $x^i(\lambda)$ es la parametrización de la curva, entonces ella es autoparalela si su vector tangente $u^i(\lambda):=dx^i/d\lambda$ evaluado en el punto correspondiente a $\lambda+d\lambda$ (es decir, el punto con coordenadas $x^i(\lambda+d\lambda)$) es \textit{proporcional} al vector resultante de transportar $u^i(\lambda)$ (definido el punto correspondiente al parámetro $\lambda$, es decir, con coordenadas $x^i(\lambda)$) hasta el punto con parámetro $\lambda+d\lambda$. Es usual describir a las curvas autoparalelas como ``las más rectas posibles'' en el sentido que ellas siguen la dirección de sus propios vectores tangentes, transportados de punto a punto por medio de la conexión.


Es simple verificar que una curva autoparalela debe satisfacer una ecuación de la forma
\begin{equation}\marginnote{ec. curvas autoparalelas}
\boxed{\frac{d^2x^i}{d\lambda^2}+\Gamma_{jk}^i \frac{dx^j}{d\lambda}
\frac{dx^k}{d\lambda} =f(\lambda)\frac{dx^i}{d\lambda}.}
\end{equation}
Tal como en el caso de las curvas de longitud extrema (geodésicas), es posible elegir un parámetro afín tal que $f=0$. 
%
% Bajo un cambio de parámetro $s=s(\lambda)$,\ la ecuación (\ref{s4}) se
% transforma en:%
% \begin{equation}
% \frac{d^2x^k}{ds^2}+\Gamma_{lm}^k \frac{dx^l}{ds}\frac{dx^m}%
% {ds}=\frac{\phi s'-s^{\prime\prime}}{s^{\prime2}}\frac{dx^k}%
% {ds}, \label{s5}%
% \end{equation}
% donde $s':=ds/d\lambda$. El segundo miembro de esta ecuación se
% anula si y sólo si $\phi s'-s^{\prime\prime}=0$, es decir, si:%
% \begin{equation}
% s=\int^{\lambda}\exp\left[ \int^{\lambda'}\phi(u)du\right]
% d\lambda'. \label{s6}%
% \end{equation}
% De este modo, siempre es posible escoger un parámetro tal que el segundo
% miembro de (\ref{s6}) se anule. En este caso, el parámetro $s$ es llamado
% \textit{parámetro afín} y la ecuación para la curva autoparalela se reduce
% a
% \begin{equation}
% \boxed{\frac{d^2x^k}{ds^2}+\Gamma_{\ lm}^k \frac{dx^l}{ds}\frac{dx^m}%
% {ds}=0.} \label{s7}%
% \end{equation}
% Se verifica directamente que esta ecuación mantiene su forma bajo cualquier
% transformación lineal del parámetro afín ($\hat{s}=as+b,$ con $a$, $b$
% constantes). De este modo, las soluciones de la ecuación (\ref{s7}),
% $x^i =x^i (s)$, son representaciones paramétricas de las curvas autoparalelas
% asociadas a una conexión $\Gamma$.

\section{Geometría riemanniana}

La \textbf{geometría riemanniana}\footnote{Llamada así en honor de Bernhard Riemann: 1826-1866, matemático alemán. Ver \url{http://es.wikipedia.org/wiki/Bernhard_Riemann}.} es una generalización de la geometría intrínseca de las superficies gaussianas (originalmente, superficies inmersas en $E_3$), a espacios de dimensiones superiores. En una geometría riemanniana tanto las propiedades métricas (longitudes, áreas, volúmenes, módulos de vectores, geodésicas, etc) como las propiedades afines (derivadas covariantes, transporte, curvatura, torsión, curvas autoparalelas) quedan determinadas por un único objeto: la métrica. En particular en una geometría de Riemann, la conexión es aquella determinada por los símbolos de Christoffel asociados al tensor métrico, adicionalmente el tensor de torsión se anula idénticamente, pero el tensor de curvatura es en general no nulo. Además, en una geometría riemanninana las curvas autoparalelas coinciden con las geodésicas.



\subsection{Propiedades de los símbolos de Christoffel}

Los símbolos de Christoffel (\ref{gm6}) definen una conexión que posee torsión idénticamente nula (pues es simétrica bajo permutación de sus índices covariantes), pero que tiene en general curvatura no nula: $R^i_{\ jkl}(\left\{_{ \ }^{\ }\right\})\neq 0$, $T^i_{\ jk}(\left\{_{ \ }^{\ }\right\})\equiv 0$.

Es útil notar que, usando esta conexión, la ecuación de la geodésica (\ref{gm5}) puede escribirse
como
\begin{equation}
 v^j\nabla_jv^i=fv^i, \label{egdc}
\end{equation}
donde $v^i:={dx^i}/{d\lambda}$ representan los vectores tangentes sobre la
curva.

Además, puede verificarse directamente que la derivada covariante definida por
la conexión (\ref{gm6}) satisface la \textbf{condición de metricidad}
\begin{equation}\marginnote{Condición de metricidad}
\boxed{\nabla_kg_{ij}\equiv 0,} \label{tI1}%
\end{equation}
o, equivalentemente,
\begin{equation}
\nabla_kg^{ij}\equiv 0. \label{tIi2}%
\end{equation}

[* Esta propiedad puede interpretarse geométricamente como la condición que
asegura que el módulo de un vector arbitrario permanezca inalterado al ser
transportado.]

Más aún, puede probarse que la única conexión que satisface i) que su
torsión sea idénticamente nula y ii) que satisfaga la condición de metricidad
(\ref{tI1}) es precisamente la conexión (\ref{gm6}).

% \subsubsection{Condición de metricidad}
%
% Para empezar vamos a demostrar dos importantes teoremas.
%
% \begin{quotation}
% \textbf{Teorema:} Sea $M_n$ una variedad con una métrica $g_{ij}$ y una
% conexión $\Gamma_{\ i j}^{k}$. Entonces, la longitud de un vector bajo
% transporte paralelo es conservada si y sólo si:
% \begin{equation}
% \nabla_kg_{ij}=0. \label{tI1}%
% \end{equation}
% \end{quotation}
% En efecto, sea $A^i (x)$ un vector definido en $P$ (con coordenadas $x^i$), y
% $A_T^i (x+dx)$ el vector que resulta de transportar $A^i $ desde $P$ hasta
% $Q$ (con coordenadas $x^i+dx^i$), es decir,
% \begin{equation}
% A_T^i (Q)=\left( A^i -\Gamma_{\ l m}^i A^{ l}%
% dx^{ m}\right) _{P}. \label{tI2}%
% \end{equation}
% Las longitudes de $A^i $ y $A_T^i $ son, respectivamente:%
% \begin{equation}
% \left\vert A\right\vert^2=\left( g_{ij}A^i A^j \right) _{P},
% \label{tI3}%
% \end{equation}
% \begin{equation}
% \left\vert A_T\right\vert^2=\left( g_{ij}A_T^i A_T%
%^j \right) _{Q}. \label{tI4}%
% \end{equation}
% Pero
% \begin{equation}
% g_{ij}(Q)=\left( g_{ij}+\partial_{ l}g_{ij}dx^l\right) _{P}, \label{tI5}%
% \end{equation}
% por lo tanto:
% \begin{equation}
% \left\vert A_T\right\vert^2=\left[ g_{ij}+\partial_{ l}%
% g_{ij}dx^{ l}\right] \left[ A^i -\Gamma_{\ l m}^{ i
%}A^{ l}dx^{ m}\right] \left[ A^j -\Gamma_{\ l m}^{ j
%}A^{ l}dx^{ m}\right] . \label{tI6}%
% \end{equation}
% Desarrollando el producto a primer orden en $dx^{ l}$, encontramos:
% \begin{equation}
% \left\vert A_T\right\vert^2=g_{ij}A^i A^j -g_{ij}%
% \Gamma_{\ kl}^j A^i A^{k}dx^l-g_{ij}%
% \Gamma_{\ lm}^i A^{ l}A^j dx^{ m}+(\partial_kg_{ij}) A^i A^j dx^k.
% \label{tI7}%
% \end{equation}
% Igualando (\ref{tI3}) y (\ref{tI7}) se obtiene la condición
% \begin{equation}
% -g_{ij}\Gamma_{\ kl}^j A^i A^kdx^l-g_{ij}\Gamma_{\ lm}^i A^{ l}A^j
% dx^m+(\partial_kg_{ij}) A^i A^j dx^k=0. \label{tI8}%
% \end{equation}
% Intercambiando adecuadamente los índices mudos se obtiene:%
% \begin{equation}
% \left(\partial_kg_{ij}-g_{il}\Gamma_{\ jk}^l-g_{lj}\Gamma_{\ ik}^l\right)
% A^i A^j dx^k =0.
% \end{equation}
% Usando la arbitrariedad de $A^i $ y $dx^i $ obtenemos:
% \begin{equation}
% \partial_kg_{ij}-g_{il}\Gamma_{\ jk}^ll-g_{lj}\Gamma_{\ ik}^l=0 ,\label{tI10}
% \end{equation}
% es decir, que la derivada covariante de la métrica debe anularse.

% \subsection{Condiciónes de metricidad, torsión y símbolos de Christoffel}
%
% \begin{quotation}
% \textbf{Teorema:} La única conexión que satisface:
% \begin{enumerate}
% \item $T_{ij}^{k}=0$ (torsión nula o, equivalentemente, $\Gamma$
% simétrico en sus índices covariantes),
% \item $\nabla_kg_{ij}=0$ (condición de metricidad: la longitud de
% un vector no cambia bajo transporte paralelo),
% \end{enumerate}
% es el símbolo de Christoffel (\ref{gm6}).
% \end{quotation}
% En efecto, de la condición de metricidad tenemos:
% \begin{equation}
% \nabla_kg_{ij}=\partial_kg_{ij}-g_{il}\Gamma_{\ j k}^l-g_{lj}\Gamma_{\ i
% k}^l=0. \label{tII2}%
% \end{equation}
% Luego:
% \begin{equation}
% \partial_kg_{ij}=g_{l j}\Gamma_{\ i k}^l+g_{ il}\Gamma_{\ j k}^l.
% \label{tII3}%
% \end{equation}
% Escribamos las ecuaciones que se obtienen por permutación circular de los
% índices $ k$, $ i$, $ j$:%
% \begin{equation}
% \partial_i g_{ j k}=g_{l k}\Gamma_{\ j i}^l+g_{ jl
%}\Gamma_{\ k i}^l ,\label{tII4}%
% \end{equation}%
% \begin{equation}
% \partial_j g_{k i}=g_{l i}\Gamma_{\ k i}^l%
% +g_{kl}\Gamma_{\ ij}^l. \label{tII5}%
% \end{equation}
% Sumando (\ref{tII3}) a (\ref{tII4}) y restando (\ref{tII5}) se obtiene:%
% \begin{equation}
% \partial_kg_{ij}+\partial_i g_{ j k}-\partial_{ j
%}g_{k i}=g_{l j}\left( \Gamma_{\ i k}^l+\Gamma_{\ k i}^l\right) +g_{
% il}\left( \Gamma_{\ j k}^l-\Gamma_{\ k i}^l\right) +g_{l k}\left(
% \Gamma_{\% ji}^l-\Gamma_{\ ij}^l\right) . \label{tII6}%
% \end{equation}
% Usando la condición de torsión nula la ecuación (\ref{tII6}) se reduce a
% \begin{equation}
% \partial_kg_{ij}+\partial_i g_{ j k}-\partial_{ j
%}g_{k i}=2g_{l j}\Gamma_{\ i k}^l, \label{tII7}%
% \end{equation}%
% es decir,
% \begin{equation}
% g_{l j}\Gamma_{\ i k}^l=\frac{1}2\left[ \partial_{k
%}g_{ij}+\partial_i g_{ j k}-\partial_j g_{k i}\right]
% =:\left\{  j i k\right\} . \label{tII8}%
% \end{equation}
% Los símbolos $\left\{  j i k\right\} $ se denominan \textit{símbolos
% de Christoffel de primera especie}. Multiplicando por $g^{ j m}$,
% obtenemos:%
% \begin{align}
% \delta_{l}^{ m}\Gamma_{\ i k}^l & =\frac{1}2g^{ j m
%}\left[ \partial_kg_{ij}+\partial_i g_{ j k}%
% -\partial_j g_{k i}\right] ,\label{tII9}\\
% \Gamma_{\ i k}^{ m} & =\frac{1}2g^{ j m}\left[ \partial
% _kg_{ij}+\partial_i g_{ j k}-\partial_j g_{k i
%}\right] \equiv\left\{ _{ i k}^{ m}\right\} .
% \end{align}

\begin{quotation}
\textbf{\textit{Desde ahora en adelante consideraremos siempre geometrías riemannianas, es
decir, donde la conexión es dada por los símbolos de Christoffel.}}
\end{quotation}

\subsection{Espacios planos}
Se dice que un espacio con geometría riemanniana es \textbf{plano} si y sólo si existen sistemas de coordenadas en los que las componentes del tensor métrico sean \textit{constantes} (es decir, independientes de las coordenadas). Como consecuencia tenemos que:
\begin{quotation}
\textbf{Si un espacio riemanniano es plano entonces su tensor de curvatura de Riemann es nulo.}
\end{quotation}
En efecto, $g_{ij}\overset{\ast}{=}$ cte. implica $\{ _{ i j}^{\, k}\}
\overset{\ast}{=}0$ y por tanto $R_{\ i j k}^l=0$. Ejemplos de estos
espacios son el espacio de Minkowski de la teoría de Relatividad Especial y
los espacios euclideanos\footnote{Note que un cilíndro es también un espacio plano.}.

Por supuesto, el contrarrecíproco es también verdadero:
\begin{quotation}
\textbf{Si $R_{\ ijk}^l\neq0$
entonces \textit{no existe} un sistema coordenado donde $g_{ij}\overset{\ast}{=}{\rm
cte.}$ y $\{ _{ i j}^{\, k}\} \overset{\ast}{=}0$ \textit{en todo punto}}.
\end{quotation}

Análogamente, es posible probar el resultado recíproco: si el tensor de curvatura de Riemann se anula en todo punto entonces es posible encontrar un SC en el que la métrica asuma valores constantes. Para esto, basta aplicar el resultado visto al final de la sección (\ref{sec:RT}) y la identidad $\partial_kg_{ij}\equiv g_{il}\,\{^{\, l} _{jk}\}+g_{jl}\,\{^{\, l} _{ik}\}$, que es equivalente a (\ref{tI1}).

En resumen:
\begin{quotation}
\textbf{La condición necesaria y suficiente para que existan sistemas coordenados en los que la métrica sea constante es que su tensor de curvatura de Riemann sea nulo.}
\end{quotation}
% \subsection{Símbolo de Christoffel contraido}
%
% De (\ref{gm6}) obtenemos que la contracción $\Gamma_{\ j i}^i$ es dada por:
% \begin{eqnarray}
% \Gamma_{\ j i}^i &=&\frac{1}2g^{ ik}\left( \partial_ig_k+\partial_j
% g_{k i}-\partial_k g_{ji}\right)\\
% &=&\frac{1}{2}g^{ ik}\partial_j g_{ki}
% \end{eqnarray}
% Usando ahora la identidad\footnote{No presentaremos aquí una demostración
% general de esta identidad, válida en cualquier dimensión, para cualquier
% métrica invertible. El lector puede verificar su validez explicitamente en
% los casos bi- y tri-dimensional.}
% \begin{equation}
% \partial_k g\equiv gg^{ij}\partial_k g_{ij},
% \end{equation}
% podemos escribir
% \begin{equation}
% \Gamma_{\ ji}^i =\frac{1}{2}g^{ ik}\partial_j
% g_{ki}=\frac{1}{2g}\partial_j g=\partial_j \ln\sqrt{\left\vert g\right\vert}.
% \end{equation}
% Por lo tanto,
% \begin{equation}
% \boxed{\Gamma_{\ j i}^i =\partial_j \ln\sqrt{\left\vert g\right\vert}.}
% \end{equation}

% Se puede demostrar que la conexión de Cristoffel satisface
% \begin{equation}
% \Gamma_{ i k}^i =\frac{1}{\sqrt{\left| g\right|}}\partial
% _k\sqrt{\left| g\right|}%
% \end{equation}
% y esto implica que
% \begin{equation}
% \nabla_i V^i =\frac{1}{\sqrt{\left| g\right|}}\partial_i \left(
% \sqrt{\left| g\right|}V^i \right)
% \end{equation}
%
% Es directo verificar que la derivada covariante de la métrica con respecto
% a la conexión métrica (símbolo de Christoffel) es
% idénticamente nula. Para la inversa también se tiene $\nabla_{k
%}g^{ij}=0$.


\subsection{Coordenadas geodésicas}\label{secCG}

Puede mostrarse que, cuando la curvatura es no nula, si bien no es posible encontrar un SC en el cual la métrica sea constante y la conexión se anule \textit{en todo punto} de una región dada, sí es posible encontrar un SC tal que \textit{en un punto dado} $P$ de la variedad, se satisfaga
\begin{equation}
 g_{ij}(P)\overset{\ast}{=}diag(\pm 1,\cdots,\pm 1)=:\eta_{ij}, \qquad \left\{_{ij}^{\,
k}\right\}(P) \overset{\ast}{=}0,
\end{equation}
o, equivalentemente
\begin{equation}
 g_{ij}(P)\overset{\ast}{=}\eta_{ij}, \qquad \partial_k
g_{ij}(P) \overset{\ast}{=}0. \label{cg1}
\end{equation}
Estas coordenadas reciben el nombre de \textbf{coordenadas geodésicas}.
Note que, sin embargo, necesariamente $\partial_l\partial_k g_{ij}(P)\overset{\ast}{\neq}0$, ya que $R_{\
ijk}^l(P)\neq0$ requiere que $\partial_l\left\{
_{ik}^{\,m}\right\}(P)\overset{\ast}{\neq}0$.

Una forma de entender el rol de las coordenadas geodésicas es expresando la métrica en una vecindad del punto $P$ en términos de una serie de potencias en torno a $P$. Denotando las coordenadas de $P$ como $x^i$ podemos, en cualquier SC, escribir:
\begin{equation}
 g_{ij}(x+\Delta x)=g_{ij}(P)+\Delta x^k(\partial_kg_{ij})(P)+\frac{1}{2}\Delta x^k\Delta x^l(\partial_k\partial_lg_{ij})(P)+\cdots . \label{expg}
\end{equation}
En un espacio plano, es posible encontrar coordenadas en las que la métrica es constante, es decir, asume el mismo valor \textit{en todo punto}, entonces todas las derivadas $\partial_kg_{ij}(P)\stackrel{*}{=}0$, $\partial_k\partial_lg_{ij}(P)\stackrel{*}{=}0$, etc. se anulan y la expansión anterior (\ref{expg}) se reduce a sólo el primer término, de la forma estándar $\eta_{ij}$.

Por otro lado, en un espacio curvo ``sólo'' es posible encontrar coordenadas geodésicas en las que se satisfaga (\ref{cg1}), de modo que
\begin{equation}
 g_{ij}(x+\Delta x)\stackrel{*}{=} \eta_{ij}+0+\frac{1}{2}\Delta x^k\Delta x^l(\partial_k\partial_lg_{ij})(P)+\cdots . \label{expg2}
\end{equation}

En otras palabras, en un espacio curvo general, siempre es posible considerar una ``pequeña región'' en torno a un punto $P$ dado (pero arbitrario) tal que la métrica adopta su forma estándar y es \textit{aproximadamente constante en esa pequeña región}. Más precisamente aquella región ``pequeña'' está determinada por la condición que el tercer término en la expansión (\ref{expg2}) sea despreciable respecto al primero, es decir, $\Delta x^i$ debe ser lo suficientemente pequeño para asegurar que $\left|\Delta x^k\Delta x^l(\partial_k\partial_lg_{ij})(P)\right|\ll 1$.

Es importante notar que \textit{las coordenadas geodésicas no son únicas}, puesto que si $x^i$ es un SC geodésico, entonces $\tilde{x}^i=\Lambda^i_{\ j}x^i$, será también un SC geodésico si $\Lambda$ es una matriz \textit{constante} (que define una transformación \textit{lineal}) que deja invariante la métrica diagonal $\eta_{ij}$. En el caso de un geometría riemanniana (con elemento de línea definido positivo) $\eta_{ij}=diag(+1,+1,\cdots)$ y entonces las transformaciones entre SC's geodésicos son rotaciones ($n$-dimensionales). En el caso de la geometría es pseudo-riemanniana usada en la Teoría de Relavidad General $\eta_{ij}=diag(1,-1,-1,-1)$ y las transformaciones entre SC's geodésicos son las transformaciones de Lorentz. En este último caso las coordenadas geodésicas se identifican con las coordenadas ``quasi''-cartesianas\footnote{Aquí ``quasi'' significa que éstas no son realmente coordenadas cartesianas, ya que su extensión es necesariamente finita.} asociadas a un SRLI.

\subsubsection{Prueba de existencia de coordenadas geodésicas}

Primero probaremos que para cualquier conexión que sea simétrica en sus índices covariantes (es decir, que tenga torsión nula) es posible, dado un punto $P$ de la variedad, encontrar un SC $\bar{x}$ en el que
\begin{equation}\label{ccgG}
\bar\Gamma^i_{\ jk}(P)\stackrel{*}{=}0.
\end{equation}
Para esto, consideramos un SC $x$ ``inicial'' en el que no se satisfaga lo requerido, e.d. $\Gamma^i_{\ jk}(P)\stackrel{*}{\neq}0$. Si las coordenadas del punto $P$ en este SC son $x^i(P)=x^i_P$ entonces consideramos el cambio \textit{nolineal} de coordenadas
\begin{equation}\label{tccg}
x^i\rightarrow \bar{x}^i=A^i+B^i_{\ j}(x^j-x^j_P)+\frac{1}{2}C^i_{\ jk}(x^j-x^j_P)(x^k-x^k_P),
\end{equation}
donde $A^i$, $B^i_{\ j}$ y $C^i_{\ jk}=C^i_{\ kj}$ son constantes.

En el SC $\bar{x}$ las coordenadas del punto $P$ son nulas $\bar{x}^i(P)=A^i$. Además,
\begin{equation}
\frac{\partial\bar{x}^i}{\partial x^j}=B^i_{\ j}+C^i_{\ jk}(x^k-x^k_P), \qquad \frac{\partial^2\bar{x}^i}{\partial x^j\partial x^k}=C^i_{\ jk}. \label{jg1}
\end{equation}
En particular, en el punto $P$ las primeras derivadas se reducen a
\begin{equation}\label{jg2}
\frac{\partial\bar{x}^i}{\partial x^j}(P)=B^i_{\ j},
\end{equation}
por lo que su inversa en ese punto es simplemente (suponemos que la matriz correspondiente a $B^i_{\ j}$ es invertible)
\begin{equation}\label{jg3}
\frac{\partial x^i}{\partial \bar{x}^j}(P)=(B^{-1})^i_{\ j}.
\end{equation}
Con esto, podemos calcular las componentes de la conexión en el SC $\bar{x}$ usando (\ref{jg1}b), (\ref{jg2}), (\ref{jg3}), y la ley de transformación (\ref{dinv72}), que escribiremos como
\begin{align}
\bar{\Gamma}_{\ jk}^i(P) &= \frac{\partial\bar{x}^i}{\partial x^p}(P)\frac{\partial
x^l}{\partial\bar{x}^j}(P)\frac{\partial x^m}{\partial\bar{x}^k}(P)\,
\Gamma_{\ lm}^p(P) -\frac{\partial x^l}{\partial \bar{x}^j}(P)\frac{\partial x^m}{\partial \bar{x}^k}(P)\frac{\partial^2\bar{x}^i}{\partial x^l \partial x^m}(P) \\
&= B^i_{\ p}(B^{-1})^l_{\ j}(B^{-1})^m_{\ k}\Gamma^p_{\ lm}(P)-(B^{-1})^l_{\ j}(B^{-1})^m_{\ k}C^i_{\ lm} \\
&= (B^{-1})^l_{\ j}(B^{-1})^m_{\ k}\left[B^i_{\ p}\Gamma^p_{\ lm}(P)-C^i_{\ lm}\right]
\end{align}
Vemos entonces que la transformación (\ref{tccg}) satisface la condición (\ref{ccgG}) si elegimos $C^i_{\ lm}\stackrel{!}{=}B^i_{\ p}\Gamma^p_{\ lm}(P)$. Note que esta elección sólo es consistente si la conexión es simétrica.

Segundo, como el resultado anterior es válido en particular para la conexión métrica (los símbolos de Christoffel), tenemos que dado $P$ existe un SC en el que $\{^{\,i}_{jk}\}\stackrel{*}{=}0$. Usando la identidad $\partial_kg_{ij}\equiv g_{il}\,\{^{\, l} _{jk}\}+g_{jl}\,\{^{\, l} _{ik}\}$ es claro que esto implica que $\partial_kg_{ij}(P)\stackrel{*}{=}0$

Finalmente, como los coeficientes $B^i_{\ j}$ no han sido aún restringidos es posible elegirlos de modo tal que en el SC $\bar{x}$ la métrica adopte el valor estándar $\bar{g}_{ij}(P)\stackrel{*}{=}\eta_{ij}$. Usando la ley de transformación del tensor métrico encontramos que esta condición es equivalente a
\begin{equation}
\eta_{ij}=(B^{-1})^k_{\ i}(B^{-1})^l_{\ j}g_{kl}(P)
\end{equation}
o, en notación matricial $\eta=(\mathbf{B}^{-1})^T\,\mathbf{g}(P)\,\mathbf{B}^{-1}$.
Esta condición implica que $\mathbf{B}^{-1}$ es la matriz que diagonaliza a $\mathbf{g}(P)$, y esta matriz siempre existe en el caso de métricas (simétricas y reales) no singulares.
% \subsection{Divergencia}
%
% La divergencia del vector $A^i $ viene dada por $A_{// i}^i $.
% Encontremos ahora explícitamente la divergencia del vector $A^i $%
% \begin{align}
% A_{// i}^i  & =A_{/ i}^i +\Gamma_{ j i}^i A^j \\
% & =\partial_i A^i +A^j \Gamma_{ j i}^i \\
% & =A_{/ i}^i +A^j \partial_j \ln\sqrt{\left\vert g\right\vert}%
% \end{align}
% re-escribiendo el segundo término tenemos%
% \begin{align}
% A_{// i}^i  & =\frac{\partial_i A^i}{\partial x^i}+A^{ j
%}\frac{1}{\sqrt{\left\vert g\right\vert}}\frac{\partial\sqrt{\left\vert
% g\right\vert}}{\partial x^j}\\
% A_{// i}^i  & =\frac{1}{\sqrt{\left\vert g\right\vert}}\left[
% \sqrt{\left\vert g\right\vert}\frac{\partial_i A^i}{\partial x^i %
%}+A^i \frac{1}{\sqrt{\left\vert g\right\vert}}\frac{\partial
% \sqrt{\left\vert g\right\vert}}{\partial x^i}\right] \\
% A_{// i}^i  & =\frac{1}{\sqrt{\left\vert g\right\vert}}\left[
% \sqrt{\left\vert g\right\vert}\partial_i A^i +A^i \frac{1}%
% {\sqrt{\left\vert g\right\vert}}\partial_i \sqrt{\left\vert g\right\vert
%}\right]
% \end{align}
% de donde%
% \begin{equation}
% A_{// i}^i =\frac{1}{\sqrt{\left\vert g\right\vert}}\partial_i \left(
% \sqrt{\left\vert g\right\vert}A^i \right)
% \end{equation}
%
%
% Una expresión análoga para la divergencia de un tensor
% antisimétrico $A^{ij}:$%
% \begin{equation}
% A_{// j}^{ij}=\frac{\partial A^{ij}}{\partial x^j}+\Gamma
% _{ l j}^i A^{ l j}+\Gamma_{ l j}^j A^{ i l}%
% \end{equation}
% y dado que $A^{ij}$ es un tensor antisimétrico, tenemos%
% \begin{equation}
% \Gamma_{ l j}^i A^{ l j}=-\Gamma_{ j l}^i A^{ l j
%}=-\Gamma_{ l j}^i A^{ l j}=0
% \end{equation}
% así%
% \begin{equation}
% \Gamma_{ l j}^j A^{ i l}=A^{ i l}\frac{\partial\ln
% \sqrt{\left\vert g\right\vert}}{\partial x^{ l}}=\frac{A^{ i l}%
%}{\sqrt{\left\vert g\right\vert}}\frac{\partial\sqrt{\left\vert g\right\vert
%}}{\partial x^{ l}}%
% \end{equation}
% reemplazando
% \begin{equation}
% A_{// j}^{ij}=\frac{1}{\sqrt{\left\vert g\right\vert}}\frac{\partial
%}{\partial x^j}\left( \sqrt{\left\vert g\right\vert}A^{ij}\right)
% \end{equation}
%
%
% Si $A_{ij}$ es un tensor simétrico, entonces $A_{ i// j}^j $
% viene dada por%
% \begin{equation}
% A_{ i// j}^j =\frac{1}{\sqrt{\left\vert g\right\vert}}\frac{\partial
%}{\partial x^j}\left( \sqrt{\left\vert g\right\vert}A_i^{ j
%}\right) -\Gamma_{ij}^{ l}A_{ l}^j %
% \end{equation}
% dado que
% \begin{align}
% \Gamma_{ij}^{ l}A_{ l}^j  & =\frac{1}2\left( \frac{\partial
% g_{ m i}}{\partial x^j}+\frac{\partial g_{ m j}}{\partial x^i %
%}-\frac{\partial g_{ij}}{\partial x^{ m}}\right) A_{ l}^j \\
% \Gamma_{ij}^{ l}A_{ l}^j  & =\frac{1}2\left( \frac{\partial
% g_{ m i}}{\partial x^j}+\frac{\partial g_{ m j}}{\partial x^i %
%}-\frac{\partial g_{ij}}{\partial x^{ m}}\right) A^{ j m}\\
% \Gamma_{ij}^{ l}A_{ l}^j  & =\frac{1}2\left( \frac{\partial
% g_{ l i}}{\partial x^j}+\frac{\partial g_{ l j}}{\partial
% x^i}-\frac{\partial g_{ij}}{\partial x^{ l}}\right) A^{ j l}%
% \end{align}
% como $A^{ j l}=A^{ l j}$ podemos intercambiar $ j$ con $ l$ sin
% generar cambios. Sin embargoal re-escribir esto vemos que%
% \begin{equation}
% \Gamma_{ij}^{ l}A_{ l}^j =\frac{1}2\frac{\partial
% g_{ j l}}{\partial x^i}A^{ j l}%
% \end{equation}
% de manera que%
% \begin{equation}
% A_{ i// j}^j =\frac{1}{\sqrt{\left\vert g\right\vert}}\frac{\partial
%}{\partial x^j}\left( \sqrt{\left\vert g\right\vert}A_i^{ j
%}\right) -\frac{1}2\frac{\partial g_{ j l}}{\partial x^i}%
% A^{ j l}%
% \end{equation}

% \subsection{Elemento de Volumen Invariante}
%
% \bigskip Consideremos un cambio de coordenadas desde $x^i $ a $\overline
% {x}^i .$El tensor métrico $g_{ik}$ se transforma de la siguiente manera%
% \begin{equation}
% \overline{g}_{ik}=\frac{\partial x^m}{\partial\overline{x}^i}%
% \frac{\partial x^n}{\partial\overline{x}^k}g_{mn}%
% \end{equation}
% de donde vemos que el determinante $g$ se transforma de acuerdo a%
% \begin{equation}
% \overline{g}=g\left( \det\frac{\partial x^m}{\partial\overline{x}^i %
%}\right)^2%
% \end{equation}
% de manera que eligiendo el sifno positivo de la raíz cuadrada tenemos%
% \begin{equation}
% \sqrt{-\overline{g}}=\sqrt{-g}\left\vert \frac{\partial\left( x^1%
% ,x^2,\dots \right)}{\partial\left( \overline{x}^1,\overline{x}%
%^2,\dots \right)}\right\vert
% \end{equation}
% por otro lado sabemos que el elemento diferencial de volumen en 4 dimensiones
% es
% \begin{equation}
% d\tau=dx_{o}dx_1dx_2dx_{3}%
% \end{equation}
% el cual se transforma de acuerdo a
% \begin{equation}
% d\overline{\tau}=\left\vert \frac{\partial\left( \overline{x}^1%
% ,\overline{x}^2,\dots \right)}{\partial\left( x^1,x^2,\dots \right)
%}\right\vert d\tau
% \end{equation}
% así,%
% \begin{align}
% \sqrt{-\overline{g}}d\overline{\tau} & =\sqrt{-g}\left\vert \frac
% {\partial\left( x^1,x^2,\dots \right)}{\partial\left( \overline{x}%
%^1,\overline{x}^2,\dots \right)}\right\vert \left\vert \frac{\partial
% \left( \overline{x}^1,\overline{x}^2,\dots \right)}{\partial\left(
% x^1,x^2,\dots \right)}\right\vert d\tau\\
% \sqrt{-\overline{g}}d\overline{\tau} & =\sqrt{-g}d\tau
% \end{align}
%
%
% Esto significa que $\sqrt{-\overline{g}}d\overline{\tau}$ es un escalar que
% llamaremos elemento de volumen cuadridimensional invariante.
%
% \subsection{Métrica y conexión inducidas}
%
% Sea $M_{N}$ una variedad de dimensión $N$ y sea \ $S_n$ una variedad de
% dimensión $n\leq N$ tal que $S_n$ esté contenida en $M_{N}.$
% Representaremos la subveriedad paramétricamente por $x^\mu =x^\mu %
% (z^i )$, $\mu=1,2,..,N$, $i=1,2,..,n$. \ Utilizaremos letras griegas cuando
% los índices corren hasta $N$, y letras latinas cuando los indices corren
% hasta $R$.
%
% Si $M_{N}$ posee una métrica $g_{\mu\nu}$ y una conexión $\Gamma
% _{\mu\nu}^{\lambda}$, deseamos encontrar la métrica y la conexión que
% se induce sobre $S_{R}$.%
%
% \begin{center}
% \begin{figure}[H]
% \centerline{\psfig{file=6.ps,height=5cm,angle=0}}
% \caption{}
% \label{6}
% \end{figure}
% \end{center}
%
%
% \subsection{Inducción de la métrica}
%
% Un elemento de línea en $M_{N}$ está dado por:%
% \begin{equation}
% ds^2=g_{\mu\nu}dx^\mu dx^{v}.
% \end{equation}
% Como $x^\mu =x^\mu (z^i ),$ tenemos que $dx^\mu =\frac{\partial x^\mu %
%}{\partial z^i}dz^i .$ Por lo tanto, sobre $S_{R}$ el elemento de
% línea inducido es%
% \begin{equation}
% ds^2=g_{\mu\nu}\frac{\partial x^\mu}{\partial z^i}\frac{\partial x^{\nu
%}}{\partial z^j}dz^i dz^j =g_{ij}^{\ast}dz^i dz^j , \label{metind}%
% \end{equation}
%
%
% \bigskip con%
%
% \begin{equation}
% g_{ij}^{\ast}=g_{\mu\nu}\frac{\partial x^\mu}{\partial z^i}\frac{\partial
% x^\nu}{\partial z^j} \label{metind2}%
% \end{equation}
% donde $g_{ij}^{\ast}$ corresponde al tensor métrico inducido sobre $S_{R}$.
%
% \subsection{Inducción de la conexión}
%
% Sea $A^i $ un vector contravariante definido sobre $S_{R}$ tal que
% $dz^i =\lambda A^i $. Necesitamos una expresión para el vector asociado
% a $A^i $ sobre $M_{N}$.
%
% Como $dx^\mu =\frac{\partial x^\mu}{\partial z^i}dz^i $ \ tenemos
% $dx^\mu =\frac{\partial x^\mu}{\partial z^i}\lambda A^i $. Esto induce
% un vector $B^\mu $ en $M_{N}$%
% \begin{equation}
% B^\mu =\frac{\partial x^\mu}{\partial z^i}A^i \label{sube}%
% \end{equation}
% donde $B^\mu $ corresponde al vector asociado a $A^i $ en $M_{N}.$
%
% Como tenemos una conexión definida sobre $M_{N}$ podemos transportar el
% vector $B^\mu $ desde el punto $P$ de coordenadas $x^\mu $ al punto $Q$ de
% coordenadas $x^\mu +dx^\mu $, el cual está dado por:
% \begin{equation}
% \bar{B}^\mu (Q)=B^\mu -\Gamma_{\nu\lambda}^\mu B^\nu dx^{\lambda}.
% \end{equation}
%
%
% Reemplazando la ec. (\ref{sube}) tenemos que el vector transportado es:%
% \begin{equation}
% \bar{B}^\mu (Q)=\frac{\partial x^\mu}{\partial z^i}A^i -\Gamma
% _{\nu\lambda}^\mu\frac{\partial x^\nu}{\partial z^i}\frac{\partial
% x^{\lambda}}{\partial z^j}A^i dz^j . \label{hola}%
% \end{equation}
%
%
% Como deseamos encontrar la conexión de $S_{R}$ transportemos el vector
% $A^k $ desde el punto $P$ de coordenadas $z^i $ al punto $Q$ de coordenadas
% $z^i +dz^i $ con la conexión $\Gamma_{\ ij}^{\ast k}$ que deseamos
% encontrar.%
%
% \begin{equation}
% \bar{A}^i (Q)=A^i -\Gamma_{jk}^{\ast i}A^j dz^{K}.
% \end{equation}
%
%
% Ahora debemos proyectar el vector $\bar{B}^k (Q)$ sobre $S_{R}$ e igualarlo
% con $\bar{A}^i (Q).$ Pero vemos de la ec. (\ref{sube}) que no podemos obtener
% directamente un vector sobre $S_{R}$ a partir de un vector en $M_{N},$ pues
% aparece un factor $\frac{\partial x^\mu}{\partial z^i},$ el cual no es
% invertible por ser una matriz no cuadrada.
%
% Por lo tanto, debemos definir algún tipo de proyección, de tal manera
% de poder recuperar la ec. (\ref{sube}). Esta proyección nos
% permitiría obtener un vector en $S_{R}$ a partir de un vector en
% $M_{N}$.
%
% Dado un vector $B^\mu $ en $M_{N}$ definimos su proyección $A^i $ sobre
% $S_{R}$ como:%
% \begin{equation}
% A^i =g^{\ast ij}\frac{\partial x^\mu}{\partial z^j}g_{\mu\nu}B^\nu ,
% \label{baja}%
% \end{equation}
% donde $g^{\ast ij}$ corresponde a la inversa de la métrica inducida.
%
% Veamos si podemos recuperar la ec. (\ref{sube}). Para ello multipliquemos a
% ambos lados por $g_{ik}^{\ast}$%
% \begin{equation}
% A^i g_{ik}^{\ast}=\delta_k^j \frac{\partial x^\mu}{\partial z^j}%
% g_{\mu\nu}B^\nu .
% \end{equation}
%
%
% Reemplazando la ec. (\ref{metind}) tenemos:%
% \begin{align}
% A^i g_{\nu\mu}\frac{\partial x^\nu}{\partial z^i}\frac{\partial x^\mu %
%}{\partial z^k} & =\frac{\partial x^\mu}{\partial z^k}g_{\mu\nu}%
% B^\nu ,\\
% A^i \frac{\partial x^\nu}{\partial z^i} & =B^\nu . \label{subed}%
% \end{align}
%
%
% Donde la ec. (\ref{subed}) es igual a la ec. (\ref{sube}). Por lo tanto
% nuestra definición de proyección satisface la condición requerida.
% Notemos que para esta definición de proyección es necesario utilizar
% la métrica. Por lo tanto, en espacios que no poseen métrica no podemos
% realizar este procedimiento.
%
% \bigskip
%
% Proyectemos ahora el vector \thinspace$\bar{B}^\mu $ sobre $S_{R}$.
%
% Si $C^i $ corresponde a la proyección de $\bar{B}^\mu $ satisface:%
% \begin{equation}
% C^i (Q)=g^{\ast ij}(Q)\frac{\partial x^\mu}{\partial z^j}(Q)g_{\mu\nu
%}(Q)\bar{B}^\nu .
% \end{equation}
%
%
% Igualemos $C^i $ con $\bar{A}^i $
% \begin{equation}
% g_{ij}^{\ast}(z^k +dz^k )\bar{A}^j =g_{\mu\nu}(Q)\frac{\partial x^\nu %
%}{\partial z^i}(Q)\bar{B}^\mu . \label{condpro}%
% \end{equation}
%
%
% Expandiendo en serie $g_{\mu\nu}(x^{\lambda}+dx^{\lambda})$ y $\frac{\partial
% x^\nu}{\partial z^i}(z^k +dz^k ),$encontramos:%
% \begin{align}
% g_{\mu\nu}(x^{\lambda}+dx^{\lambda}) & =g_{\mu\nu}(x^{\lambda}%
% )+\partial_{\alpha}g_{\mu\nu}dx^{\alpha},\\
% \frac{\partial x^\nu}{\partial z^i}(z^k +dz^k ) & =\frac{\partial
% x^\nu}{\partial z^i}(z^k )+\frac{\partial^2 x^\nu}{\partial
% z^j \partial z^i}dz^j .
% \end{align}
%
%
% Reemplazando en (\ref{condpro}) y expandiendo en serie $g_{ij}^{\ast}%
% (z^k +dz^k )$ obtenemos:%
%
% \begin{align}
% (g_{ij}^{\ast}+\partial_{s}g_{ij}^{\ast}dz^s)(A^j -\Gamma_{lm}^{\ast
% j}A^l dz^m ) & =(g_{\mu\nu}+\partial_{\alpha}g_{\mu\nu}dx^{\alpha})\left(
% \frac{\partial x^\nu}{\partial z^i}+\frac{\partial^2 x^\nu}{\partial
% z^j \partial z^i}dz^j \right) \bar{B}^\mu ,\\
% \left( g_{ij}^{\ast}A^j -g_{ij}^{\ast}\Gamma_{lm}^{\ast j}A^l %
% dz^m +\partial_{s}g_{ij}^{\ast}A^j dz^s\right)  & =\left( g_{\mu\nu
%}\frac{\partial x^\nu}{\partial z^i}+g_{\mu\nu}\frac{\partial^2 x^\nu %
%}{\partial z^j \partial z^i}dz^j +\partial_{\alpha}g_{\mu\nu}%
% \frac{\partial x^\nu}{\partial z^i}dx^{\alpha}\right) \bar{B}^\mu .
% \end{align}
%
%
% Reemplazando $\bar{B}^\mu $ de (\ref{hola}) y multiplicando tenemos:%
% \begin{align}
% \left( g_{ij}^{\ast}A^j -g_{ij}^{\ast}\Gamma_{lm}^{\ast j}A^l %
% dz^m +\partial_{s}g_{ij}^{\ast}A^j dz^s\right)  & =g_{\mu\nu}%
% \frac{\partial x^\nu}{\partial z^i}\frac{\partial x^\mu}{\partial z^l %
%}A^l +g_{\mu\nu}\frac{\partial^2 x^\nu}{\partial z^j \partial z^i}%
% \frac{\partial x^\mu}{\partial z^l}A^l dz^j \label{faltapoco}\\
% & +\partial_{\alpha}g_{\mu\nu}\frac{\partial x^\nu}{\partial z^i}%
% \frac{\partial x^\mu}{\partial z^l}A^l dx^{\alpha}-\Gamma_{\alpha\beta
%}^\mu g_{\mu\nu}\frac{\partial x^\nu}{\partial z^i}\frac{\partial
% x^{\alpha}}{\partial z^m}\frac{\partial x^{\beta}}{\partial z^n}%
% A^m dz^n.\nonumber
% \end{align}
%
%
% Desarrollando la expresión $\partial_{s}g_{ij}^{\ast}$, llegamos a :%
% \begin{align}
% \partial_{s}g_{ij}^{\ast} & =\partial_{s}\left( g_{\mu\nu}\frac{\partial
% x^\mu}{\partial z^i}\frac{\partial x^\nu}{\partial z^j}\right) \\
% & =\left( \partial_{\alpha}g_{\mu\nu}\frac{\partial x^\mu}{\partial z^i %
%}\frac{\partial x^\nu}{\partial z^j}\frac{\partial x^{\alpha}}{\partial
% z^s}+g_{\mu\nu}\frac{\partial^2 x^\mu}{\partial z^s\partial z^i}%
% \frac{\partial x^\nu}{\partial z^j}+g_{\mu\nu}\frac{\partial^2 x^\nu %
%}{\partial z^s\partial z^j}\frac{\partial x^\mu}{\partial z^i}\right)
% .\nonumber
% \end{align}
%
%
% Reemplazando en (\ref{faltapoco}) y reduciendo los términos semejantes,
% encontramos%
% \begin{equation}
% g_{ij}^{\ast}\Gamma_{lm}^{\ast j}A^l dz^m =g_{\mu\nu}\frac{\partial
%^2x^\nu}{\partial z^s\partial z^j}\frac{\partial x^\mu}{\partial
% z^i}A^j dz^s+\Gamma_{\alpha\beta}^\mu g_{\mu\nu}\frac{\partial x^\nu %
%}{\partial z^i}\frac{\partial x^{\alpha}}{\partial z^m}\frac{\partial
% x^{\beta}}{\partial z^n}A^m dz^n.
% \end{equation}
%
%
% Cambiando algunos índices mudos y factorizando, llegamos a%
%
% \begin{align}
% g_{ij}^{\ast}\Gamma_{lm}^{\ast j}A^l dz^m  & =g_{\mu\nu}\frac{\partial
% x^\mu}{\partial z^i}\left( \frac{\partial^2 x^\nu}{\partial
% z^m \partial z^l}+\Gamma_{\alpha\beta}^\nu\frac{\partial x^{\alpha}%
%}{\partial z^l}\frac{\partial x^{\beta}}{\partial z^m}\right) A^l %
% dz^m ,\\
% g_{ij}^{\ast}\Gamma_{lm}^{\ast j} & =g_{\mu\nu}\frac{\partial x^\mu %
%}{\partial z^i}\left( \frac{\partial^2 x^\nu}{\partial z^m \partial
% z^l}+\Gamma_{\alpha\beta}^\nu\frac{\partial x^{\alpha}}{\partial z^l %
%}\frac{\partial x^{\beta}}{\partial z^m}\right) . \label{casiconind}%
% \end{align}
%
%
% Finalmente despejando $\Gamma_{lm}^{\ast j}$, encontramos%
% \begin{equation}
% \Gamma_{lm}^{\ast k}=g^{\ast ki}g_{\mu\nu}\frac{\partial x^\mu}{\partial
% z^i}\left( \frac{\partial^2 x^\nu}{\partial z^m \partial z^l}%
% +\Gamma_{\alpha\beta}^\nu\frac{\partial x^{\alpha}}{\partial z^l}%
% \frac{\partial x^{\beta}}{\partial z^m}\right) . \label{conind}%
% \end{equation}
%
%
% La ecuación anterior nos entrega la conexión inducida sobre $S_{N}.$
%
%
% \subsection{Características de una variedad que está contenida dentro
% de un espacio con geometría de Riemann}
%
% Queremos ver qué sucede con la conexión inducida si $M_{N}$ posee una
% geometría de Riemann. , queremos saber si $S_{R}$ posee
% también una geometría de Riemann. Para ello supongamos que la
% conexión de $M_{N}$ es el símbolo de Christoffel:%
% \begin{equation}
% \Gamma_{\alpha\beta}^\nu =\frac{1}2g^{\nu\lambda}(\partial_{\alpha}%
% g_{\beta\lambda}+\partial_{\beta}g_{\alpha\lambda}-\partial_{\lambda}%
% g_{\alpha\beta}).
% \end{equation}
%
%
% Reemplazando en (\ref{conind}), obtenemos:%
% \begin{align}
% \Gamma_{lm}^{\ast k} & =g^{\ast ki}g_{\mu\nu}\frac{\partial x^\mu %
%}{\partial z^i}\frac{\partial^2 x^\nu}{\partial z^m \partial z^l %
%}+\frac{1}2g^{\ast ki}g_{\mu\nu}\frac{\partial x^\mu}{\partial z^i %
%}g^{\nu\lambda}\left( \partial_{\alpha}g_{\beta\lambda}+\partial_{\beta
%}g_{\alpha\lambda}-\partial_{\lambda}g_{\alpha\beta}\right) \frac{\partial
% x^{\alpha}}{\partial z^l}\frac{\partial x^{\beta}}{\partial z^m}\\
% & =g^{\ast ki}g_{\mu\nu}\frac{\partial x^\mu}{\partial z^i}\frac
% {\partial^2 x^\nu}{\partial z^m \partial z^l}+\frac{1}2g^{\ast
% ki}\frac{\partial x^\mu}{\partial z^i}\left( \partial_{\alpha}g_{\beta
% \mu}+\partial_{\beta}g_{\alpha\mu}-\partial_\mu g_{\alpha\beta}\right)
% \frac{\partial x^{\alpha}}{\partial z^l}\frac{\partial x^{\beta}}{\partial
% z^m}\\
% & =g^{\ast ki}\frac{\partial x^\mu}{\partial z^i}\left[ g_{\mu\nu}%
% \frac{\partial^2 x^\nu}{\partial z^m \partial z^l}+\frac{1}2\left(
% \partial_{\alpha}g_{\beta\mu}+\partial_{\beta}g_{\alpha\mu}-\partial_{\mu
%}g_{\alpha\beta}\right) \frac{\partial x^{\alpha}}{\partial z^l}%
% \frac{\partial x^{\beta}}{\partial z^m}\right] . \label{mitcon}%
% \end{align}
%
%
% Si $S_{R}$ posee una geometría de Riemann debe satisfacer que su
% conexión es el símbolo de Christoffel, :%
%
% \begin{equation}
% \Gamma_{lm}^{\ast k}=\frac{1}2g^{\ast ki}(\partial_l g_{mi}^{\ast}%
% +\partial_mg_{li}^{\ast}-\partial_ig_{lm}^{\ast}) \label{simch}%
% \end{equation}
%
%
% Desarrollemos esta expresión e intentemos llegar a (\ref{mitcon}). Si
% $g_{ij}^{\ast}=g_{\mu\nu}\frac{\partial x^\mu}{\partial z^i}\frac{\partial
% x^\nu}{\partial z^j}$ entonces%
% \begin{subequations}
% \begin{align}
% \frac{\partial g_{mi}^{\ast}}{\partial z^l} & =\frac{\partial g_{\mu\nu}%
%}{\partial x^{\alpha}}\frac{\partial x^{\alpha}}{\partial z^l}\frac{\partial
% x^\mu}{\partial z^m}\frac{\partial x^\nu}{\partial z^i}+g_{\mu\nu
%}\frac{\partial^2 x^\mu}{\partial z^l \partial z^m}\frac{\partial
% x^\nu}{\partial z^i}+g_{\mu\nu}\frac{\partial^2 x^\nu}{\partial
% z^l \partial z^i}\frac{\partial x^\mu}{\partial z^m}\\
% \frac{\partial g_{li}^{\ast}}{\partial z^m} & =\frac{\partial g_{\mu\nu}%
%}{\partial x^{\alpha}}\frac{\partial x^{\alpha}}{\partial z^m}\frac{\partial
% x^\mu}{\partial z^l}\frac{\partial x^\nu}{\partial z^i}+g_{\mu\nu
%}\frac{\partial^2 x^\mu}{\partial z^m \partial z^l}\frac{\partial
% x^\nu}{\partial z^i}+g_{\mu\nu}\frac{\partial^2 x^\nu}{\partial
% z^m \partial z^i}\frac{\partial x^\mu}{\partial z^l}\\
% \frac{\partial g_{lm}^{\ast}}{\partial z^i} & =\frac{\partial g_{\mu\nu}%
%}{\partial x^{\alpha}}\frac{\partial x^{\alpha}}{\partial z^i}\frac{\partial
% x^\mu}{\partial z^l}\frac{\partial x^\nu}{\partial z^m}+g_{\mu\nu
%}\frac{\partial^2 x^\mu}{\partial z^i \partial z^l}\frac{\partial
% x^\nu}{\partial z^m}+g_{\mu\nu}\frac{\partial^2 x^\nu}{\partial
% z^i \partial z^m}\frac{\partial x^\mu}{\partial z^l}%
% \end{align}
%
%
% Reemplazando en (\ref{simch}) obtenemos:%
% \end{subequations}
% \begin{equation}
% \Gamma_{lm}^{\ast k}=g^{\ast ki}\frac{\partial x^\mu}{\partial z^i}\left[
% g_{\mu\nu}\frac{\partial^2 x^\nu}{\partial z^m \partial z^l}+\frac{1}%
% 2\left( \partial_{\alpha}g_{\beta\mu}+\partial_{\beta}g_{\alpha\mu
%}-\partial_\mu g_{\alpha\beta}\right) \frac{\partial x^{\alpha}}{\partial
% z^l}\frac{\partial x^{\beta}}{\partial z^m}\right]
% \end{equation}
% que corresponde a la ec. (\ref{mitcon}).
%
% Por lo tanto la conexión de $S_{R}$ es (\ref{simch}), que es el
% símbolo de Christoffel asociado a la métrica inducida sobren $S_{R}.$
% Así, hemos demostrado el siguiente teorema.
%
% \textbf{Teorema:}
%
% Si una variedad dada posee una geometría de Riemann, entonces cualquier
% variedad que esté contenida en ella también tendrá una
% geometría de Riemann.
%
% \bigskip
%
% Esto explica el hecho de que la conexión asociada a una superficie inmersa
% en $R_3$ es el símbolo de Christoffel y que no es posible
% encontrar una superficie que posea otro tipo de conexión, respetando la
% proyección definida en (\ref{baja})
%
% \subsection{Proyección sobre un vector.}
%
% Podemos definir la proyección de un vector en $M_{N}$ sobre un vector en
% $S_{R}$ de la siguiente forma:
%
% Si $B^\mu $ es un vector en $M_{N}$ y $C^i $ es un vector en $S_{R}$,
% entonces la proyección de $B^\mu $ sobre $C^i $ está dada por:
% \begin{equation}
% B_{p}^i =\frac{\left[ g_{\mu\nu}B^\mu\frac{\partial x^\nu}{\partial
% z^m}C^m \right]}{\left[ g_{\lambda\delta}\frac{\partial x^{\lambda}%
%}{\partial z^m}\frac{\partial x^{\delta}}{\partial z^j}C^m C^j \right]
%}C^i . \label{provec}%
% \end{equation}
%
%
% Probemos que a partir de esta definición podemos recuperar la
% expresión (\ref{baja}).
%
% Si queremos que $B_{p}^i $ coincida con $C^i $ tenemos:%
%
% \begin{align}
% C^i \left[ g_{\lambda\delta}\frac{\partial x^{\lambda}}{\partial z^m}%
% \frac{\partial x^{\delta}}{\partial z^j}C^m C^j \right]  & =\left[
% g_{\mu\nu}B^\mu\frac{\partial x^\nu}{\partial z^m}C^m \right] C^i ,\\
% g_{mj}^{\ast}C^m C^j  & =g_{\mu\nu}B^\mu\frac{\partial x^\nu}{\partial
% z^m}C^m ,\\
% g_{mj}^{\ast}C^j  & =g_{\mu\nu}B^\mu\frac{\partial x^\nu}{\partial
% z^m},\\
% C^k  & =g^{\ast km}g_{\mu\nu}B^\mu\frac{\partial x^\nu}{\partial z^m %
%}.
% \end{align}
%
%
% , a partir de la proyección definida por (\ref{provec}), podemos
% reobtener (\ref{baja}).



\subsection{Propiedades del tensor de Riemann}
Si consideramos las componentes totalmente covariantes del tensor de curvarura, $R_{ijkl}:=g_{im}R^m_{\ jkl}$, es posible probar que en el caso de una geometría riemanniana, es decir, cuando la conexión es el símbolo de Christoffel de la métrica, el tensor de curvatura de Riemann posee, adicionalmente a (\ref{asRT}a), las siguientes (anti-)simetrías:
\begin{align}
R_{ijkl} & =-R_{jikl},\label{simR2}\\
R_{ijkl} & =R_{klij}, \label{simR3}
\end{align}
y, además\footnote{En general se satisface $R^l_{\ [ijk]}\equiv -T^m_{\ [ij}T^l_{\ k]m}-\nabla_{[i}T^l_{\ jk]}$.},
\begin{equation}
R_{ijkl}+R_{iklj}+R_{iljk}=0\quad\Leftrightarrow\quad
R_{i[jkl]}=0. \label{simR4}
\end{equation}

En efecto, de la definición del tensor de Riemann (\ref{curvatura}) y usando la expresión de los símbolos de Christoffel, obtenemos que las componentes completamente covariantes $R_{ijkl}$ pueden escribirse de la forma siguiente:
\begin{eqnarray}
 R_{ijkl}&=&g_{im}\left(\partial_k\Gamma^m_{\ jl}-\partial_l\Gamma^m_{\ jk}+\Gamma^m_{\ nk}\Gamma^n_{\ jl}-\Gamma^m_{\ nl}\Gamma^n_{\ jk}\right) \\
&=&\partial_k\left(g_{im}\Gamma^m_{\ jl}\right)-\Gamma^m_{\ jl}\partial_k g_{im}-\partial_l\left(g_{im}\Gamma^m_{\ jk}\right)+\Gamma^m_{\ jk}\partial_l g_{im} \nonumber\\
&&+\,g_{im}\left(\Gamma^m_{\ nk}\Gamma^n_{\ jl}-\Gamma^m_{\ nl}\Gamma^n_{\ jk}\right) \\
&=&\frac{1}{2}\partial_k\left(\partial_jg_{il}+\partial_lg_{ij}-\partial_ig_{jl}\right)-\frac{1}{2}\partial_l\left(\partial_jg_{ik}+\partial_kg_{ij}-\partial_ig_{jk}\right)\nonumber\\
&&+\left(\partial_lg_{in}-g_{im}\Gamma^m_{\ nl}\right)\Gamma^n_{\ jk}-\left(\partial_kg_{in}-g_{im}\Gamma^m_{\ nk}\right)\Gamma^n_{\ jl}.
\end{eqnarray}
Usamos ahora la identidad (\ref{tI1}) para escribir $\partial_lg_{in}-g_{im}\Gamma^m_{\ nl}=\nabla_lg_{in}+g_{mn}\Gamma^m_{\ il}= g_{mn}\Gamma^m_{\ il}$. Con esto obtenemos
\begin{equation}
 R_{ijkl}=\frac{1}{2}\left(\partial_j\partial_kg_{il}+\partial_i\partial_lg_{jk}-\partial_j\partial_lg_{ik}-\partial_i\partial_kg_{jl}\right) + g_{mn}\left(\Gamma^m_{\ il}\Gamma^n_{\ jk}-\Gamma^m_{\ ik}\Gamma^n_{\ jl}\right), \label{Rdddd}
\end{equation}
de donde es posible verificar directamente las propiedades de simetría (\ref{simR2})-(\ref{simR4}).

Debido a estas simetrías adicionales, las \textit{componentes completamente covariantes linealmente independientes} del tensor de curvatura de una variedad con geometría riemanniana de dimensión $n$ se reducen a ``sólo''\, $n^2(n^2-1)/12$. Así, en los casos $n=1,2,3,4$ se tienen 0, 1, 6 y 20 componentes linealmente independientes, respectivamente.

\subsection{Tensor de Ricci, y escalar de curvatura}
A partir del tensor de Riemann podemos definir el \textbf{tensor de Ricci}\footnote{En honor a Gregorio Ricci-Curbastro: 1853-1925, matemático Italiano. Ver \url{http://es.wikipedia.org/wiki/Gregorio_Ricci-Curbastro}.}:
\begin{equation}
R_{jl}:=R_{\ jil}^i,
\end{equation}
que, en un espacio con geometría riemanniana, es simétrico $R_{ij}=R_{ji}$, en virtud de (\ref{simR3}). Debido a las simetrías del tensor de curvatura, la contracción que define al tensor de Ricci es la única linealmente independiente. En otras palabras, otras contracciones son nulas o proporcionales al tensor de Ricci. Por ejemplo, $R^i_{\ ijk}=0$, $R^i_{\ jki}=-R_{jk}$.

El \textbf{escalar de curvatura} es definido como la ``traza del tensor de
Ricci'',
\begin{equation}
R :=g^{ij}R_{ij}.
\end{equation}

Otro tensor útil es el \textbf{tensor de Einstein}, definido por
\begin{equation}
G_{ij} :=R_{ij}-\frac{1}{2}g_{ij}R
\end{equation}
que, en un espacio con geometría riemanniana, es también simétrico.



\subsection{Identidades de Bianchi}
Es posible probar que las derivadas covariantes del tensor de curvatura de Riemann
satisfacen \textbf{las identidades de Bianchi}\footnote{En honor a Luigi Bianchi: 1856-1928, matemático italiano. Ver \url{http://en.wikipedia.org/wiki/Luigi_Bianchi}  \textbf{$\leftarrow$ se necesita traducción al español de esto!}.}:
\begin{equation}
\boxed{\nabla_{\lbrack i}R_{jk]l m}\equiv 0,}
\end{equation}
o, equivalentemente,
\begin{equation}
 \nabla_iR_{jklm}+\nabla_jR_{kilm}+\nabla_kR_{ijlm}=0. \label{Bi2}
\end{equation}
Aquí verificaremos su validez en el caso riemanniano, usando nuestro resultado anterior (\ref{Rdddd}) y evaluando $\nabla_{\lbrack i}R_{jk]l m}$ en un punto $P$ dado, pero arbitrario, \textit{usando coordenadas geodésicas}. En efecto, a partir de (\ref{Rdddd}) vemos que
\begin{eqnarray}
 \nabla_mR_{ijkl}(P)&=&\partial_mR_{ijkl}-\Gamma^n_{\ im} R_{njkl}-\Gamma^n_{\ jm} R_{inkl}-\Gamma^n_{\ km} R_{ijnl}-\Gamma^n_{\ lm} R_{ijkn}\\
&\stackrel{*}{=}&\partial_mR_{ijkl}+0+0+0+0\\
&\stackrel{*}{=}&\partial_m\left[\frac{1}{2}\left(\partial_j\partial_kg_{il}+\partial_i\partial_lg_{jk}-\partial_j\partial_lg_{ik}-\partial_i\partial_kg_{jl}\right) \right. \nonumber\\
& &  \qquad + \left. g_{\alpha\beta}\left(\Gamma^\alpha_{\ il}\Gamma^\beta_{\ jk}-\Gamma^\alpha_{\ ik}\Gamma^\beta_{\ jl}\right)\right]\\
&\stackrel{*}{=}&\frac{1}{2}\partial_m\left(\partial_j\partial_kg_{il}+\partial_i\partial_lg_{jk}-\partial_j\partial_lg_{ik}-\partial_i\partial_kg_{jl}\right) + 0 +0.
\end{eqnarray}
De aquí es directo verificar que la parte antisimétrica $\nabla_{\lbrack i}R_{jk]l m}(P)$ efectivamente es nula. Como esta cantidad es un tensor y el punto $P$ es arbitrario, hemos entonces verificado que $\nabla_{\lbrack i}R_{jk]l m}=0$ en todo sistema coordenado y \textit{cada punto} $P$ de la variedad.

A partir la identidad de Bianchi es directo verificar que el tensor de Einstein es covariantemente constante:
\begin{equation}
\boxed{\nabla_i G^{ij}\equiv 0.} \label{dcG0}
\end{equation}
En efecto, contrayendo (\ref{Bi2}) con $g^{k m}g^{il}$ podemos escribir
\begin{eqnarray}
0&=&g^{k m}g^{il}\left( \nabla_iR_{jklm}+\nabla_jR_{kil m}+\nabla_kR_{ij
l m}\right) \\
&=&\nabla_i\left(g^{k m}g^{il}R_{jklm}\right)+\nabla_j\left(g^{k m}g^{il}R_{kil m}\right)+\nabla_k\left(g^{k m}g^{il}R_{ijl m}\right) \\
&=&\nabla_i\left(g^{k m}g^{il}R_{kjml}\right)-\nabla_j\left(g^{k m}g^{il}R_{ikl m}\right)+\nabla_k\left(g^{k m}g^{il}R_{ijl m}\right) \\
&=&\nabla_i\left(g^{il}R_{jl}\right)-\nabla_j\left(g^{k m}R_{km}\right)+\nabla_k\left(g^{k m}R_{jm}\right) \\
&=&\nabla_iR^i_{\ j}-\nabla_jR+\nabla_kR^k_{\ j} \\
&=&\nabla_i\left(2R^i_{\ j}-\delta^i_jR\right)
\end{eqnarray}
que, luego de usar nuevamente (\ref{tI1}), es equivalente a (\ref{dcG0}).

